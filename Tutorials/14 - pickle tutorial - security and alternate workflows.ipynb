{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0375e76e",
   "metadata": {},
   "source": [
    "# pickle tutorial - security and alternate workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96cb98e",
   "metadata": {},
   "source": [
    "The purpose of this notebook will be to demonstrate some conventions for saving the \"postprocess_dict\" dictionary, which is the returned dictionary from an automunge(.) call which may be used to productionize a model with preprocessing built through the Automunge library. \n",
    "\n",
    "Note that our tutorials so far have relied on the pickle library which is a native python module used to serialize and download python dictionaries, where the serialization is a non-encrypted form of data compression, and then the same pickle module may then be used to upload and initialize that same dictionary in a new notebook or environment. In some cases the serialization of a postprocessdict may be beneficial from a storage memory standpoint when preprocessing includes forms of ML infill which may store in the dictionary a trained model specific to each tabular feature. We understand that there are other options available to download a python dictionary without serialization (e.g. libraries like JSON, YAML, etc), however the pickle module has support for serializing additional datya types populated in a dictionary that may not be supported by other forms of JSON download. The most relevant data types that pickle may help with include those trained machine learning models returned Automunge library components for e.g. ML infill, PCA, feature selection, etc, as well as any custom data transformation functions that may have been iniatialized by a user for purposes of custom data transformations integrated into the family tree primitives API.\n",
    "\n",
    "The pickle library, although widely used in data science and other circles, has a known security vulnerability associated with cases with an uploaded serialized dictionary had been altered and not matched to the original intended form of distribution. We note in our readme that python's documentation has suggested a mitigation for this vulnerability that relies on a second python module called [hmac](https://docs.python.org/3/library/hmac.html#module-hmac), which serves the purposes of deriving a form of signature for an original downloaded pickle object which can then be compared to an uploaded object received from a potentially unsecure channel in order to validate that the uploaded form matches the original intended basis. In the demonstrations of this notebook the hmac signature will be derived and then affixed to the pickled object for comparison prior to loading. An alternative and even more secure approach may be to share the signature thorugh a seperate more secure channel as an additional means of redundancy. This notebook is our first demonstration of the incorporation of the hmac tool into a workflow in our documentation, which will be one of the agendas of this notebook.\n",
    "\n",
    "One of the ways that pickle appears to attempt to circumvent aspects of this vulnerability is that for cases where a data type is associated with a custom defined function (such as we noted above may be the cases when the Automunge API is used for custom designed feature transformation sets), the serialized pickle object doesn't actually store and retain a full function definition, it merely saves a pointer to a function identifier, such that when a pickle object is uploaded and reinitialized into a seperate notebook or environment a user must first reinitialize any custom functions that were oriignally populated through an automunge(.) call. \n",
    "\n",
    "In the context of the Automunge workflow, there are definitely tradeoffs associated with relying on a user to re-initalize a custom function definition in a new production environment. Intuitively, for any scenario where the user training a machine learning model with preprocessing conducted through the Automunge API (\"Alice\") does not match the user running the model in a productionized environment (\"Bob\"), there may be a desire for Bob not to have easy access to the full function definitions, either from a privacy standpoint or just as important from an ease of distribution and initialization standpoint. There are alternatives to the pickle library that may have different treatment for the serialization of custom function definitions. Two examples of pickle alternatives include the [dill](https://pypi.org/project/dill/) and [joblib](https://pypi.org/project/joblib/) libraries, this notebook will mainly focus on conventions of the dill library as a starting point. \n",
    "\n",
    "The dill library we understand to have several similar conventions to the pickle library, in fact for our demonstrations below we merely replaced the import and accessing of \"pickle\" with \"dill\", however a key difference is that when serializing a python dictionary with entries of custom defined functions, instead of serializing the function as a pointer like pickle, the dill library serializes the entire function definition. From an ease of distribtuion and integration into a new productionized environment this has obvious benefits (no need to seperately distribute function definitions), however from a security standpoint this raises the stakes quite significantly for trust in a recieved serialized object. We suggest that in any scenario where a user is considering a library like dill for ease of distriobution in context of custom functions that they _always_ do so in conjunction with the hmac signature functionality which we will also demonstrate below. Please note that while the pickle is an internal module to the python language with all of the quality control this would imply, libraries like dill (or Automunge for that matter) are often open source projects from smaller development teams and so a user may consider performing their own diligence in conjection with a productionized implemention.\n",
    "\n",
    "To restate an important distinction, please note that our demonstrations of the hmac signature will be associated with affixing a derived signature as the first line in the serialized package. We expect that an even more secure form of packaging could be implemented by distirbuting such signature through some seperate and more secure channel as an added bit of redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b64a12",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ca6e5",
   "metadata": {},
   "source": [
    "The Automunge library [readme](https://github.com/Automunge/AutoMunge/blob/master/README.md) contains a concise demonstration / code sample for the integration of the pickle module into a user workflow, we provide again here:\n",
    "\n",
    "-----\n",
    "\n",
    "```\n",
    "#Sample pickle code:\n",
    "\n",
    "#sample code to download postprocess_dict dictionary returned from automunge(.)\n",
    "import pickle\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "  pickle.dump(postprocess_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#to upload for later use in postmunge(.) in another notebook\n",
    "import pickle\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "  postprocess_dict = pickle.load(handle)\n",
    "\n",
    "#Please note that if you included externally initialized functions in an automunge(.) call\n",
    "#like for custom_train transformation functions or customML inference functions\n",
    "#they will need to be reinitialized prior to uploading the postprocess_dict with pickle.\n",
    "```\n",
    "-----\n",
    "\n",
    "In cases where we demonstrate alternate workflows to this sample code it will be presented in a form that may be directly substituted in place of the code sample above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4a777",
   "metadata": {},
   "source": [
    "Our agenda for this notebook will be to present the following alternatives to the sample pickle workflow noted above:\n",
    "\n",
    "**1. demonstration of the Automunge API in a workflow relying on the pickle code demonstration above**\n",
    "\n",
    "**2. pickle module as used in conjunction with custom defined transformation functions in Automunge API**\n",
    "\n",
    "**3. demonstration of the integration of the hmac signature into a pickle workflow**\n",
    "\n",
    "**4. demonstration of the dill library (in conjunction with hmac signature) as an alternative to pickle**\n",
    "\n",
    "**5. demonstration of the dill library (in conjunction with hmac signature) with custom defined transformation functions**\n",
    "\n",
    "_____\n",
    "\n",
    "Great let's get started.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2902b9e",
   "metadata": {},
   "source": [
    "# 1. demonstration of the Automunge API in a workflow relying on the pickle code demonstration above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa776f",
   "metadata": {},
   "source": [
    "Some basics of the workflow under automation presented here. This is just meant to demonstrate the inegration of the pickle downloads / upload.\n",
    "\n",
    "Assume we are working with the Titanic data set as a simple common benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575b0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Automunge\n",
      "\n",
      "______\n",
      "\n",
      "versioning serial stamp:\n",
      "_8.33_185349838215\n",
      "\n",
      "Automunge returned train column set: \n",
      "['Pclass_nmbr', 'Sex_bnry', 'Age_nmbr', 'SibSp_nmbr', 'Parch_nmbr', 'Ticket_nmbr', 'Fare_nmbr', 'Pclass_NArw', 'Name_NArw', 'Name_hash_0', 'Name_hash_1', 'Name_hash_2', 'Name_hash_3', 'Name_hash_4', 'Name_hash_5', 'Name_hash_6', 'Name_hash_7', 'Name_hash_8', 'Name_hash_9', 'Name_hash_10', 'Name_hash_11', 'Name_hash_12', 'Name_hash_13', 'Sex_NArw', 'Age_NArw', 'SibSp_NArw', 'Parch_NArw', 'Ticket_NArw', 'Fare_NArw', 'Cabin_NArw', 'Cabin_1010_0', 'Cabin_1010_1', 'Cabin_1010_2', 'Cabin_1010_3', 'Cabin_1010_4', 'Cabin_1010_5', 'Cabin_1010_6', 'Cabin_1010_7', 'Embarked_NArw', 'Embarked_1010_0', 'Embarked_1010_1']\n",
      "\n",
      "Automunge returned ID column set: \n",
      "['PassengerId', 'Automunge_index']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['Survived_lbbn']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "#run automunge(.) to encode the dataframes and populate a postprocess_dict\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(df_train,\n",
    "             labels_column = labels_column,\n",
    "             trainID_column = trainID_column,\n",
    "             ML_cmnd = {'stochastic_impute_numeric' : False,\n",
    "                        'stochastic_impute_categoric' : False,\n",
    "                       },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6aaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample code to download postprocess_dict dictionary returned from automunge(.)\n",
    "import pickle\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "  pickle.dump(postprocess_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952b03a",
   "metadata": {},
   "source": [
    "Now if we want to simulate uplading the serialized pickle dictionary in a seperate notebook we can restart the jupyter notebook kernel, usually can use this keyboard shortcut.\n",
    "```\n",
    "#now apply a Kernel restart\n",
    "#Esc + 0 0\n",
    "```\n",
    "\n",
    "Then in the reset notebook we can re-apply imports and uplaod the pickled postprocess_dict for use to consistently encode a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a364b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload for later use in postmunge(.) in another notebook\n",
    "import pickle\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "  postprocess_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9113e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload for later use in postmunge(.) in another notebook\n",
    "import pickle\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "  postprocess_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "\n",
    "# df_test\n",
    "\n",
    "\n",
    "#the uplaoded postprocess_dict is used as basis to encode any additional test data\n",
    "test, test_ID, test_labels, \\\n",
    "postreports_dict \\\n",
    "= am.postmunge(postprocess_dict, \n",
    "               df_test,\n",
    "               printstatus=False,\n",
    "              testID_column=trainID_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faddc41c",
   "metadata": {},
   "source": [
    "# 2. pickle module as used in conjunction with custom defined transformation functions in Automunge API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2bc32a",
   "metadata": {},
   "source": [
    "We refer reader to the tutorial notebook on github \"11 - Custom Transformations\" for a full demonstration of defining custom transformation functions for integration into the Automunge API for encoding dataframes. As a clarification to this workflow, it should be noted that in the context of relying on the pickle library for downloading a serialized postprocess_dict dictionary for use in a seperate notebook, any custom transformation functions that are integrated into the Automunge API call will need to reinitiatized in the seperate notebook prior to uplaoding the postprocess_dict with pickle. Should a user not desire that the functions be visible for reinitializion in a seperate notebook we note above that using libraries like Dill noted above there may be potential to package full function definitions into a serialized postprocess_dict although such practices will be outside the scope of this tutorial.\n",
    "\n",
    "We present here a demonstration of custom transformation funciton definition for integration into an automunge(.) call followed by pickle download, reset of notebook to simiulate a new environment, reinitializaiton of the custom functions, followed by pickle upload, and then the postmunge(.) call for processing additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fcfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325d6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we define our custom transformation functions\n",
    "#here we will define three functions for training data, tests data, and an inversino funciton\n",
    "#these are consistent with the seperate tutorial notebook \"Custom Transformations\"\n",
    "\n",
    "\n",
    "def custom_train_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #Template for a custom_train transformation function to be applied to a train feature set.\n",
    "  #further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the application of z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  \n",
    "  #which can accept any kind of numeric data \n",
    "  #so corresponding NArowtype processdict entry can be 'numeric'\n",
    "  #and returns a single column of continuous numeric data \n",
    "  #so corresponding MLinfilltype processdict entry will need to be 'numeric'\n",
    "\n",
    "  #where we'll include the option for a parameter 'muiltiplier'\n",
    "  #which is an arbitrary example to demonstrate accessing parameters\n",
    "  \n",
    "  #basically we check if that parameter had been passed in assignparam or defaultparams\n",
    "  if 'multiplier' in normalization_dict:\n",
    "    multiplier = normalization_dict['multiplier']\n",
    "    \n",
    "  #or otherwise assign and save a default value\n",
    "  else:\n",
    "    multiplier = 1\n",
    "    normalization_dict.update({'multiplier' : multiplier})\n",
    "\n",
    "  #Now we measure any properties of the train data used for the transformation\n",
    "  mean = df[column].mean()\n",
    "  stdev = df[column].std()\n",
    "  \n",
    "  #It's good practice to ensure numbers used in derivation haven't been derived as nan\n",
    "  #or would result in dividing by zero\n",
    "  if mean != mean:\n",
    "    mean = 0\n",
    "  if stdev != stdev or stdev == 0:\n",
    "    stdev = 1\n",
    "    \n",
    "  #In general if that same basis will be needed to process test data we'll store in normalization_dict\n",
    "  normalization_dict.update({'mean' : mean,\n",
    "                             'stdev': stdev})\n",
    "\n",
    "  #Optionally we can measure additional drift stats for a postmunge driftreport\n",
    "  #we will also save those in the normalization_dict\n",
    "  minimum = df[column].min()\n",
    "  maximum = df[column].max()\n",
    "  normalization_dict.update({'minimum' : minimum,\n",
    "                             'maximum' : maximum})\n",
    "\n",
    "  #Now we can apply the transformation\n",
    "  \n",
    "  #The generic formula for z-score normalization is (x - mean) / stdev\n",
    "  #here we incorporate an additional variable as the multiplier parameter (defaults to 1)\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df, normalization_dict\n",
    "\n",
    "def custom_test_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #This transform will be applied to a test data feature set\n",
    "  #on a basis of a corresponding custom_train entry\n",
    "  #Such as test data passed to either automunge(.) or postmunge(.)\n",
    "  #Using properties from the train set basis stored in the normalization_dict\n",
    "\n",
    "  #Further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the corresponding z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which was populated in a normalization_dict in the custom_train example given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #apply the transform\n",
    "  #and return the transformed dataframe\n",
    "\n",
    "  #access the train set properties from normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #then apply the transformation and return the dataframe\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def custom_inversion_template(df, returnedcolumn_list, inputcolumn, normalization_dict):\n",
    "  \"\"\"\n",
    "  #User also has the option to define a custom inversion function\n",
    "  #Corresponding to custom_train and custom_test\n",
    "  #furher detail on conventions provided in readme file\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here we'll be inverting the z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which corresponds to the examples given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #Initialize the new column inputcolumn\n",
    "  #And use values in the set from returnedcolumn_list to recover values for inputcolumn\n",
    "\n",
    "  #First let's access the values we'll need from the normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #Now initialize the inputcolumn\n",
    "  df[inputcolumn] = 0\n",
    "\n",
    "  #So for the example of z-score normalization, we know returnedcolumn_list will only have one entry\n",
    "  #In some other cases transforms may have returned multiple columns\n",
    "  returnedcolumn = returnedcolumn_list[0]\n",
    "\n",
    "  #now we perform the inversion\n",
    "  df[inputcolumn] = (df[returnedcolumn] * stdev / multiplier) + mean\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a35634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Automunge\n",
      "\n",
      "______\n",
      "\n",
      "versioning serial stamp:\n",
      "_8.33_735411028851\n",
      "\n",
      "Automunge returned train column set: \n",
      "['Pclass_nmbr', 'Sex_bnry', 'Age_newt', 'SibSp_nmbr', 'Parch_nmbr', 'Ticket_nmbr', 'Fare_nmbr', 'Pclass_NArw', 'Name_NArw', 'Name_hash_0', 'Name_hash_1', 'Name_hash_2', 'Name_hash_3', 'Name_hash_4', 'Name_hash_5', 'Name_hash_6', 'Name_hash_7', 'Name_hash_8', 'Name_hash_9', 'Name_hash_10', 'Name_hash_11', 'Name_hash_12', 'Name_hash_13', 'Sex_NArw', 'Age_NArw', 'SibSp_NArw', 'Parch_NArw', 'Ticket_NArw', 'Fare_NArw', 'Cabin_NArw', 'Cabin_1010_0', 'Cabin_1010_1', 'Cabin_1010_2', 'Cabin_1010_3', 'Cabin_1010_4', 'Cabin_1010_5', 'Cabin_1010_6', 'Cabin_1010_7', 'Embarked_NArw', 'Embarked_1010_0', 'Embarked_1010_1']\n",
      "\n",
      "Automunge returned ID column set: \n",
      "['PassengerId', 'Automunge_index']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['Survived_lbbn']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#having defined the functions, in order to pass them through an automunge(.) call\n",
    "#we then incorporate them into a simple data structure\n",
    "#these are consistent with the seperate tutorial notebook \"Custom Transformations\"\n",
    "\n",
    "#this is the transformation funcitons for a new trasnformatino category we'll call 'newt'\n",
    "processdict = \\\n",
    "{'newt' :\n",
    " {'custom_train' : custom_train_template,\n",
    "  'custom_test' : custom_test_template,\n",
    "  'custom_inversion' : custom_inversion_template,\n",
    "  'functionpointer' : 'nmbr',\n",
    " }}\n",
    "\n",
    "#here we define a family tree for use of 'newt' as a root category\n",
    "transformdict = \\\n",
    "{'newt' :\n",
    " {'parents'       : [],\n",
    "  'siblings'      : [],\n",
    "  'auntsuncles'   : ['newt'],\n",
    "  'cousins'       : ['NArw'],\n",
    "  'children'      : [],\n",
    "  'niecesnephews' : [],\n",
    "  'coworkers'     : [],\n",
    "  'friends'       : [],\n",
    " }}\n",
    "\n",
    "#now we assign 'newt' to a column\n",
    "targetcolumn = 'Age'\n",
    "assigncat = {'newt' : targetcolumn}\n",
    "\n",
    "#optionally, if we want to pass parameters to the funcitons, we can use assignparam\n",
    "assignparam = \\\n",
    "{'newt' : \n",
    " {targetcolumn :\n",
    "  {'multiplier' : 10}}}\n",
    "\n",
    "\n",
    "#we can then put all together in an automunge(.) call\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(\n",
    "  df_train,\n",
    "  labels_column = 'Survived',\n",
    "  trainID_column = trainID_column,\n",
    "  shuffletrain=False,\n",
    "  assigncat = assigncat,\n",
    "  assignparam = assignparam,\n",
    "  processdict = processdict,\n",
    "  transformdict = transformdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bc7bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1.022554\n",
       "1      8.070384\n",
       "2      1.250681\n",
       "3      6.365458\n",
       "4      6.365458\n",
       "         ...   \n",
       "886    1.818989\n",
       "887   -2.727479\n",
       "888   -8.853846\n",
       "889    1.250681\n",
       "890    4.660532\n",
       "Name: Age_newt, Length: 891, dtype: float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is the column that was target for the custom transformation\n",
    "train['Age_newt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8de2a",
   "metadata": {},
   "source": [
    "We can then download the saved postprocess_dict which can be later used to consistently prepare additional data in a new notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15349ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample code to download postprocess_dict dictionary returned from automunge(.)\n",
    "import pickle\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "  pickle.dump(postprocess_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c1154",
   "metadata": {},
   "source": [
    "### To simulate a new notebook we can now reset this session with 'Esc 00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf7dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the purpose of this section is to demonstrate that in a new notebook\\\n",
    "#because we applied custome defined transfomration functions in our automunge(.) call\n",
    "#we will need to reinitialize those functions prior to uplaoding with pickle\n",
    "#here we reinitialize those custome functions \n",
    "#to avoid the need for reinitializing such functions, \n",
    "#refer to the dill demonstration provided further below\n",
    "\n",
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#now we define our custom transformation functions\n",
    "#here we will define three functions for training data, tests data, and an inversino funciton\n",
    "#these are consistent with the seperate tutorial notebook \"Custom Transformations\"\n",
    "\n",
    "\n",
    "def custom_train_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #Template for a custom_train transformation function to be applied to a train feature set.\n",
    "  #further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the application of z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  \n",
    "  #which can accept any kind of numeric data \n",
    "  #so corresponding NArowtype processdict entry can be 'numeric'\n",
    "  #and returns a single column of continuous numeric data \n",
    "  #so corresponding MLinfilltype processdict entry will need to be 'numeric'\n",
    "\n",
    "  #where we'll include the option for a parameter 'muiltiplier'\n",
    "  #which is an arbitrary example to demonstrate accessing parameters\n",
    "  \n",
    "  #basically we check if that parameter had been passed in assignparam or defaultparams\n",
    "  if 'multiplier' in normalization_dict:\n",
    "    multiplier = normalization_dict['multiplier']\n",
    "    \n",
    "  #or otherwise assign and save a default value\n",
    "  else:\n",
    "    multiplier = 1\n",
    "    normalization_dict.update({'multiplier' : multiplier})\n",
    "\n",
    "  #Now we measure any properties of the train data used for the transformation\n",
    "  mean = df[column].mean()\n",
    "  stdev = df[column].std()\n",
    "  \n",
    "  #It's good practice to ensure numbers used in derivation haven't been derived as nan\n",
    "  #or would result in dividing by zero\n",
    "  if mean != mean:\n",
    "    mean = 0\n",
    "  if stdev != stdev or stdev == 0:\n",
    "    stdev = 1\n",
    "    \n",
    "  #In general if that same basis will be needed to process test data we'll store in normalization_dict\n",
    "  normalization_dict.update({'mean' : mean,\n",
    "                             'stdev': stdev})\n",
    "\n",
    "  #Optionally we can measure additional drift stats for a postmunge driftreport\n",
    "  #we will also save those in the normalization_dict\n",
    "  minimum = df[column].min()\n",
    "  maximum = df[column].max()\n",
    "  normalization_dict.update({'minimum' : minimum,\n",
    "                             'maximum' : maximum})\n",
    "\n",
    "  #Now we can apply the transformation\n",
    "  \n",
    "  #The generic formula for z-score normalization is (x - mean) / stdev\n",
    "  #here we incorporate an additional variable as the multiplier parameter (defaults to 1)\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df, normalization_dict\n",
    "\n",
    "def custom_test_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #This transform will be applied to a test data feature set\n",
    "  #on a basis of a corresponding custom_train entry\n",
    "  #Such as test data passed to either automunge(.) or postmunge(.)\n",
    "  #Using properties from the train set basis stored in the normalization_dict\n",
    "\n",
    "  #Further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the corresponding z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which was populated in a normalization_dict in the custom_train example given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #apply the transform\n",
    "  #and return the transformed dataframe\n",
    "\n",
    "  #access the train set properties from normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #then apply the transformation and return the dataframe\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def custom_inversion_template(df, returnedcolumn_list, inputcolumn, normalization_dict):\n",
    "  \"\"\"\n",
    "  #User also has the option to define a custom inversion function\n",
    "  #Corresponding to custom_train and custom_test\n",
    "  #furher detail on conventions provided in readme file\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here we'll be inverting the z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which corresponds to the examples given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #Initialize the new column inputcolumn\n",
    "  #And use values in the set from returnedcolumn_list to recover values for inputcolumn\n",
    "\n",
    "  #First let's access the values we'll need from the normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #Now initialize the inputcolumn\n",
    "  df[inputcolumn] = 0\n",
    "\n",
    "  #So for the example of z-score normalization, we know returnedcolumn_list will only have one entry\n",
    "  #In some other cases transforms may have returned multiple columns\n",
    "  returnedcolumn = returnedcolumn_list[0]\n",
    "\n",
    "  #now we perform the inversion\n",
    "  df[inputcolumn] = (df[returnedcolumn] * stdev / multiplier) + mean\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6940475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the functions are reinitialized,\n",
    "#we can upload our serialized postprocess_dict with pickle\n",
    "#note that if we had attempted this prior to initializing the functions\n",
    "#the pickle operaiton wouldn't be able to run\n",
    "\n",
    "#to upload for later use in postmunge(.) in another notebook\n",
    "import pickle\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "  postprocess_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dedd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can then prepare additional data in the new notebook with postmunge\n",
    "\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "test, test_ID, test_labels, \\\n",
    "postreports_dict \\\n",
    "= am.postmunge(postprocess_dict, \n",
    "               df_test,\n",
    "               printstatus=False,\n",
    "              testID_column=trainID_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d106a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6.081304\n",
       "1      13.185161\n",
       "2      21.709789\n",
       "3       1.818989\n",
       "4      -1.022554\n",
       "         ...    \n",
       "413     2.654403\n",
       "414     8.638692\n",
       "415     8.354538\n",
       "416     5.831248\n",
       "417   -11.599174\n",
       "Name: Age_newt, Length: 418, dtype: float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is the column with custom transform in the test data\n",
    "test['Age_newt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9880f7",
   "metadata": {},
   "source": [
    "# 3. demonstration of the integration of the hmac signature into a pickle workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aefa4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we revisit the workflow from example 1\n",
    "#but with the added integration of the derivation of an hmac signature with pickling\n",
    "#the purpose of the hmac signature is so that a different user\n",
    "#looking to upload a serialized postprocess_dict in a seperate notebook\n",
    "#can verify that the pickled object they have received is identical to the original source\n",
    "#which may have security benfits\n",
    "#note that for this operation to be secure\n",
    "#the derived hmac signature may also need to be transmitted to the second user\n",
    "#in some more secure channel than with the serialized postprocess_dict\n",
    "#with corresponding extension of this demonstration for a simple comparison verificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71575373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Automunge\n",
      "\n",
      "______\n",
      "\n",
      "versioning serial stamp:\n",
      "_8.33_918885590673\n",
      "\n",
      "Automunge returned train column set: \n",
      "['Pclass_nmbr', 'Sex_bnry', 'Age_nmbr', 'SibSp_nmbr', 'Parch_nmbr', 'Ticket_nmbr', 'Fare_nmbr', 'Pclass_NArw', 'Name_NArw', 'Name_hash_0', 'Name_hash_1', 'Name_hash_2', 'Name_hash_3', 'Name_hash_4', 'Name_hash_5', 'Name_hash_6', 'Name_hash_7', 'Name_hash_8', 'Name_hash_9', 'Name_hash_10', 'Name_hash_11', 'Name_hash_12', 'Name_hash_13', 'Sex_NArw', 'Age_NArw', 'SibSp_NArw', 'Parch_NArw', 'Ticket_NArw', 'Fare_NArw', 'Cabin_NArw', 'Cabin_1010_0', 'Cabin_1010_1', 'Cabin_1010_2', 'Cabin_1010_3', 'Cabin_1010_4', 'Cabin_1010_5', 'Cabin_1010_6', 'Cabin_1010_7', 'Embarked_NArw', 'Embarked_1010_0', 'Embarked_1010_1']\n",
      "\n",
      "Automunge returned ID column set: \n",
      "['PassengerId', 'Automunge_index']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['Survived_lbbn']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "#run automunge(.) to encode the dataframes and populate a postprocess_dict\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(df_train,\n",
    "             labels_column = labels_column,\n",
    "             trainID_column = trainID_column,\n",
    "             ML_cmnd = {'stochastic_impute_numeric' : False,\n",
    "                        'stochastic_impute_categoric' : False,\n",
    "                       },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "324d02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def download_postprocess_dict_with_hmac(postprocess_dict, filename):\n",
    "    # Create a new HMAC object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "\n",
    "    # Serialize the object\n",
    "    serialized_obj = pickle.dumps(postprocess_dict)\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object.update(serialized_obj)\n",
    "    signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Write the signature and the serialized object to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(signature.encode(\"ascii\"))\n",
    "        f.write(serialized_obj)\n",
    "        \n",
    "#     return signature\n",
    "\n",
    "signature = download_postprocess_dict_with_hmac(postprocess_dict, \"filename.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170735e",
   "metadata": {},
   "source": [
    "### To simulate a new notebook we can now reset this session with 'Esc 00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb87b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def upload_postprocess_dict_with_hmac_verification(filename):\n",
    "    # Load the serialized object and the signature\n",
    "    with open(filename, \"rb\") as f:\n",
    "        signature = f.readline(64).decode(\"ascii\")\n",
    "        serialized_obj = f.read()\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "    hmac_object.update(serialized_obj)\n",
    "    computed_signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Verify the signature\n",
    "    if signature != computed_signature:\n",
    "        raise Exception(\"The serialized object has been tampered with!\")\n",
    "\n",
    "    # Deserialize the object\n",
    "    postprocess_dict = pickle.loads(serialized_obj)\n",
    "\n",
    "    return postprocess_dict\n",
    "\n",
    "\n",
    "# Reinitialize the dictionary with HMAC signature inspection\n",
    "postprocess_dict = upload_postprocess_dict_with_hmac_verification(\"filename.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e57ce81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "\n",
    "# df_test\n",
    "\n",
    "\n",
    "#the uplaoded postprocess_dict is used as basis to encode any additional test data\n",
    "test, test_ID, test_labels, \\\n",
    "postreports_dict \\\n",
    "= am.postmunge(postprocess_dict, \n",
    "               df_test,\n",
    "               printstatus=False,\n",
    "              testID_column=trainID_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93d317",
   "metadata": {},
   "source": [
    "# 4. demonstration of the dill library as an alternative to pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bd8c9",
   "metadata": {},
   "source": [
    "We will find that the dill library can be very easily substituted for the pickle implmentation by simply directly replacing calls to pickle with dill. Although dill can read .pickle formatted files, it is probably better practice to encoded as .dill for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c65a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Automunge\n",
      "\n",
      "______\n",
      "\n",
      "versioning serial stamp:\n",
      "_8.33_208496506434\n",
      "\n",
      "Automunge returned train column set: \n",
      "['Pclass_nmbr', 'Sex_bnry', 'Age_nmbr', 'SibSp_nmbr', 'Parch_nmbr', 'Ticket_nmbr', 'Fare_nmbr', 'Pclass_NArw', 'Name_NArw', 'Name_hash_0', 'Name_hash_1', 'Name_hash_2', 'Name_hash_3', 'Name_hash_4', 'Name_hash_5', 'Name_hash_6', 'Name_hash_7', 'Name_hash_8', 'Name_hash_9', 'Name_hash_10', 'Name_hash_11', 'Name_hash_12', 'Name_hash_13', 'Sex_NArw', 'Age_NArw', 'SibSp_NArw', 'Parch_NArw', 'Ticket_NArw', 'Fare_NArw', 'Cabin_NArw', 'Cabin_1010_0', 'Cabin_1010_1', 'Cabin_1010_2', 'Cabin_1010_3', 'Cabin_1010_4', 'Cabin_1010_5', 'Cabin_1010_6', 'Cabin_1010_7', 'Embarked_NArw', 'Embarked_1010_0', 'Embarked_1010_1']\n",
      "\n",
      "Automunge returned ID column set: \n",
      "['PassengerId', 'Automunge_index']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['Survived_lbbn']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "#run automunge(.) to encode the dataframes and populate a postprocess_dict\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(df_train,\n",
    "             labels_column = labels_column,\n",
    "             trainID_column = trainID_column,\n",
    "             ML_cmnd = {'stochastic_impute_numeric' : False,\n",
    "                        'stochastic_impute_categoric' : False,\n",
    "                       },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522161a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def download_postprocess_dict_with_hmac(postprocess_dict, filename):\n",
    "    # Create a new HMAC object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "\n",
    "    # Serialize the object\n",
    "    serialized_obj = dill.dumps(postprocess_dict)\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object.update(serialized_obj)\n",
    "    signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Write the signature and the serialized object to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(signature.encode(\"ascii\"))\n",
    "        f.write(serialized_obj)\n",
    "        \n",
    "#     return signature\n",
    "\n",
    "signature = download_postprocess_dict_with_hmac(postprocess_dict, \"filename.dill\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcef86f",
   "metadata": {},
   "source": [
    "### To simulate a new notebook we can now reset this session with 'Esc 00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eb9c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dill\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def upload_postprocess_dict_with_hmac_verification(filename):\n",
    "    # Load the serialized object and the signature\n",
    "    with open(filename, \"rb\") as f:\n",
    "        signature = f.readline(64).decode(\"ascii\")\n",
    "        serialized_obj = f.read()\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "    hmac_object.update(serialized_obj)\n",
    "    computed_signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Verify the signature\n",
    "    if signature != computed_signature:\n",
    "        raise Exception(\"The serialized object has been tampered with!\")\n",
    "\n",
    "    # Deserialize the object\n",
    "    postprocess_dict = dill.loads(serialized_obj)\n",
    "\n",
    "    return postprocess_dict\n",
    "\n",
    "\n",
    "# Reinitialize the dictionary with HMAC signature inspection\n",
    "postprocess_dict = upload_postprocess_dict_with_hmac_verification(\"filename.dill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "935e64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload for later use in postmunge(.) in another notebook\n",
    "\n",
    "\n",
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "\n",
    "# df_test\n",
    "\n",
    "\n",
    "#the uplaoded postprocess_dict is used as basis to encode any additional test data\n",
    "test, test_ID, test_labels, \\\n",
    "postreports_dict \\\n",
    "= am.postmunge(postprocess_dict, \n",
    "               df_test,\n",
    "               printstatus=False,\n",
    "              testID_column=trainID_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11bf7e2",
   "metadata": {},
   "source": [
    "# 5. demonstration of the dill library (in conjunction with hmac signature) with custom defined transformation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e6d57",
   "metadata": {},
   "source": [
    "This demonstration is similar to that in section 3. of this notebook, the purpose is to demonstrate that by using the dill library instead of pickle, we gain ability to upload the serialized postprocess_dict dictionary without having to re-initialize any custom trasnformation functions, which will have their full definitions saved in the dill encoded dictionary as opposed to pickle which only stores pointers. (This is why it mayt be more important to incorproate a hmac signiature into this workflow.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa061628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0330053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we define our custom transformatino functions\n",
    "#here we will define three functions for training data, tests data, and an inversino funciton\n",
    "#these are consistent with the seperate tutorial notebook \"Custom Transformations\"\n",
    "\n",
    "\n",
    "def custom_train_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #Template for a custom_train transformation function to be applied to a train feature set.\n",
    "  #further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the application of z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  \n",
    "  #which can accept any kind of numeric data \n",
    "  #so corresponding NArowtype processdict entry can be 'numeric'\n",
    "  #and returns a single column of continuous numeric data \n",
    "  #so corresponding MLinfilltype processdict entry will need to be 'numeric'\n",
    "\n",
    "  #where we'll include the option for a parameter 'muiltiplier'\n",
    "  #which is an arbitrary example to demonstrate accessing parameters\n",
    "  \n",
    "  #basically we check if that parameter had been passed in assignparam or defaultparams\n",
    "  if 'multiplier' in normalization_dict:\n",
    "    multiplier = normalization_dict['multiplier']\n",
    "    \n",
    "  #or otherwise assign and save a default value\n",
    "  else:\n",
    "    multiplier = 1\n",
    "    normalization_dict.update({'multiplier' : multiplier})\n",
    "\n",
    "  #Now we measure any properties of the train data used for the transformation\n",
    "  mean = df[column].mean()\n",
    "  stdev = df[column].std()\n",
    "  \n",
    "  #It's good practice to ensure numbers used in derivation haven't been derived as nan\n",
    "  #or would result in dividing by zero\n",
    "  if mean != mean:\n",
    "    mean = 0\n",
    "  if stdev != stdev or stdev == 0:\n",
    "    stdev = 1\n",
    "    \n",
    "  #In general if that same basis will be needed to process test data we'll store in normalization_dict\n",
    "  normalization_dict.update({'mean' : mean,\n",
    "                             'stdev': stdev})\n",
    "\n",
    "  #Optionally we can measure additional drift stats for a postmunge driftreport\n",
    "  #we will also save those in the normalization_dict\n",
    "  minimum = df[column].min()\n",
    "  maximum = df[column].max()\n",
    "  normalization_dict.update({'minimum' : minimum,\n",
    "                             'maximum' : maximum})\n",
    "\n",
    "  #Now we can apply the transformation\n",
    "  \n",
    "  #The generic formula for z-score normalization is (x - mean) / stdev\n",
    "  #here we incorporate an additional variable as the multiplier parameter (defaults to 1)\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df, normalization_dict\n",
    "\n",
    "def custom_test_template(df, column, normalization_dict):\n",
    "  \"\"\"\n",
    "  #This transform will be applied to a test data feature set\n",
    "  #on a basis of a corresponding custom_train entry\n",
    "  #Such as test data passed to either automunge(.) or postmunge(.)\n",
    "  #Using properties from the train set basis stored in the normalization_dict\n",
    "\n",
    "  #Further detail on conventions provided in readme file\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here is the corresponding z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which was populated in a normalization_dict in the custom_train example given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #apply the transform\n",
    "  #and return the transformed dataframe\n",
    "\n",
    "  #access the train set properties from normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #then apply the transformation and return the dataframe\n",
    "  df[column] = (df[column] - mean) * multiplier / stdev\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def custom_inversion_template(df, returnedcolumn_list, inputcolumn, normalization_dict):\n",
    "  \"\"\"\n",
    "  #User also has the option to define a custom inversion function\n",
    "  #Corresponding to custom_train and custom_test\n",
    "  #furher detail on conventions provided in readme file\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #As an example, here we'll be inverting the z-score normalization \n",
    "  #derived based on the training set mean and standard deviation\n",
    "  #which corresponds to the examples given above\n",
    "\n",
    "  #Basically the workflow is we access any values needed from the normalization_dict\n",
    "  #Initialize the new column inputcolumn\n",
    "  #And use values in the set from returnedcolumn_list to recover values for inputcolumn\n",
    "\n",
    "  #First let's access the values we'll need from the normalization_dict\n",
    "  mean = normalization_dict['mean']\n",
    "  stdev = normalization_dict['stdev']\n",
    "  multiplier = normalization_dict['multiplier']\n",
    "\n",
    "  #Now initialize the inputcolumn\n",
    "  df[inputcolumn] = 0\n",
    "\n",
    "  #So for the example of z-score normalization, we know returnedcolumn_list will only have one entry\n",
    "  #In some other cases transforms may have returned multiple columns\n",
    "  returnedcolumn = returnedcolumn_list[0]\n",
    "\n",
    "  #now we perform the inversion\n",
    "  df[inputcolumn] = (df[returnedcolumn] * stdev / multiplier) + mean\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a04775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Automunge\n",
      "\n",
      "______\n",
      "\n",
      "versioning serial stamp:\n",
      "_8.33_531125869863\n",
      "\n",
      "Automunge returned train column set: \n",
      "['Pclass_nmbr', 'Sex_bnry', 'Age_newt', 'SibSp_nmbr', 'Parch_nmbr', 'Ticket_nmbr', 'Fare_nmbr', 'Pclass_NArw', 'Name_NArw', 'Name_hash_0', 'Name_hash_1', 'Name_hash_2', 'Name_hash_3', 'Name_hash_4', 'Name_hash_5', 'Name_hash_6', 'Name_hash_7', 'Name_hash_8', 'Name_hash_9', 'Name_hash_10', 'Name_hash_11', 'Name_hash_12', 'Name_hash_13', 'Sex_NArw', 'Age_NArw', 'SibSp_NArw', 'Parch_NArw', 'Ticket_NArw', 'Fare_NArw', 'Cabin_NArw', 'Cabin_1010_0', 'Cabin_1010_1', 'Cabin_1010_2', 'Cabin_1010_3', 'Cabin_1010_4', 'Cabin_1010_5', 'Cabin_1010_6', 'Cabin_1010_7', 'Embarked_NArw', 'Embarked_1010_0', 'Embarked_1010_1']\n",
      "\n",
      "Automunge returned ID column set: \n",
      "['PassengerId', 'Automunge_index']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['Survived_lbbn']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#having defined the functions, in order to pass them through an automunge(.) call\n",
    "#we then incorporate them into a simple data structure\n",
    "#these are consistent with the seperate tutorial notebook \"Custom Transformations\"\n",
    "#further detail on conventions provided in readme file\n",
    "\n",
    "#this is the transformation funcitons for a new trasnformatino category we'll call 'newt'\n",
    "processdict = \\\n",
    "{'newt' :\n",
    " {'custom_train' : custom_train_template,\n",
    "  'custom_test' : custom_test_template,\n",
    "  'custom_inversion' : custom_inversion_template,\n",
    "  'functionpointer' : 'nmbr',\n",
    " }}\n",
    "\n",
    "#here we define a family tree for use of 'newt' as a root category\n",
    "transformdict = \\\n",
    "{'newt' :\n",
    " {'parents'       : [],\n",
    "  'siblings'      : [],\n",
    "  'auntsuncles'   : ['newt'],\n",
    "  'cousins'       : ['NArw'],\n",
    "  'children'      : [],\n",
    "  'niecesnephews' : [],\n",
    "  'coworkers'     : [],\n",
    "  'friends'       : [],\n",
    " }}\n",
    "\n",
    "#now we assign 'newt' to a column\n",
    "targetcolumn = 'Age'\n",
    "assigncat = {'newt' : targetcolumn}\n",
    "\n",
    "#optionally, if we want to pass parameters to the funcitons, we can use assignparam\n",
    "assignparam = \\\n",
    "{'newt' : \n",
    " {targetcolumn :\n",
    "  {'multiplier' : 10}}}\n",
    "\n",
    "\n",
    "#we can then put all together in an automunge(.) call\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(\n",
    "  df_train,\n",
    "  labels_column = 'Survived',\n",
    "  trainID_column = trainID_column,\n",
    "  shuffletrain=False,\n",
    "  assigncat = assigncat,\n",
    "  assignparam = assignparam,\n",
    "  processdict = processdict,\n",
    "  transformdict = transformdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8ba6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def download_postprocess_dict_with_hmac(postprocess_dict, filename):\n",
    "    # Create a new HMAC object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "\n",
    "    # Serialize the object\n",
    "    serialized_obj = dill.dumps(postprocess_dict)\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object.update(serialized_obj)\n",
    "    signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Write the signature and the serialized object to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(signature.encode(\"ascii\"))\n",
    "        f.write(serialized_obj)\n",
    "        \n",
    "#     return signature\n",
    "\n",
    "signature = download_postprocess_dict_with_hmac(postprocess_dict, \"filename.dill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119ebf6",
   "metadata": {},
   "source": [
    "### To simulate a new notebook we can now reset this session with 'Esc 00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e203a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dill\n",
    "import hmac\n",
    "\n",
    "#drafting of this function supported by Bard LLM service\n",
    "def upload_postprocess_dict_with_hmac_verification(filename):\n",
    "    # Load the serialized object and the signature\n",
    "    with open(filename, \"rb\") as f:\n",
    "        signature = f.readline(64).decode(\"ascii\")\n",
    "        serialized_obj = f.read()\n",
    "\n",
    "    # Compute the signature of the serialized object\n",
    "    hmac_object = hmac.new(b\"secret_key\", digestmod=\"sha256\")\n",
    "    hmac_object.update(serialized_obj)\n",
    "    computed_signature = hmac_object.hexdigest()\n",
    "\n",
    "    # Verify the signature\n",
    "    if signature != computed_signature:\n",
    "        raise Exception(\"The serialized object has been tampered with!\")\n",
    "\n",
    "    # Deserialize the object\n",
    "    postprocess_dict = dill.loads(serialized_obj)\n",
    "\n",
    "    return postprocess_dict\n",
    "\n",
    "\n",
    "# Reinitialize the dictionary with HMAC signature inspection\n",
    "postprocess_dict = upload_postprocess_dict_with_hmac_verification(\"filename.dill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35301522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload for later use in postmunge(.) in another notebook\n",
    "\n",
    "\n",
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#titanic set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'\n",
    "\n",
    "\n",
    "# df_test\n",
    "\n",
    "\n",
    "#the uplaoded postprocess_dict is used as basis to encode any additional test data\n",
    "test, test_ID, test_labels, \\\n",
    "postreports_dict \\\n",
    "= am.postmunge(postprocess_dict, \n",
    "               df_test,\n",
    "               printstatus=False,\n",
    "              testID_column=trainID_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "864bb0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voila"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
