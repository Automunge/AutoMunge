{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "CRJNTuOaLzug",
   "metadata": {
    "id": "CRJNTuOaLzug"
   },
   "source": [
    "# Automunge integration with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c84a6",
   "metadata": {
    "id": "7a7c84a6"
   },
   "source": [
    "The Automunge library can be thought of as an extension of the Pandas dataframe library. As such, any external library with build in Pandas support should be capable of direct integration in a push-button manner. In this notebook we demonstrate the inttegratino of Automunge preprocessing into a Spark workflow.\n",
    "\n",
    "Spark is a library from the Apache team that in some sense is capable of more sophisticated forms of data processing than Pandas from the standpoint that it can be applied to streaming data in a distributed environment. In this notebook we will provide demonstrations making use of PySpark which is their python based API.\n",
    "\n",
    "The easiest way to integrate Pandas functions into a PySpark workflow is probably to do so in a maner that makes use of their native pyspark.pandas module, and in this notebook we will demonstrate using a pandas_on_spark.apply_batch() function on a Spark dataframe, which can be thought of as a method to apply pandas functions to a Spark streaming session in a manner that processes batches of data at a time.\n",
    "\n",
    "This demonstration will not be a fully optimized implementation, for example we understand that Spark may benefit from additional forms of specification on returned data types etc, this notebook can be thought of a simple demonstration of integrate data preprocessing with the Automunge API directly into a Spark session as a kind of proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ff300fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ff300fa",
    "outputId": "863f3bcb-3dc2-4086-d5f4-03e8014d6f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Automunge in /usr/local/lib/python3.10/dist-packages (8.33)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Automunge) (1.23.5)\n",
      "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from Automunge) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from Automunge) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from Automunge) (1.11.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->Automunge) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->Automunge) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->Automunge) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Automunge) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->Automunge) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->Automunge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#install Automunge if not yet installed:\n",
    "!pip install Automunge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e975a02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e975a02",
    "outputId": "9abc8c84-6179-4b2d-fbdd-3841ee7cbaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "#install pyspark if not yet installed:\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13a0cd3d",
   "metadata": {
    "id": "13a0cd3d"
   },
   "outputs": [],
   "source": [
    "#This notebook also assumes that java is installed\n",
    "#if not ready to install on your local session\n",
    "#this notebook can be run in an online Colab notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d899b321",
   "metadata": {
    "id": "d899b321"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from Automunge import *\n",
    "am = AutoMunge()\n",
    "\n",
    "import pyspark.pandas as ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1070dce8",
   "metadata": {
    "id": "1070dce8"
   },
   "outputs": [],
   "source": [
    "#we'll also assume the availability of a dataset\n",
    "#here we'll rely on Titanic set, a simple tabular benchmark\n",
    "#Titanic set can be accessed from Kaggle among other places\n",
    "#we'll assume the 'train.csv' and 'test.csv' files\n",
    "#are in the same folder as this notebook\n",
    "#and the conventions of train data set having addition label and ID columns\n",
    "\n",
    "#titanic set\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#Automunge convention to accomodate different/extra columns in train versus test set\n",
    "labels_column = 'Survived'\n",
    "trainID_column = 'PassengerId'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4f731",
   "metadata": {
    "id": "15d4f731"
   },
   "source": [
    "The workflow to prepare streams of data in spark would be as follows:\n",
    "- identify the in memory scale training corpus to be used to establish a basis\n",
    "- run that training corpus through automunge() as a pandas dataframe\n",
    "- this will result in an encoded training dataframe and a populated postprocess_dict which can be used to prepare additional data\n",
    "- can either then translate the training corpus to a spark dataframe if you like or otherwise just use to traing a supervised learning model in whatever convention is desired\n",
    "- then for streams of additional test data channeled through spark can encode on a consistent basis (eg for inference) with apply_batch and postmunge\n",
    "- where to pass parameters to postmunge we could create a wrapper function that accepts a dataframe as input and internally has a set of parameters to pass to postmunge\n",
    "- note that a small deviation may be appropriate for postmunge, since apply_batch appears to rely on an assumptino that the  pandas function returns a dataframe, the sets of dataframes returned from postmunge() by default will need to be translated to a single dataframe\n",
    "- we could either thus return a single dataframe from the wrapper function or alternatively could make use of the postmunge returnedsets parameter which adjusts the postmunge() function to return a few different scenarios of returned set configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95fa57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "be95fa57",
    "outputId": "9dc5da2a-e743-447c-d1d1-490897bfdaa5"
   },
   "outputs": [],
   "source": [
    "#here we populate a postproccess_dict using a pandas df_train set as basis\n",
    "#where postprocess_dict cabn be used as a key to consistently prepare additional data\n",
    "#using postmunge(.) function\n",
    "\n",
    "#although this operation could be sped up by omitting MLinfill\n",
    "#the postmunge latency is less sensitive to ML infill since is just running inference\n",
    "\n",
    "#the returned sets (like train and labels) could bhe used to train a model\n",
    "\n",
    "train, train_ID, labels, \\\n",
    "val, val_ID, val_labels, \\\n",
    "test, test_ID, test_labels, \\\n",
    "postprocess_dict = \\\n",
    "am.automunge(df_train,\n",
    "             labels_column = labels_column,\n",
    "             trainID_column = trainID_column,\n",
    "             MLinfill = True,\n",
    "             shuffletrain = False,\n",
    "             printstatus=False,\n",
    "             )\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "s6rzkEId59om",
   "metadata": {
    "id": "s6rzkEId59om"
   },
   "outputs": [],
   "source": [
    "#in order to apply postmunge(.) to encode Spark dataframes\n",
    "#one way to simplify the operation is to prepare a wrapper function\n",
    "#which handles postmunge parameters internally\n",
    "#and returns a single dataframe\n",
    "\n",
    "# here is the postmunge wrapper function\n",
    "\n",
    "#first here is a support function for data type conversions\n",
    "def pd_dtype_convert(df):\n",
    "\n",
    "    #since spark doesn't support uint data types\n",
    "    #and since automunge defaults to uint for ordinal type encodings\n",
    "    #we will convert any uint to int in the wrapper function\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        if str(df[column].dtype) == 'uint8':\n",
    "            df[column] = df[column].astype('int16')\n",
    "\n",
    "        elif str(df[column].dtype) == 'uint16':\n",
    "            df[column] = df[column].astype('int32')\n",
    "\n",
    "        elif str(df[column].dtype) == 'uint32':\n",
    "            df[column] = df[column].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "#now here is the postmunge wrapper function for spark's apply_batch()\n",
    "#where postmunge is used to encode additional data on a training data basis\n",
    "\n",
    "def postmunge_wrapper_for_spark_ab(pandas_df):\n",
    "  #returns a pandas dataframe encoded by postmunge\n",
    "  #assumes a postprocess_dict is available in memory\n",
    "  #(python allows you to access dictionaries in a function\n",
    "  #even when not explicitly passed to the function)\n",
    "\n",
    "  #here are the postmunge parameters\n",
    "  #normally we would defer to defaults when not specified\n",
    "  #however since this is for a wrapper function\n",
    "  #let's go ahead an make explicit specifications\n",
    "\n",
    "  testID_column = False\n",
    "  pandasoutput = 'dataframe'\n",
    "  printstatus = False #'summary'\n",
    "  inplace = True #False\n",
    "  dupl_rows = False\n",
    "  TrainLabelFreqLevel = False\n",
    "  featureeval = False\n",
    "  traindata = False\n",
    "  noise_augment = 0\n",
    "  driftreport = False\n",
    "  inversion = False\n",
    "  # returnedsets = True #specified below\n",
    "  shuffletrain = False\n",
    "  entropy_seeds = False\n",
    "  random_generator = False\n",
    "  sampling_dict = False\n",
    "  randomseed = False\n",
    "  encrypt_key = False\n",
    "  logger = {}\n",
    "\n",
    "  #refer to readme docs on postmunge returnedsets parameter\n",
    "  #for this scenario where postmunge returns a single dataframe\n",
    "  test = \\\n",
    "  am.postmunge(postprocess_dict,\n",
    "  pandas_df,\n",
    "  returnedsets = False, #this resutls in postmunge returning single set\n",
    "  testID_column = testID_column,\n",
    "  pandasoutput = pandasoutput,\n",
    "  printstatus = printstatus,\n",
    "  inplace = inplace,\n",
    "  dupl_rows = dupl_rows,\n",
    "  TrainLabelFreqLevel = TrainLabelFreqLevel,\n",
    "  featureeval = featureeval,\n",
    "  traindata = traindata,\n",
    "  noise_augment = noise_augment,\n",
    "  driftreport = driftreport,\n",
    "  inversion = inversion,\n",
    "  # returnedsets = returnedsets,\n",
    "  shuffletrain = shuffletrain,\n",
    "  entropy_seeds = entropy_seeds,\n",
    "  random_generator = random_generator,\n",
    "  sampling_dict = sampling_dict,\n",
    "  randomseed = randomseed,\n",
    "  encrypt_key = encrypt_key,\n",
    "  logger = logger,\n",
    "  )\n",
    "\n",
    "  test = pd_dtype_convert(test)\n",
    "\n",
    "  return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18f083",
   "metadata": {
    "id": "df18f083"
   },
   "outputs": [],
   "source": [
    "#then encoding Spark DataFrames\n",
    "#can be demonstrated be creating a test dataset in spark\n",
    "#we can use the same Titanic test set to demonstrate\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"My Spark App\").getOrCreate()\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "# spark_df_test = spark.createDataFrame(df_test)\n",
    "spark_df_test = spark.read.option(\"header\", True).csv(\"test.csv\")\n",
    "\n",
    "spark_df_test = ps.DataFrame(spark_df_test)\n",
    "\n",
    "print(spark_df_test.head())\n",
    "\n",
    "test_sparkdf_encoded = spark_df_test.pandas_on_spark.apply_batch(postmunge_wrapper_for_spark_ab)\n",
    "\n",
    "# print(test_sparkdf_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a6d32f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a6d32f7",
    "outputId": "de87123f-d0ba-4536-881f-b598f90732a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_sparkdf_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "lZtYS6ZxDSus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "lZtYS6ZxDSus",
    "outputId": "3d9bf45d-2c63-49f9-f1c0-237ef719b5ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_nmbr</th>\n",
       "      <th>Sex_bnry</th>\n",
       "      <th>Age_nmbr</th>\n",
       "      <th>SibSp_nmbr</th>\n",
       "      <th>Parch_nmbr</th>\n",
       "      <th>Ticket_nmbr</th>\n",
       "      <th>Fare_nmbr</th>\n",
       "      <th>Pclass_NArw</th>\n",
       "      <th>Name_NArw</th>\n",
       "      <th>Name_hash_0</th>\n",
       "      <th>Name_hash_1</th>\n",
       "      <th>Name_hash_2</th>\n",
       "      <th>Name_hash_3</th>\n",
       "      <th>Name_hash_4</th>\n",
       "      <th>Name_hash_5</th>\n",
       "      <th>Name_hash_6</th>\n",
       "      <th>Name_hash_7</th>\n",
       "      <th>Name_hash_8</th>\n",
       "      <th>Name_hash_9</th>\n",
       "      <th>Name_hash_10</th>\n",
       "      <th>Name_hash_11</th>\n",
       "      <th>Name_hash_12</th>\n",
       "      <th>Name_hash_13</th>\n",
       "      <th>Sex_NArw</th>\n",
       "      <th>Age_NArw</th>\n",
       "      <th>SibSp_NArw</th>\n",
       "      <th>Parch_NArw</th>\n",
       "      <th>Ticket_NArw</th>\n",
       "      <th>Fare_NArw</th>\n",
       "      <th>Cabin_NArw</th>\n",
       "      <th>Cabin_1010_0</th>\n",
       "      <th>Cabin_1010_1</th>\n",
       "      <th>Cabin_1010_2</th>\n",
       "      <th>Cabin_1010_3</th>\n",
       "      <th>Cabin_1010_4</th>\n",
       "      <th>Cabin_1010_5</th>\n",
       "      <th>Cabin_1010_6</th>\n",
       "      <th>Cabin_1010_7</th>\n",
       "      <th>Embarked_NArw</th>\n",
       "      <th>Embarked_1010_0</th>\n",
       "      <th>Embarked_1010_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.826913</td>\n",
       "      <td>1</td>\n",
       "      <td>0.330491</td>\n",
       "      <td>-0.474279</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>0.149684</td>\n",
       "      <td>-0.490508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>1004</td>\n",
       "      <td>494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.826913</td>\n",
       "      <td>0</td>\n",
       "      <td>1.190988</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>0.218302</td>\n",
       "      <td>-0.507194</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>437</td>\n",
       "      <td>538</td>\n",
       "      <td>494</td>\n",
       "      <td>71</td>\n",
       "      <td>581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.369157</td>\n",
       "      <td>1</td>\n",
       "      <td>2.223584</td>\n",
       "      <td>-0.474279</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>-0.042498</td>\n",
       "      <td>-0.453112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>716</td>\n",
       "      <td>1004</td>\n",
       "      <td>585</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.826913</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.185806</td>\n",
       "      <td>-0.474279</td>\n",
       "      <td>-0.473408</td>\n",
       "      <td>0.116273</td>\n",
       "      <td>-0.473739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1004</td>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.826913</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.530005</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>0.767199</td>\n",
       "      <td>6.024011</td>\n",
       "      <td>-0.400792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706</td>\n",
       "      <td>538</td>\n",
       "      <td>849</td>\n",
       "      <td>292</td>\n",
       "      <td>519</td>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_nmbr  Sex_bnry  Age_nmbr  SibSp_nmbr  Parch_nmbr  Ticket_nmbr  Fare_nmbr  Pclass_NArw  Name_NArw  Name_hash_0  Name_hash_1  Name_hash_2  Name_hash_3  Name_hash_4  Name_hash_5  Name_hash_6  Name_hash_7  Name_hash_8  Name_hash_9  Name_hash_10  Name_hash_11  Name_hash_12  Name_hash_13  Sex_NArw  Age_NArw  SibSp_NArw  Parch_NArw  Ticket_NArw  Fare_NArw  Cabin_NArw  Cabin_1010_0  Cabin_1010_1  Cabin_1010_2  Cabin_1010_3  Cabin_1010_4  Cabin_1010_5  Cabin_1010_6  Cabin_1010_7  Embarked_NArw  Embarked_1010_0  Embarked_1010_1\n",
       "0     0.826913         1  0.330491   -0.474279   -0.473408     0.149684  -0.490508            0          0          344         1004          494            0            0            0            0            0            0            0             0             0             0             0         0         0           0           0            0          0           1             0             0             0             0             0             0             0             0              0                1                0\n",
       "1     0.826913         0  1.190988    0.432550   -0.473408     0.218302  -0.507194            0          0          437          538          494           71          581            0            0            0            0            0             0             0             0             0         0         0           0           0            0          0           1             0             0             0             0             0             0             0             0              0                1                1\n",
       "2    -0.369157         1  2.223584   -0.474279   -0.473408    -0.042498  -0.453112            0          0          716         1004          585          504            0            0            0            0            0            0             0             0             0             0         0         0           0           0            0          0           1             0             0             0             0             0             0             0             0              0                1                0\n",
       "3     0.826913         1 -0.185806   -0.474279   -0.473408     0.116273  -0.473739            0          0          108         1004          826            0            0            0            0            0            0            0             0             0             0             0         0         0           0           0            0          0           1             0             0             0             0             0             0             0             0              0                1                1\n",
       "4     0.826913         0 -0.530005    0.432550    0.767199     6.024011  -0.400792            0          0          706          538          849          292          519          921            0            0            0            0             0             0             0             0         0         0           0           0            0          0           1             0             0             0             0             0             0             0             0              0                1                1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sparkdf_encoded.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
