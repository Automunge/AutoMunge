"""
This file is part of Automunge which is released under GNU General Public License v3.0.
See file LICENSE or go to https://github.com/Automunge/AutoMunge for full license details.

contact available via automunge.com

Copyright (C) 2018, 2019, 2020 Nicholas Teague - All Rights Reserved

patent pending, application 16552857

"""


#global imports
import numpy as np
import pandas as pd
from copy import deepcopy

#imports for process_time_class, postprocess_time_class
import datetime as dt

#imports for process_bxcx_class
from scipy import stats

#imports for process_hldy_class
from pandas.tseries.holiday import USFederalHolidayCalendar

#imports for evalcategory, getNArows
import collections
import datetime as dt
from scipy.stats import shapiro
from scipy.stats import skew

#imports for predictinfill, predictpostinfill, trainFSmodel
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
# from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
#stats may be used for cases where user elects RandomSearchCV hyperparameter tuning
# from scipy import stats

#imports for shuffleaccuracy
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_log_error

#imports for PCA dimensionality reduction
from sklearn.decomposition import PCA
from sklearn.decomposition import SparsePCA
from sklearn.decomposition import KernelPCA

#imports for automunge
import random
#import datetime as dt
import types



class AutoMunge:
  
  def __init__(self):
    pass

  def assembletransformdict(self, binstransform, NArw_marker):
    '''
    #assembles the range of transformations to be applied based on the evaluated \
    #category of data
    #the primitives are intented as follows:
    #_greatgrandparents_: supplemental column derived from source column, only applied
    #to first generation, with downstream transforms included
    #_grandparents_: supplemental column derived from source column, only applied
    #to first generation
    #_parents_: replace source column, with downstream trasnforms performed
    #_siblings_: supplemental column derived from source column, \
    #with downstream transforms performed
    #_auntsuncles_: replace source column, without downstream transforms performed
    #_cousins_: supplemental column derived from source column, \
    #without downstream transforms performed
    #downstream transform primitives are:
    #_children_: becomes downstream parent
    #_niecenephews_: treated like a downstream sibling
    #_coworkers_: becomes a downstream auntsuncles
    #_friends_: become downstream cousins    
    
    #for example, if we set 'bxcx' entry to have both 'bxcx' as parents and \
    #'nmbr' as cousin, then the output would be column_nmbr, column_bxcx_nmbr
    #(because 'bxcx' has a downstream primitive entry of 'nmbr' as well 
    
    #note a future extension will allow automunge class to run experiments
    #on different configurations of trasnform_dict to improve the feature selection
    '''

    transform_dict = {}

    #initialize bins based on what was passed through application of automunge(.)
    if binstransform is True:
      bins = 'bins'
      bint = 'bint'
    else:
      bins = None
      bint = None
        
    if NArw_marker is True:
      NArw = 'NArw'
    else:
      NArw = None

    #initialize trasnform_dict. Note in a future extension the range of categories
    #is intended to be built out
    transform_dict.update({'nmbr' : {'parents' : ['nmbr'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : [bint]}})
    
    transform_dict.update({'dxdt' : {'parents' : ['dxdt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'d2dt' : {'parents' : ['d2dt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['dxdt'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'d3dt' : {'parents' : ['d3dt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d2dt'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d4dt' : {'parents' : ['d4dt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d3dt'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d5dt' : {'parents' : ['d5dt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d4dt'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d6dt' : {'parents' : ['d6dt'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d5dt'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'dxd2' : {'parents' : ['dxd2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'d2d2' : {'parents' : ['d2d2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['dxd2'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'d3d2' : {'parents' : ['d3d2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d2d2'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d4d2' : {'parents' : ['d4d2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d3d2'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d5d2' : {'parents' : ['d5d2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d4d2'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'d6d2' : {'parents' : ['d6d2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['d5d2'], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'nmdx' : {'parents' : ['nmdx'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['dxdt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'nmd2' : {'parents' : ['nmd2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['d2dt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'nmd3' : {'parents' : ['nmd3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['d3dt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'nmd4' : {'parents' : ['nmd4'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['d4dt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'nmd5' : {'parents' : ['nmd5'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['d5dt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})

    transform_dict.update({'nmd6' : {'parents' : ['nmd6'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['d6dt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['retn'], \
                                     'friends' : []}})
    
    transform_dict.update({'mmdx' : {'parents' : ['mmdx'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})
    
    transform_dict.update({'mmd2' : {'parents' : ['mmd2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['mmdx'], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})
    
    transform_dict.update({'mmd3' : {'parents' : ['mmd3'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['mmd2'], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})

    transform_dict.update({'mmd4' : {'parents' : ['mmd4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['mmd3'], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})

    transform_dict.update({'mmd5' : {'parents' : ['mmd5'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['mmd4'], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})

    transform_dict.update({'mmd6' : {'parents' : ['mmd6'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nbr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['mmd5'], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})
    
    transform_dict.update({'dddt' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dddt', 'exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ddd2' : {'parents' : ['ddd2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['dddt'], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ddd3' : {'parents' : ['ddd3'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ddd2'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ddd4' : {'parents' : ['ddd4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ddd3'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ddd5' : {'parents' : ['ddd5'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ddd4'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ddd6' : {'parents' : ['ddd6'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ddd5'], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dedt' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dedt', 'exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ded2' : {'parents' : ['ded2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['dedt'], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ded3' : {'parents' : ['ded3'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ded2'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ded4' : {'parents' : ['ded4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ded3'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ded5' : {'parents' : ['ded5'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ded4'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'ded6' : {'parents' : ['ded6'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['ded5'], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'bnry' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnry'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnr2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'text' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['text'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'txt2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['text'], \
                                     'cousins' : [NArw, 'splt'], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'txt3' : {'parents' : ['txt3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['text'], \
                                     'friends' : []}})

    transform_dict.update({'lngt' : {'parents' : ['lngt'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['mnmx'], \
                                     'friends' : []}})
  
    transform_dict.update({'lnlg' : {'parents' : ['lnlg'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['log0'], \
                                     'friends' : []}})

    transform_dict.update({'UPCS' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['UPCS'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'Utxt' : {'parents' : ['Utxt'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['text'], \
                                     'friends' : []}})
    
    transform_dict.update({'Utx2' : {'parents' : ['Utx2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['text'], \
                                     'friends' : ['splt']}})

    transform_dict.update({'Utx3' : {'parents' : ['Utx3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['txt3'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'Ucct' : {'parents' : ['Ucct'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ucct', 'ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'Uord' : {'parents' : ['Uord'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ordl'], \
                                     'friends' : []}})
        
    transform_dict.update({'Uor2' : {'parents' : ['Uor2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['ord2'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'Uor3' : {'parents' : ['Uor3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'Uor6' : {'parents' : ['Uor6'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['spl6'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'U101' : {'parents' : ['U101'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
    
    transform_dict.update({'splt' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['splt'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'spl2' : {'parents' : ['spl2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'spl3' : {'parents' : ['spl2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'spl4' : {'parents' : ['spl4'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['spl3'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'spl5' : {'parents' : ['spl5'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'spl6' : {'parents' : ['spl6'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['splt'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : ['ord3']}})
    
    transform_dict.update({'spl7' : {'parents' : ['spl7'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})

    transform_dict.update({'spl8' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['spl8'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'spl9' : {'parents' : ['spl9'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})

    transform_dict.update({'sp10' : {'parents' : ['sp10'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    
    transform_dict.update({'sp11' : {'parents' : ['sp11'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['spl5'], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'sp12' : {'parents' : ['sp12'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['sp11'], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'sp13' : {'parents' : ['sp13'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['sp10'], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'sp14' : {'parents' : ['sp14'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : ['sp13'], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'sp15' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['sp15'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'sp16' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['sp16'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'srch' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['srch'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'src2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['src2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'src3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['src3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'src4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['src4'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'strn' : {'parents' : ['strn'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'nmrc' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmrc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmr2' : {'parents' : ['nmr2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmr3' : {'parents' : ['nmr3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'nmr4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmr4'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmr5' : {'parents' : ['nmr5'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmr6' : {'parents' : ['nmr6'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmr7' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmr7'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmr8' : {'parents' : ['nmr8'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmr9' : {'parents' : ['nmr9'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmcm' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmcm'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmc2' : {'parents' : ['nmc2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmc3' : {'parents' : ['nmc3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'nmc4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmc4'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmc5' : {'parents' : ['nmc5'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmc6' : {'parents' : ['nmc6'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'nmc7' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmc7'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'nmc8' : {'parents' : ['nmc8'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nmc9' : {'parents' : ['nmc9'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['mnmx'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ors7' : {'parents' : ['spl6', 'nmr2'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ors5' : {'parents' : ['spl5'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ors6' : {'parents' : ['spl6'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ordl' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ordl'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
        
    transform_dict.update({'ord2' : {'parents' : ['ord2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['mnmx'], \
                                     'friends' : []}})
    
    transform_dict.update({'ord3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ucct' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ucct'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
        
    transform_dict.update({'ord4' : {'parents' : ['ord4'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['mnmx'], \
                                     'friends' : []}})
    
    transform_dict.update({'ors2' : {'parents' : ['spl3'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'or10' : {'parents' : ['ord4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['mnmx'], \
                                     'friends' : []}})
    
    transform_dict.update({'or11' : {'parents' : ['sp11'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'or12' : {'parents' : ['nmr2'], \
                                     'siblings': ['sp11'], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'or13' : {'parents' : ['sp12'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'or14' : {'parents' : ['nmr2'], \
                                     'siblings': ['sp12'], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'or15' : {'parents' : ['or15'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['sp13'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
  
    transform_dict.update({'or16' : {'parents' : ['or16'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmr2'], \
                                     'niecesnephews' : ['sp13'], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
    
    transform_dict.update({'or17' : {'parents' : ['or17'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['sp14'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
    
    transform_dict.update({'or18' : {'parents' : ['or18'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmr2'], \
                                     'niecesnephews' : ['sp14'], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})

    transform_dict.update({'or19' : {'parents' : ['or19'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmc8'], \
                                     'niecesnephews' : ['sp13'], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
    
    transform_dict.update({'or20' : {'parents' : ['or20'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmc8'], \
                                     'niecesnephews' : ['sp14'], \
                                     'coworkers' : ['1010'], \
                                     'friends' : []}})
    
    transform_dict.update({'om10' : {'parents' : ['ord4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010', 'mnmx'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['mnmx'], \
                                     'friends' : []}})

    transform_dict.update({'mmor' : {'parents' : ['ord4'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'1010' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'null' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['null'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'NArw' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['NArw'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'NAr2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['NAr2'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'NAr3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['NAr3'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'NAr4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['NAr4'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'NAr5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['NAr5'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nbr2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['nmbr'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nbr3' : {'parents' : ['nbr3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : ['bint']}})
    
    transform_dict.update({'MADn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['MADn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'MAD2' : {'parents' : ['MAD2'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'MAD3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['MAD3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnmx' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm2' : {'parents' : ['nmbr'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm3' : {'parents' : ['nmbr'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnm3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnm3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx'], \
                                     'cousins' : ['nmbr', NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm6' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnm6'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnm7' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx', 'bins'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'retn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['retn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'mean' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mean'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mea2' : {'parents' : ['nmbr'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mean'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mea3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mean', 'bins'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'date' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mnth', 'days', 'hour', 'mint', 'scnd'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'dat2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bshr', 'wkdy', 'hldy'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dat3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mnsn', 'mncs', 'dysn', 'dycs', 'hrsn', 'hrcs', 'misn', 'mics', 'scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dat4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs', 'hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dat5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs', 'dysn', 'dycs', 'hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dat6' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs', 'hmss', 'hmsc', 'bshr', 'wkdy', 'hldy'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'year' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'yea2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnth' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnth'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'mnt2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnsn', 'mncs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnt3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnsn', 'mncs', 'dysn', 'dycs', 'hrsn', 'hrcs', 'misn', 'mics', 'scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnt4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mdsn', 'mdcs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnt5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mdsn', 'mdcs', 'hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnt6' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mdsn', 'mdcs', 'dysn', 'dycs', 'hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mnsn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnsn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mncs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mncs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mdsn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mdsn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mdcs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mdcs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'days' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['days'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'day2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dysn', 'dycs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'day3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dysn', 'dycs', 'hrsn', 'hrcs', 'misn', 'mics', 'scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'day4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dhms', 'dhmc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'day5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dhms', 'dhmc', 'hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dysn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dysn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dycs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dycs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dhms' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dhms'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'dhmc' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['dhmc'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hour' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hour'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'hrs2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hrsn', 'hrcs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hrs3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hrsn', 'hrcs', 'misn', 'mics', 'scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hrs4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hmss', 'hmsc'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hrsn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hrsn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hrcs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hrcs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hmss' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hmss'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hmsc' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hmsc'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mint' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mint'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'min2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['misn', 'mics'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'min3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['misn', 'mics', 'scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'min4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mssn', 'mscs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'misn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['misn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mics' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mics'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mssn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mssn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mscs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mscs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'scnd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['scnd'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'scn2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['scsn', 'sccs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'scsn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['scsn'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'sccs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['sccs'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bxcx' : {'parents' : ['bxcx'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['nmbr'], \
                                     'friends' : []}})
    
    transform_dict.update({'bxc2' : {'parents' : ['bxc2'], \
                                     'siblings': ['nmbr'], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bxc3' : {'parents' : ['bxc3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : ['nmbr'], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bxc4' : {'parents' : ['bxc4'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['nbr2'], \
                                     'friends' : []}})

    transform_dict.update({'bxc5' : {'parents' : ['bxc5'], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mnmx'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['nbr2', 'bins'], \
                                     'friends' : []}})
    
    transform_dict.update({'pwrs' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['pwrs'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'pwr2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['pwr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'log0' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['log0'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'log1' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['log0', 'pwr2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'logn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['logn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'lgnm' : {'parents' : ['lgnm'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['nmbr'], \
                                     'friends' : []}})
    
    transform_dict.update({'sqrt' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['sqrt'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'addd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['addd'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'sbtr' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['sbtr'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'mltp' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['mltp'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'divd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['divd'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'rais' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['rais'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'absl' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['absl'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bkt1' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bkt1'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bkt2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bkt2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bkt3' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bkt3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bkt4' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bkt4'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'wkdy' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['wkdy'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bshr' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bshr'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'hldy' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['hldy'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'wkds' : {'parents' : ['wkds'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['text'], \
                                     'friends' : []}})
  
    transform_dict.update({'wkdo' : {'parents' : ['wkdo'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'mnts' : {'parents' : ['mnts'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['text'], \
                                     'friends' : []}})
  
    transform_dict.update({'mnto' : {'parents' : ['mnto'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : ['ord3'], \
                                     'friends' : []}})
    
    transform_dict.update({'bins' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bins'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bint' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bint'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bsor' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bsor'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnwd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnwd'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnwK' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnwK'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'bnwM' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnwM'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnwo' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnwo'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
  
    transform_dict.update({'bnKo' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnKo'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnMo' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnMo'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})    
    
    transform_dict.update({'bnep' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnep'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bne7' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bne7'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bne9' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bne9'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bneo' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bneo'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bn7o' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bn7o'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'tlbn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['tlbn'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'bn9o' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bn9o'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'pwor' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['pwor'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'por2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['por2'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'copy' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['copy'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'excl' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : ['excl'], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'exc2' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'exc3' : {'parents' : ['exc3'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : ['bins']}})
    
    transform_dict.update({'exc4' : {'parents' : ['exc4'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : ['pwr2']}})
    
    transform_dict.update({'exc5' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc5'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'exc6' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc6'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'shfl' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['shfl'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'nmbd' : {'parents' : ['nmbr'], \
                                     'siblings': [], \
                                     'auntsuncles' : [], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : [bint]}})

    transform_dict.update({'101d' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['1010'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'ordd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'texd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['text'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'bnrd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnry'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'datd' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs', 'hmss', 'hmsc', 'bshr', 'wkdy', 'hldy'], \
                                     'cousins' : [NArw], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'nuld' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['null'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'lbnm' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['exc2'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})

    transform_dict.update({'lb10' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['text'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'lbor' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['ord3'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'lbte' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['text'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'lbbn' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['bnry'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})
    
    transform_dict.update({'lbda' : {'parents' : [], \
                                     'siblings': [], \
                                     'auntsuncles' : ['year', 'mdsn', 'mdcs', 'hmss', 'hmsc', 'bshr', 'wkdy', 'hldy'], \
                                     'cousins' : [], \
                                     'children' : [], \
                                     'niecesnephews' : [], \
                                     'coworkers' : [], \
                                     'friends' : []}})


    return transform_dict
  

  
  
  def assembleprocessdict(self):
    '''
    #creates a dictionary storing all of the processing functions for each
    #category. Note that the convention is that every dualprocess entry 
    #(to process both train and text set in automunge) is meant
    #to have a coresponding postprocess entry (to process the test set in 
    #postmunge). If the dualprocess/postprocess pair aren't included a 
    #singleprocess funciton will be instead which processes a single column
    #at a time and is neutral to whether that set is from train or test data.
    
    #starting in version 1.79, this also stores entries for 'NArowtype' and
    #'MLinfilltype', which were added to facilitate user definition of 
    #custom processing functions
    
    #NArowtype entries are:
    # - 'numeric' for source columns with expected numeric entries
    # - 'integer' for source column with expected integer entries
    # - 'justNaN' for source columns that may have expected entries other than numeric
    # - 'exclude' for source columns that aren't needing NArow columns derived
    # - 'positivenumeric' for source columns with expected positive numeric entries
    # - 'nonnegativenumeric' for source columns with expected non-nbegative numeric (zero allowed)
    # - 'nonzeronumeric' for source columns with allowed postiive and negative but no zero
    # - 'parsenumeric' marks for infill strings that don't contain any numeric character entries
    # - 'parsenumeric_commas' marks for infill strings that don't contain any numeric character entries, recognizes commas
    # - 'datetime' marks for infill cells that arent' recognized as datetime objects
    
    #MLinfilltype entries are:
    # - 'numeric' for single columns with numeric entries (such as could be floats)
    # - 'singlct' for single column sets with ordinal entries (integers)
    # - 'binary' for single column sets with boolean entries (0/1)
    # - 'multirt' for categorical multicolumn sets with boolean entries (0/1)
    # - 'multisp' for bins multicolumn sets with boolean entries
    #(the two (rt/sp) are treated differently in labelfrequencylevelizer, where
    # multisp are elligeble to serve as basis for levelizing a numerical set via bins)
    # - 'concurrent_act' for multicolumn sets with boolean entries as may have 
    #multiple entries in the same row
    # - 'concurrent_nmbr' for multicolumn sets with numerical entries
    # - 'exclude' for columns which will be excluded from ML infill
    # - '1010' for binary encoded columns, will be converted to onehot for ML
    # - 'boolexclude' boolean set suitable for Binary transform but exluded from MLinfill
    '''
    
    process_dict = {}
    
    #categories are nmbr, bnry, text, date, bxcx, bins, bint, NArw, null
    #note a future extension will allow the definition of new categories 
    #to automunge

    #dual column functions
    process_dict.update({'nmbr' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'inverseprocess' : self.inverseprocess_nmbr, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'dxdt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d2dt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d3dt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d4dt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d5dt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d6dt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'dxd2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d2d2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d3d2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d4d2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d5d2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'d6d2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmdx' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmd2' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmd3' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmd4' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmd5' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'nmd6' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'mmdx' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'mmd2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'mmd3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'mmd4' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'mmd5' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'mmd6' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'dddt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'ddd2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'ddd3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'ddd4' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'ddd5' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'ddd6' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxdt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxdt'}})
    process_dict.update({'dedt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'ded2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'ded3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'ded4' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'ded5' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'ded6' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_dxd2_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dxd2'}})
    process_dict.update({'nbr2' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'inverseprocess' : self.inverseprocess_nmbr, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nbr3' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'inverseprocess' : self.inverseprocess_nmbr, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'MADn' : {'dualprocess' : self.process_MADn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_MADn_class, \
                                  'inverseprocess' : self.inverseprocess_MADn, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'MADn'}})
    process_dict.update({'MAD2' : {'dualprocess' : self.process_MADn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_MADn_class, \
                                  'inverseprocess' : self.inverseprocess_MADn, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'MADn'}})
    process_dict.update({'MAD3' : {'dualprocess' : self.process_MAD3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_MAD3_class, \
                                  'inverseprocess' : self.inverseprocess_MAD3, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'MAD3'}})
    process_dict.update({'mnmx' : {'dualprocess' : self.process_mnmx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnmx_class, \
                                  'inverseprocess' : self.inverseprocess_mnmx, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'mnm2' : {'dualprocess' : self.process_mnmx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnmx_class, \
                                  'inverseprocess' : self.inverseprocess_mnmx, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'mnm3' : {'dualprocess' : self.process_mnm3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnm3_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnm3'}})
    process_dict.update({'mnm4' : {'dualprocess' : self.process_mnm3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnm3_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnm3'}})
    process_dict.update({'mnm5' : {'dualprocess' : self.process_mnmx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnmx_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'mnm6' : {'dualprocess' : self.process_mnm6_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnm6_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnm6'}})
    process_dict.update({'mnm7' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnm7'}})
    process_dict.update({'retn' : {'dualprocess' : self.process_retn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_retn_class, \
                                  'inverseprocess' : self.inverseprocess_retn, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'retn'}})
    process_dict.update({'mean' : {'dualprocess' : self.process_mean_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mean_class, \
                                  'inverseprocess' : self.inverseprocess_mean, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mean'}})
    process_dict.update({'mea2' : {'dualprocess' : self.process_mean_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mean_class, \
                                  'inverseprocess' : self.inverseprocess_mean, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mean'}})
    process_dict.update({'mea3' : {'dualprocess' : self.process_mean_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mean_class, \
                                  'inverseprocess' : self.inverseprocess_mean, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mean'}})
    process_dict.update({'bnry' : {'dualprocess' : self.process_binary_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_binary_class, \
                                  'inverseprocess' : self.inverseprocess_bnry, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'bnry'}})
    process_dict.update({'bnr2' : {'dualprocess' : self.process_binary2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_binary2_class, \
                                  'inverseprocess' : self.inverseprocess_bnry, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'bnr2'}})
    process_dict.update({'text' : {'dualprocess' : self.process_text_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_text_class, \
                                  'inverseprocess' : self.inverseprocess_text, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'txt2' : {'dualprocess' : self.process_text_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_text_class, \
                                  'inverseprocess' : self.inverseprocess_text, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'txt3' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'lngt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_lngt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'lnlg' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_lngt_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'log0'}})
    process_dict.update({'UPCS' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_UPCS, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'exclude'}})
    process_dict.update({'Utxt' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'Utx2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'Utx3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'Ucct' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'ucct'}})
    process_dict.update({'Uord' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'ordl'}})
    process_dict.update({'Uor2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'Uor3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'Uor6' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'U101' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_UPCS_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : '1010'}})
    process_dict.update({'splt' : {'dualprocess' : self.process_splt_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_splt_class, \
                                  'inverseprocess' : self.inverseprocess_splt, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'splt'}})
    process_dict.update({'spl2' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ordl'}})
    process_dict.update({'spl3' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'spl4' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'spl5' : {'dualprocess' : self.process_spl5_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl5_class, \
                                  'inverseprocess' : self.inverseprocess_spl5, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'spl6' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'spl7' : {'dualprocess' : self.process_spl7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl7_class, \
                                  'inverseprocess' : self.inverseprocess_spl5, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'spl8' : {'dualprocess' : self.process_spl8_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl8_class, \
                                  'inverseprocess' : self.inverseprocess_spl8, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'spl9' : {'dualprocess' : self.process_spl9_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl9_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ordl'}})
    process_dict.update({'sp10' : {'dualprocess' : self.process_sp10_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sp10_class, \
                                  'inverseprocess' : self.inverseprocess_spl5, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'sp11' : {'dualprocess' : self.process_spl2_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_spl2_class, \
                                   'inverseprocess' : self.inverseprocess_spl2, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'sp12' : {'dualprocess' : self.process_spl2_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_spl2_class, \
                                   'inverseprocess' : self.inverseprocess_spl2, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'sp13' : {'dualprocess' : self.process_spl9_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_spl9_class, \
                                   'inverseprocess' : self.inverseprocess_spl2, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'sp14' : {'dualprocess' : self.process_spl9_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_spl9_class, \
                                   'inverseprocess' : self.inverseprocess_spl2, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'sp15' : {'dualprocess' : self.process_sp15_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sp15_class, \
                                  'inverseprocess' : self.inverseprocess_sp15, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'concurrent_act', \
                                  'labelctgy' : 'sp15'}})
    process_dict.update({'sp16' : {'dualprocess' : self.process_sp16_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sp16_class, \
                                  'inverseprocess' : self.inverseprocess_sp16, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'concurrent_act', \
                                  'labelctgy' : 'sp16'}})
    process_dict.update({'srch' : {'dualprocess' : self.process_srch_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_srch_class, \
                                   'inverseprocess' : self.inverseprocess_srch, \
                                   'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'concurrent_act', \
                                  'labelctgy' : 'srch'}})
    process_dict.update({'src2' : {'dualprocess' : self.process_src2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_src2_class, \
                                   'inverseprocess' : self.inverseprocess_src2, \
                                   'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'concurrent_act', \
                                  'labelctgy' : 'src2'}})
    process_dict.update({'src3' : {'dualprocess' : self.process_src3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_src3_class, \
                                   'inverseprocess' : self.inverseprocess_src3, \
                                   'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'concurrent_act', \
                                  'labelctgy' : 'src3'}})
    process_dict.update({'src4' : {'dualprocess' : self.process_src4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_src4_class, \
                                   'inverseprocess' : self.inverseprocess_src4, \
                                   'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'src4'}})
    process_dict.update({'strn' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_strn_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'nmrc' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmrc_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmrc'}})
    process_dict.update({'nmr2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmrc_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmr3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmrc_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'nmr4' : {'dualprocess' : self.process_nmr4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmrc'}})
    process_dict.update({'nmr5' : {'dualprocess' : self.process_nmr4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmr6' : {'dualprocess' : self.process_nmr4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'nmr7' : {'dualprocess' : self.process_nmr7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmrc'}})
    process_dict.update({'nmr8' : {'dualprocess' : self.process_nmr7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmr9' : {'dualprocess' : self.process_nmr7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmr7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'nmcm' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmcm_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric_commas', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmcm'}})
    process_dict.update({'nmc2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmcm_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric_commas', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmc3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_nmcm_class, \
                                  'postprocess' : None, \
                                  'inverseprocess' : self.inverseprocess_nmrc, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'parsenumeric_commas', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'nmc4' : {'dualprocess' : self.process_nmc4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmrc'}})
    process_dict.update({'nmc5' : {'dualprocess' : self.process_nmc4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmc6' : {'dualprocess' : self.process_nmc4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc4_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'nmc7' : {'dualprocess' : self.process_nmc7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmrc'}})
    process_dict.update({'nmc8' : {'dualprocess' : self.process_nmc7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'nmc9' : {'dualprocess' : self.process_nmc7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_nmc7_class, \
                                  'NArowtype' : 'parsenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'ors7' : {'dualprocess' : self.process_spl5_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl5_class, \
                                  'inverseprocess' : self.inverseprocess_sp15, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'ors5' : {'dualprocess' : self.process_spl5_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl5_class, \
                                  'inverseprocess' : self.inverseprocess_sp15, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'ors6' : {'dualprocess' : self.process_spl5_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl5_class, \
                                  'inverseprocess' : self.inverseprocess_sp15, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'ordl' : {'dualprocess' : self.process_ordl_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ordl_class, \
                                  'inverseprocess' : self.inverseprocess_ordl, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ordl'}})
    process_dict.update({'ord2' : {'dualprocess' : self.process_ordl_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ordl_class, \
                                  'inverseprocess' : self.inverseprocess_ordl, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'ord3' : {'dualprocess' : self.process_ord3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ord3_class, \
                                  'inverseprocess' : self.inverseprocess_ord3, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'ucct' : {'dualprocess' : self.process_ucct_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ucct_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'ucct'}})
    process_dict.update({'ord4' : {'dualprocess' : self.process_ord3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ord3_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'ors2' : {'dualprocess' : self.process_spl2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_spl2_class, \
                                  'inverseprocess' : self.inverseprocess_spl2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'or10' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'or11' : {'dualprocess' : self.process_1010_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_1010_class, \
                                   'inverseprocess' : self.inverseprocess_1010, \
                                   'info_retention' : True, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : '1010', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or12' : {'dualprocess' : self.process_1010_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_1010_class, \
                                   'inverseprocess' : self.inverseprocess_1010, \
                                   'info_retention' : True, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : '1010', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or13' : {'dualprocess' : self.process_1010_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_1010_class, \
                                   'inverseprocess' : self.inverseprocess_1010, \
                                   'info_retention' : True, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : '1010', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or14' : {'dualprocess' : self.process_1010_class, \
                                   'singleprocess' : None, \
                                   'postprocess' : self.postprocess_1010_class, \
                                   'inverseprocess' : self.inverseprocess_1010, \
                                   'info_retention' : True, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : '1010', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or15' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or16' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or17' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or18' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or19' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'or20' : {'dualprocess' : None, \
                                   'singleprocess' : self.process_UPCS_class, \
                                   'postprocess' : None, \
                                   'inverseprocess' : self.inverseprocess_UPCS, \
                                   'info_retention' : False, \
                                   'NArowtype' : 'justNaN', \
                                   'MLinfilltype' : 'exclude', \
                                   'labelctgy' : 'ord3'}})
    process_dict.update({'om10' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'mmor' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnmx'}})
    process_dict.update({'1010' : {'dualprocess' : self.process_1010_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_1010_class, \
                                  'inverseprocess' : self.inverseprocess_1010, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : '1010', \
                                  'labelctgy' : '1010'}})
    process_dict.update({'bxcx' : {'dualprocess' : self.process_bxcx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bxcx_class, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'date' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mnth'}})
    process_dict.update({'dat2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'hldy'}})
    process_dict.update({'dat3' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mnsn'}})
    process_dict.update({'dat4' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'dat5' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'dat6' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'year' : {'dualprocess' : self.process_year_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_year_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'year'}})
    process_dict.update({'yea2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'year'}})
    process_dict.update({'mnth' : {'dualprocess' : self.process_mnth_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnth_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnth'}})
    process_dict.update({'mnt2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnsn'}})
    process_dict.update({'mnt3' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnsn'}})
    process_dict.update({'mnt4' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'mnt5' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'mnt6' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'mnsn' : {'dualprocess' : self.process_mnsn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mnsn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mnsn'}})
    process_dict.update({'mncs' : {'dualprocess' : self.process_mncs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mncs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mncs'}})
    process_dict.update({'mdsn' : {'dualprocess' : self.process_mdsn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mdsn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'mdcs' : {'dualprocess' : self.process_mdcs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mdcs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mdcs'}})
    process_dict.update({'days' : {'dualprocess' : self.process_days_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_days_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'days'}})
    process_dict.update({'day2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dysn'}})
    process_dict.update({'day3' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dysn'}})
    process_dict.update({'day4' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dhms'}})
    process_dict.update({'day5' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dhms'}})
    process_dict.update({'dysn' : {'dualprocess' : self.process_dysn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_dysn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dysn'}})
    process_dict.update({'dycs' : {'dualprocess' : self.process_dycs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_dycs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dycs'}})
    process_dict.update({'dhms' : {'dualprocess' : self.process_dhms_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_dhms_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dhms'}})
    process_dict.update({'dhmc' : {'dualprocess' : self.process_dhmc_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_dhmc_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'dhmc'}})
    process_dict.update({'hour' : {'dualprocess' : self.process_hour_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_hour_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hour'}})
    process_dict.update({'hrs2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hrsn'}})
    process_dict.update({'hrs3' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hrsn'}})
    process_dict.update({'hrs4' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hmss'}})
    process_dict.update({'hrsn' : {'dualprocess' : self.process_hrsn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_hrsn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hrsn'}})
    process_dict.update({'hrcs' : {'dualprocess' : self.process_hrcs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_hrcs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hrcs'}})
    process_dict.update({'hmss' : {'dualprocess' : self.process_hmss_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_hmss_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hmss'}})
    process_dict.update({'hmsc' : {'dualprocess' : self.process_hmsc_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_hmsc_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'hmsc'}})
    process_dict.update({'mint' : {'dualprocess' : self.process_mint_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mint_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mint'}})
    process_dict.update({'min2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'misn'}})
    process_dict.update({'min3' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'misn'}})
    process_dict.update({'min4' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'misn'}})
    process_dict.update({'misn' : {'dualprocess' : self.process_misn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_misn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'misn'}})
    process_dict.update({'mics' : {'dualprocess' : self.process_mics_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mics_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mics'}})
    process_dict.update({'mssn' : {'dualprocess' : self.process_mssn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mssn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mssn'}})
    process_dict.update({'mscs' : {'dualprocess' : self.process_mscs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mscs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mscs'}})
    process_dict.update({'scnd' : {'dualprocess' : self.process_scnd_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_scnd_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'scnd'}})
    process_dict.update({'scn2' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'scsn'}})
    process_dict.update({'scsn' : {'dualprocess' : self.process_scsn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_scsn_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'scsn'}})
    process_dict.update({'sccs' : {'dualprocess' : self.process_sccs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sccs_class, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'sccs'}})
    process_dict.update({'bxc2' : {'dualprocess' : self.process_bxcx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bxcx_class, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'bxc3' : {'dualprocess' : self.process_bxcx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bxcx_class, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'bxc4' : {'dualprocess' : self.process_bxcx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bxcx_class, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'bxc5' : {'dualprocess' : self.process_bxcx_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bxcx_class, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'pwrs' : {'dualprocess' : self.process_pwrs_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_pwrs_class, \
                                  'inverseprocess' : self.inverseprocess_pwrs, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'pwrs'}})
    process_dict.update({'pwr2' : {'dualprocess' : self.process_pwr2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_pwr2_class, \
                                  'inverseprocess' : self.inverseprocess_pwr2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'nonzeronumeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'pwrs'}})
    process_dict.update({'log0' : {'dualprocess' : self.process_log0_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_log0_class, \
                                  'inverseprocess' : self.inverseprocess_log0, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'log0'}})
    process_dict.update({'log1' : {'dualprocess' : self.process_log0_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_log0_class, \
                                  'inverseprocess' : self.inverseprocess_log0, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'log0'}})
    process_dict.update({'logn' : {'dualprocess' : self.process_logn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_logn_class, \
                                  'inverseprocess' : self.inverseprocess_logn, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'logn'}})
    process_dict.update({'lgnm' : {'dualprocess' : self.process_logn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_logn_class, \
                                  'inverseprocess' : self.inverseprocess_logn, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'sqrt' : {'dualprocess' : self.process_sqrt_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sqrt_class, \
                                  'inverseprocess' : self.inverseprocess_sqrt, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'nonnegativenumeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'sqrt'}})
    process_dict.update({'addd' : {'dualprocess' : self.process_addd_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_addd_class, \
                                  'inverseprocess' : self.inverseprocess_addd, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'addd'}})
    process_dict.update({'sbtr' : {'dualprocess' : self.process_sbtr_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_sbtr_class, \
                                  'inverseprocess' : self.inverseprocess_sbtr, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'sbtr'}})
    process_dict.update({'mltp' : {'dualprocess' : self.process_mltp_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_mltp_class, \
                                  'inverseprocess' : self.inverseprocess_mltp, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'mltp'}})
    process_dict.update({'divd' : {'dualprocess' : self.process_divd_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_divd_class, \
                                  'inverseprocess' : self.inverseprocess_divd, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'divd'}})
    process_dict.update({'rais' : {'dualprocess' : self.process_rais_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_rais_class, \
                                  'inverseprocess' : self.inverseprocess_rais, \
                                  'info_retention' : True, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'rais'}})
    process_dict.update({'absl' : {'dualprocess' : self.process_absl_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_absl_class, \
                                  'inverseprocess' : self.inverseprocess_absl, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'absl'}})
    process_dict.update({'bkt1' : {'dualprocess' : self.process_bkt1_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bkt1_class, \
                                  'inverseprocess' : self.inverseprocess_bkt1, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bkt1'}})
    process_dict.update({'bkt2' : {'dualprocess' : self.process_bkt2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bkt2_class, \
                                  'inverseprocess' : self.inverseprocess_bkt2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bkt2'}})
    process_dict.update({'bkt3' : {'dualprocess' : self.process_bkt3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bkt3_class, \
                                  'inverseprocess' : self.inverseprocess_bkt3, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bkt3'}})
    process_dict.update({'bkt4' : {'dualprocess' : self.process_bkt4_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bkt4_class, \
                                  'inverseprocess' : self.inverseprocess_bkt4, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bkt4'}})
    process_dict.update({'wkdy' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_wkdy_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'wkdy'}})
    process_dict.update({'bshr' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_bshr_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'bshr'}})
    process_dict.update({'hldy' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_hldy_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'hldy'}})
    process_dict.update({'wkds' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_wkds_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'wkdo' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_wkds_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'mnts' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_mnts_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'mnto' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_mnts_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    
    process_dict.update({'bins' : {'dualprocess' : self.process_bins_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bins_class, \
                                  'inverseprocess' : self.inverseprocess_bins, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bins'}})
    process_dict.update({'bint' : {'dualprocess' : self.process_bint_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bint_class, \
                                  'inverseprocess' : self.inverseprocess_bint, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bint'}})
    process_dict.update({'bsor' : {'dualprocess' : self.process_bsor_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bsor_class, \
                                  'inverseprocess' : self.inverseprocess_bsor, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bsor'}})
    process_dict.update({'bnwd' : {'dualprocess' : self.process_bnwd_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnwd_class, \
                                  'inverseprocess' : self.inverseprocess_bnwd, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bnwd'}})
    process_dict.update({'bnwK' : {'dualprocess' : self.process_bnwK_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnwK_class, \
                                  'inverseprocess' : self.inverseprocess_bnwK, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bnwK'}})
    process_dict.update({'bnwM' : {'dualprocess' : self.process_bnwM_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnwM_class, \
                                  'inverseprocess' : self.inverseprocess_bnwM, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bnwM'}})
    process_dict.update({'bnwo' : {'dualprocess' : self.process_bnwo_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnwo_class, \
                                  'inverseprocess' : self.inverseprocess_bnwo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bnwo'}})
    process_dict.update({'bnKo' : {'dualprocess' : self.process_bnKo_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnKo_class, \
                                  'inverseprocess' : self.inverseprocess_bnwo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bnwK'}})
    process_dict.update({'bnMo' : {'dualprocess' : self.process_bnMo_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnMo_class, \
                                  'inverseprocess' : self.inverseprocess_bnwo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bnwM'}})
    process_dict.update({'bnep' : {'dualprocess' : self.process_bnep_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bnep_class, \
                                  'inverseprocess' : self.inverseprocess_bnep, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bnep'}})
    process_dict.update({'bne7' : {'dualprocess' : self.process_bne7_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bne7_class, \
                                  'inverseprocess' : self.inverseprocess_bnep, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bne7'}})
    process_dict.update({'bne9' : {'dualprocess' : self.process_bne9_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bne9_class, \
                                  'inverseprocess' : self.inverseprocess_bnep, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'multisp', \
                                  'labelctgy' : 'bne9'}})
    process_dict.update({'bneo' : {'dualprocess' : self.process_bneo_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bneo_class, \
                                  'inverseprocess' : self.inverseprocess_bneo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bneo'}})
    process_dict.update({'bn7o' : {'dualprocess' : self.process_bn7o_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bn7o_class, \
                                  'inverseprocess' : self.inverseprocess_bneo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bn7o'}})
    process_dict.update({'bn9o' : {'dualprocess' : self.process_bn9o_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_bn9o_class, \
                                  'inverseprocess' : self.inverseprocess_bneo, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'bn9o'}})
    process_dict.update({'tlbn' : {'dualprocess' : self.process_tlbn_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_tlbn_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'concurrent_nmbr', \
                                  'labelctgy' : 'tlbn'}})
    process_dict.update({'pwor' : {'dualprocess' : self.process_pwor_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_pwor_class, \
                                  'inverseprocess' : self.inverseprocess_pwor, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'pwor'}})
    process_dict.update({'por2' : {'dualprocess' : self.process_por2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_por2_class, \
                                  'inverseprocess' : self.inverseprocess_por2, \
                                  'info_retention' : False, \
                                  'NArowtype' : 'nonzeronumeric', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'por2'}})
    process_dict.update({'NArw' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_NArw_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'NArw'}})
    process_dict.update({'NAr2' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_NArw_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'NArw'}})
    process_dict.update({'NAr3' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_NArw_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'positivenumeric', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'NArw'}})
    process_dict.update({'NAr4' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_NArw_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'nonnegativenumeric', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'NArw'}})
    process_dict.update({'NAr5' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_NArw_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'integer', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'NArw'}})
    process_dict.update({'null' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_null_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'exclude', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : None}})
    process_dict.update({'copy' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_copy_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'exclude', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'copy'}})
    process_dict.update({'excl' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_excl_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'exclude', \
                                  'MLinfilltype' : 'boolexclude', \
                                  'labelctgy' : 'excl'}})
    process_dict.update({'exc2' : {'dualprocess' : self.process_exc2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_exc2_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'exc2'}})
    process_dict.update({'exc3' : {'dualprocess' : self.process_exc2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_exc2_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'exc2'}})
    process_dict.update({'exc4' : {'dualprocess' : self.process_exc2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_exc2_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'exc2'}})
    process_dict.update({'exc5' : {'dualprocess' : self.process_exc5_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_exc5_class, \
                                  'NArowtype' : 'integer', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'exc5'}})
    process_dict.update({'exc6' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_exc6_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'exclude', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'excl'}})
    process_dict.update({'shfl' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_shfl_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'shfl'}})
    process_dict.update({'nmbd' : {'dualprocess' : self.process_numerical_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_numerical_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'nmbr'}})
    process_dict.update({'101d' : {'dualprocess' : self.process_1010_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_1010_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : '1010', \
                                  'labelctgy' : '1010'}})
    process_dict.update({'ordd' : {'dualprocess' : self.process_ord3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ord3_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'texd' : {'dualprocess' : self.process_text_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_text_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'bnrd' : {'dualprocess' : self.process_binary_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_binary_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'bnry'}})
    process_dict.update({'datd' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mdsn'}})
    process_dict.update({'nuld' : {'dualprocess' : None, \
                                  'singleprocess' : self.process_null_class, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'exclude', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : None}})
    process_dict.update({'lbnm' : {'dualprocess' : self.process_exc2_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_exc2_class, \
                                  'NArowtype' : 'numeric', \
                                  'MLinfilltype' : 'numeric', \
                                  'labelctgy' : 'exc2'}})
    process_dict.update({'lb10' : {'dualprocess' : self.process_text_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_text_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'lbor' : {'dualprocess' : self.process_ord3_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_ord3_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'singlct', \
                                  'labelctgy' : 'ord3'}})
    process_dict.update({'lbte' : {'dualprocess' : self.process_text_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_text_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'multirt', \
                                  'labelctgy' : 'text'}})
    process_dict.update({'lbbn' : {'dualprocess' : self.process_binary_class, \
                                  'singleprocess' : None, \
                                  'postprocess' : self.postprocess_binary_class, \
                                  'NArowtype' : 'justNaN', \
                                  'MLinfilltype' : 'binary', \
                                  'labelctgy' : 'bnry'}})
    process_dict.update({'lbda' : {'dualprocess' : None, \
                                  'singleprocess' : None, \
                                  'postprocess' : None, \
                                  'NArowtype' : 'datetime', \
                                  'MLinfilltype' : 'exclude', \
                                  'labelctgy' : 'mdsn'}})

    return process_dict



  def processfamily(self, df_train, df_test, column, category, origcategory, process_dict, \
                    transform_dict, postprocess_dict, assign_param):
    '''
    #as automunge runs a for loop through each column in automunge, this is the master 
    #processing function applied which runs through the different family primitives
    #populated in the transform_dict by assembletransformdict
    
    #we will run in order of
    #parents, auntsuncles, siblings, cousins
    '''

    #process the parents (with downstream, with replacement)
    for parent in transform_dict[category]['parents']:

      if parent != None:

        df_train, df_test, postprocess_dict = \
        self.processparent(df_train, df_test, column, parent, origcategory, \
                          process_dict, transform_dict, postprocess_dict, assign_param)
        
        
    #process the auntsuncles (no downstream, with replacement)
    for auntuncle in transform_dict[category]['auntsuncles']:

      if auntuncle != None:

        #note we use the processcousin function here
        df_train, df_test, postprocess_dict = \
        self.processcousin(df_train, df_test, column, auntuncle, origcategory, \
                            process_dict, transform_dict, postprocess_dict, assign_param)
        
    #process the siblings (with downstream, supplemental)
    for sibling in transform_dict[category]['siblings']:

      if sibling != None:
        #note we use the processparent function here
        df_train, df_test, postprocess_dict = \
        self.processparent(df_train, df_test, column, sibling, origcategory, \
                          process_dict, transform_dict, postprocess_dict, assign_param)
        
    
    #process the cousins (no downstream, supplemental)
    for cousin in transform_dict[category]['cousins']:
      
      #this if statement kind of a placeholder such as for validation of primitive entry
      if cousin != None:

        #note we use the processcousin function here
        df_train, df_test, postprocess_dict = \
        self.processcousin(df_train, df_test, column, cousin, origcategory, \
                            process_dict, transform_dict, postprocess_dict, assign_param)



    #if we had replacement transformations performed then mark column for deletion
    #(circle of life)
    if len(transform_dict[category]['auntsuncles']) \
    + len(transform_dict[category]['parents']) > 0:
      #here we'll only address downstream generaitons
      if column in postprocess_dict['column_dict']:
        postprocess_dict['column_dict'][column]['deletecolumn'] = True

    return df_train, df_test, postprocess_dict


  def circleoflife(self, df_train, df_test, column, category, origcategory, process_dict, \
                    transform_dict, postprocess_dict):
    '''
    #This function deletes source column for cases where family primitives 
    #included replacement, with maintenance of the associated data structures.
    '''

    #if we had replacement transformations performed on first generation \
    #then delete the original column
    if len(transform_dict[category]['auntsuncles']) \
    + len(transform_dict[category]['parents']) > 0:
      del df_train[column]
      del df_test[column]

    #if we had replacement transformations performed on downstream generation \
    #then delete the associated parent column 
    for columndict_column in postprocess_dict['column_dict']:
      if postprocess_dict['column_dict'][columndict_column]['deletecolumn'] is True:

        #first we'll remove the column from columnslists 
        for columnslistcolumn in postprocess_dict['column_dict'][columndict_column]['columnslist']:

          if columndict_column in postprocess_dict['column_dict'][columnslistcolumn]['columnslist']:

            postprocess_dict['column_dict'][columnslistcolumn]['columnslist'].remove(columndict_column)

        #now we'll delete column
        #note this only worksa on single column  parents, need to incioroprate categorylist
        #for multicolumn parents (future extension)
        if columndict_column in list(df_train):
          del df_train[columndict_column]
          del df_test[columndict_column]

    return df_train, df_test, postprocess_dict


  def dictupdate(self, column, column_dict, postprocess_dict):
    '''
    #dictupdate function takes as input column_dict, postprocess_dict, then for cases
    #where origcolmn is the same fo rhte two combines the columnslist and the 
    #normalization_dict, then appends the column_dict onto the postprocess_dict
    #returns the column_dict and postprocess_dict. Note that the passed column name
    #"column" is the column name prior to the applicaiton of processing, and the
    #name of the column after the. last processing funciton is saved as a key
    #in the column_dict
    '''


    #(reason for "key2" instead of key1 is some shuffling during editing)
    for key2 in column_dict:

      #first address carry-though of origcolumn and origcategory from parent to child
      if column in postprocess_dict['column_dict']:

        #if column is not origcolumn in postprocess_dict
        if postprocess_dict['column_dict'][column]['origcolumn'] \
        != column:

          #assign origcolumn from postprocess_dict to column_dict
          column_dict[key2]['origcolumn'] = \
          postprocess_dict['column_dict'][column]['origcolumn']

          #assign origcategory from postprocess_dict to column_dict
          column_dict[key2]['origcategory'] = \
          postprocess_dict['column_dict'][column]['origcategory']

      for key1 in postprocess_dict['column_dict']:


        #if origcolumn is the same between column_dict saved in postprocess_dict and
        #the column_dict outputed from our processing, we'll combine a few values
        if postprocess_dict['column_dict'][key1]['origcolumn'] == column_dict[key2]['origcolumn']:
          #first we'll combine the columnslist capturing all columns 
          #originating from same origcolumn for these two sets
          postprocess_dict['column_dict'][key1]['columnslist'] = \
          list(set(postprocess_dict['column_dict'][key1]['columnslist'])|set(column_dict[key2]['columnslist']))
          #apply that value to the column_dict columnslist as well
          column_dict[key2]['columnslist'] = postprocess_dict['column_dict'][key1]['columnslist']


          #now we'll combine the normalization dictionary
          #remember that we are updating the strucutre to include the column+'_ctgy'
          #identifier as a key

          #first we'll append column_dict normalization_dict onto postprocess_dict, 
          postprocess_dict['column_dict'][key1]['normalization_dict'].update(column_dict[key2]['normalization_dict'])

          #then we'll copy postprocess_dict normalization_dict back onto column_dict
          column_dict[key2]['normalization_dict'] = postprocess_dict['column_dict'][key1]['normalization_dict']




          #now save the postprocess_dict normalization dicitonary to the column_dict
          #the idea is that every normalization parameter for every column is saved in 
          #every normalizaiton dict. Not extremely efficient but todo otherwise we 
          #would need to update our approach in postmunge in getting a column key
          column_dict[key2]['normalization_dict'] = \
          postprocess_dict['column_dict'][key1]['normalization_dict']


    #now append column_dict onto postprocess_dict
    postprocess_dict['column_dict'].update(column_dict)


    #return column_dict, postprocess_dict
    return postprocess_dict




  def processcousin(self, df_train, df_test, column, cousin, origcategory, \
                     process_dict, transform_dict, postprocess_dict, assign_param):
    '''
    #cousin is one of the primitives for processfamily function, and it involves
    #transformations without downstream derivations without replacement of source column
    #although this same funciton can be used with the auntsuncles primitive
    #by following with a deletion of original column, also this funciton can be
    #used on the niecesnephews primitive downstream of parents or siblings since 
    #they don't have children (they're way to young for that)
    #note the processing funcitons are accessed through the process_dict

    #reminder the format of assign_param is e.g.
    #assignparam = {'splt' : {'column1' : {'minsplit' : 4}}, \
    #               'spl2' : {'column2' : {'minsplit' : 3}}}

    '''
    
    params = self.grab_params(assign_param, cousin, column)
    
    if bool(params):

      #call processing functions including passing params from assign_param

      #if this is a dual process function
      if process_dict[cousin]['dualprocess'] != None:
        df_train, df_test, column_dict_list = \
        process_dict[cousin]['dualprocess'](df_train, df_test, column, origcategory, \
                                            postprocess_dict, params)

      #else if this is a single process function process train and test seperately
      elif process_dict[cousin]['singleprocess'] != None:

        df_train, column_dict_list =  \
        process_dict[cousin]['singleprocess'](df_train, column, origcategory, \
                                              postprocess_dict, params)

        df_test, _1 = \
        process_dict[cousin]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict, params)


    #else if there were no params call process functions without passing params
    else:

      #if this is a dual process function
      if process_dict[cousin]['dualprocess'] != None:
        df_train, df_test, column_dict_list = \
        process_dict[cousin]['dualprocess'](df_train, df_test, column, origcategory, \
                                            postprocess_dict)

      #else if this is a single process function process train and test seperately
      elif process_dict[cousin]['singleprocess'] != None:

        df_train, column_dict_list =  \
        process_dict[cousin]['singleprocess'](df_train, column, origcategory, \
                                              postprocess_dict)

        df_test, _1 = \
        process_dict[cousin]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict)


    #update the columnslist and normalization_dict for both column_dict and postprocess_dict
    for column_dict in column_dict_list:
      postprocess_dict = self.dictupdate(column, column_dict, postprocess_dict)


    return df_train, df_test, postprocess_dict




  def processparent(self, df_train, df_test, column, parent, origcategory, \
                    process_dict, transform_dict, postprocess_dict, assign_param):
    '''
    #parent is one of the primitives for processfamily function, and it involves
    #transformations with downstream derivations with replacement of source column
    #although this same funciton can be used with the siblinga primitive
    #by not following with a deletion of original column, also this funciton can be
    #used on the children primitive downstream of parents or siblings, allowing
    #the children to have children of their own, you know, grandchildren and stuff.
    #note the processing functions are accessed through the process_dict
    
    #reminder the format of assign_param is e.g.
    #assignparam = {'splt' : {'column1' : {'minsplit' : 4}}, \
    #               'spl2' : {'column2' : {'minsplit' : 3}}}
    
    #we want to apply in order of
    #upstream process, children, coworkers, niecesnephews, friends
    '''

    #upstream process
    
    params = self.grab_params(assign_param, parent, column)
    
    
    if bool(params):
    
      #if this is a dual process function
      if process_dict[parent]['dualprocess'] != None:

        df_train, df_test, column_dict_list = \
        process_dict[parent]['dualprocess'](df_train, df_test, column, origcategory, \
                                           postprocess_dict, params)

      #else if this is a single process function process train and test seperately
      elif process_dict[parent]['singleprocess'] != None:

        df_train, column_dict_list =  \
        process_dict[parent]['singleprocess'](df_train, column, origcategory, \
                                           postprocess_dict, params)

        df_test, _1 = \
        process_dict[parent]['singleprocess'](df_test, column, origcategory, \
                                           postprocess_dict, params)
        
    else:
      
      #if this is a dual process function
      if process_dict[parent]['dualprocess'] != None:

        df_train, df_test, column_dict_list = \
        process_dict[parent]['dualprocess'](df_train, df_test, column, origcategory, \
                                           postprocess_dict)

      #else if this is a single process function process train and test seperately
      elif process_dict[parent]['singleprocess'] != None:

        df_train, column_dict_list =  \
        process_dict[parent]['singleprocess'](df_train, column, origcategory, \
                                           postprocess_dict)

        df_test, _1 = \
        process_dict[parent]['singleprocess'](df_test, column, origcategory, \
                                           postprocess_dict)

    #update the columnslist and normalization_dict for both column_dict and postprocess_dict
    for column_dict in column_dict_list:
      postprocess_dict = self.dictupdate(column, column_dict, postprocess_dict)

      #note this only works for single column source, as currently implemented
      #multicolumn transforms (such as text or bins) cannot serve as parents
      #a future extension may check the categorylist from column_dict for 
      #purposes of transforms applied to multicolumn source
      parentcolumn = list(column_dict.keys())[0]
    

    #if transform_dict[parent] != None:
    
    #process any children
    for child in transform_dict[parent]['children']:

      if child != None:

        #process the child
        #note the function applied is processparent (using recursion)
        #parent column
        df_train, df_test, postprocess_dict = \
        self.processparent(df_train, df_test, parentcolumn, child, origcategory, \
                           process_dict, transform_dict, postprocess_dict, assign_param)
  #         self.processfamily(df_train, df_test, parentcolumn, child, origcategory, \
  #                            process_dict, transform_dict, postprocess_dict)
    

    #process any coworkers
    for coworker in transform_dict[parent]['coworkers']:

      if coworker != None:

        #process the coworker
        #note the function applied is processcousin
        df_train, df_test, postprocess_dict = \
        self.processcousin(df_train, df_test, parentcolumn, coworker, origcategory, \
                           process_dict, transform_dict, postprocess_dict, assign_param)
        


    #process any niecesnephews
    #note the function applied is comparable to processsibling, just a different
    #parent column
    for niecenephew in transform_dict[parent]['niecesnephews']:

      if niecenephew != None:

        #process the niecenephew
        #note the function applied is processparent (using recursion)
        #parent column
        df_train, df_test, postprocess_dict = \
        self.processparent(df_train, df_test, parentcolumn, niecenephew, origcategory, \
                           process_dict, transform_dict, postprocess_dict, assign_param)
  #         self.processfamily(df_train, df_test, parentcolumn, niecenephew, origcategory, \
  #                            process_dict, transform_dict, postprocess_dict)


    #process any friends
    for friend in transform_dict[parent]['friends']:

      if friend != None:

        #process the friend
        #note the function applied is processcousin
        df_train, df_test, postprocess_dict = \
        self.processcousin(df_train, df_test, parentcolumn, friend, origcategory, \
                           process_dict, transform_dict, postprocess_dict, assign_param)

  

  #     #if we had replacement transformations performed then delete the original column 
  #     #(circle of life)
  #     if len(transform_dict[parent]['children']) \
  #     + len(transform_dict[parent]['coworkers']) > 0:
  #       del df_train[parentcolumn]
  #       del df_test[parentcolumn]
  # #       if column in postprocess_dict['column_dict']:
  # #         postprocess_dict['column_dict'][parentcolumn]['deletecolumn'] = True

  # #       else:
  # #         postprocess_dict['column_dict'].update({parentcolumn : {'deletecolumn' : True}})
  

    #if we had replacement transformations performed then mark column for deletion
    #(circle of life)
    if len(transform_dict[parent]['children']) \
    + len(transform_dict[parent]['coworkers']) > 0:
      #here we'll only address downstream generaitons
      if parentcolumn in postprocess_dict['column_dict']:
        postprocess_dict['column_dict'][parentcolumn]['deletecolumn'] = True


    return df_train, df_test, postprocess_dict
  
  
  def process_NArw_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton that creates a boolean column indicating 1 for rows
    #corresponding to missing or improperly formated data in source column
    #note this uses the NArows function which has a category specific approach
    #returns same dataframe with new column of name column + '_NArw'
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #add a second column with boolean expression indicating a missing cell
    #(using NArows(.) function defined below, column name will be column+'_NArows')
    NArows_nmbr = self.getNArows(df, column, category, postprocess_dict)
    
    df[column + '_NArw'] = NArows_nmbr.copy()
    del NArows_nmbr

    #change NArows data type to 8-bit (1 byte) integers for memory savings
    df[column + '_NArw'] = df[column + '_NArw'].astype(np.int8)

    #create list of columns
    nmbrcolumns = [column + '_NArw']
    
    #for drift report
    pct_NArw = df[column + '_NArw'].sum() / df[column + '_NArw'].shape[0]

    #create normalization dictionary
    NArwnormalization_dict = {column + '_NArw' : {'pct_NArw':pct_NArw}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []
    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'NArw', \
                           'origcategory' : category, \
                           'normalization_dict' : NArwnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list

  
  
  
  def process_numerical_class(self, mdf_train, mdf_test, column, category, \
                              postprocess_dict, params = {}):
    '''
    #process_numerical_class(mdf_train, mdf_test, column, category)
    #function to normalize data to mean of 0 and standard deviation of 1 \
    #z score normalization) 
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_nmbr'
    #note this is a "dualprocess" function since is applied to both dataframes
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    #initialize parameters
    if 'offset' in params:
      offset = params['offset']
    else:
      offset = 0
      
    if 'multiplier' in params:
      multiplier = params['multiplier']
    else:
      multiplier = 1
    
    if 'cap' in params:
      cap = params['cap']
    else:
      cap = False
      
    if 'floor' in params:
      floor = params['floor']
    else:
      floor = False
    
    #copy source column into new column
    mdf_train[column + '_nmbr'] = mdf_train[column].copy()
    mdf_test[column + '_nmbr'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_nmbr'] = pd.to_numeric(mdf_train[column + '_nmbr'], errors='coerce')
    mdf_test[column + '_nmbr'] = pd.to_numeric(mdf_test[column + '_nmbr'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_nmbr'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_nmbr'].min()
    
    #if cap < maximum, maximum = cap
    if cap is not False and cap is not True:
      if cap < maximum:
        maximum = cap
    if floor is not False and floor is not True:
      if floor > minimum:
        minimum = floor
        
    #cap and floor application
    if cap is True:
      cap = maximum
    if floor is True:
      floor = minimum
      
    if cap is not False:
      #replace values in test > cap with cap
      mdf_train.loc[mdf_train[column + '_nmbr'] > cap, (column + '_nmbr')] \
      = cap
      
      mdf_test.loc[mdf_test[column + '_nmbr'] > cap, (column + '_nmbr')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_train.loc[mdf_train[column + '_nmbr'] < floor, (column + '_nmbr')] \
      = floor
      
      mdf_test.loc[mdf_test[column + '_nmbr'] < floor, (column + '_nmbr')] \
      = floor

    #get mean of training data
    mean = mdf_train[column + '_nmbr'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_nmbr'] = mdf_train[column + '_nmbr'].fillna(mean)
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'].fillna(mean)

    #subtract mean from column for both train and test
    mdf_train[column + '_nmbr'] = mdf_train[column + '_nmbr'] - mean
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'] - mean

    #get standard deviation of training data
    std = mdf_train[column + '_nmbr'].std()
    
    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if std == 0:
      std = 1

    #divide column values by std for both training and test data
    #offset, multiplier are parameters that defaults to zero, one
    mdf_train[column + '_nmbr'] = mdf_train[column + '_nmbr'] / std * multiplier + offset
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'] / std * multiplier + offset
    
#     #change data type for memory savings
#     mdf_train[column + '_nmbr'] = mdf_train[column + '_nmbr'].astype(np.float32)
#     mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'].astype(np.float32)


    #create list of columns
    nmbrcolumns = [column + '_nmbr']


    nmbrnormalization_dict = {column + '_nmbr' : {'mean' : mean, 'std' : std, \
                                                  'max' : maximum, 'min' : minimum, \
                                                  'offset' : offset, 'multiplier': multiplier, \
                                                  'cap' : cap, 'floor' : floor}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmbr', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_dxdt_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #process_dxdt_class(df, column, category, postprocess_dict)
    #function to translate a continues variable into a bounded variable
    #by taking delta of row from preceding row
    #assumes the rows are not shuffled and represent a continuous funciton 
    #with consistent time steps
    
    #for missing values, uses adjacent cell infill as default
    '''
    
    #initialize parameters
    if 'periods' in params:
        
      periods = params['periods']
    
    else:
      
      periods = 1
    
    #copy source column into new column
    df[column + '_dxdt'] = df[column].copy()
    
    #convert all values to either numeric or NaN
    df[column + '_dxdt'] = pd.to_numeric(df[column + '_dxdt'], errors='coerce')
    
    #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
    df[column + '_dxdt'] = df[column + '_dxdt'].fillna(method='ffill')
    
    #we'll follow with a bfill just in case first row had a nan
    df[column + '_dxdt'] = df[column + '_dxdt'].fillna(method='bfill')
    
    #(still a potential bug if both first and last row had a nan, we'll address with 
    #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
    df[column + '_dxdt'] = df[column + '_dxdt'].fillna(method='ffill')   
    
    #subtract preceding row
    df[column + '_dxdt'] = df[column + '_dxdt'] - df[column + '_dxdt'].shift(periods = periods)
    
    #first row will have a nan so just one more backfill
    df[column + '_dxdt'] = df[column + '_dxdt'].fillna(method='bfill')
    
    #then one more infill with to address scenario when data wasn't numeric
    #get arbitrary cell value, if one is nan then all will be
    value = df[column + '_dxdt'].values[0]
    if value != value:
      value = 0

      df[column + '_dxdt'] = df[column + '_dxdt'].fillna(value)
    
    #create list of columns
    nmbrcolumns = [column + '_dxdt']


    #grab some driftreport metrics
    #note that if this function implemented for data streams at scale it may be appropriate
    #to consider creating an alternate to dxdt without the driftreport metrics for postmunge efficiency
    positiveratio = df[df[column + '_dxdt'] >= 0].shape[0] / df[column + '_dxdt'].shape[0]
    negativeratio = df[df[column + '_dxdt'] < 0].shape[0] / df[column + '_dxdt'].shape[0]
    zeroratio = df[df[column + '_dxdt'] == 0].shape[0] / df[column + '_dxdt'].shape[0]
    minimum = df[column + '_dxdt'].min()
    maximum = df[column + '_dxdt'].max()
    mean = df[column + '_dxdt'].mean()
    std = df[column + '_dxdt'].std()


    nmbrnormalization_dict = {column + '_dxdt' : {'positiveratio' : positiveratio, \
                                                  'negativeratio' : negativeratio, \
                                                  'zeroratio' : zeroratio, \
                                                  'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'periods' : periods}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      column_dict = { nc : {'category' : 'dxdt', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return df, column_dict_list
  


  def process_dxd2_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #process_dxd2_class(df, column, category, postprocess_dict)
    #function to translate a continues variable into a bounded variable
    #by taking delta of average of last two rows minus 
    #average of preceding two rows before that
    #should take a littel noise out of noisy data
    #assumes the rows are not shuffled and represent a continuous funciton 
    #with consistent time steps
    
    #for missing values, uses adjacent cell infill as default
    '''
    
    #initialize parameters
    if 'periods' in params:
        
      periods = params['periods']
    
    else:
      
      periods = 2
    
    #copy source column into new column
    df[column + '_dxd2'] = df[column].copy()
    
    #convert all values to either numeric or NaN
    df[column + '_dxd2'] = pd.to_numeric(df[column + '_dxd2'], errors='coerce')
    
    #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
    df[column + '_dxd2'] = df[column + '_dxd2'].fillna(method='ffill')
    
    #we'll follow with a bfill just in case first row had a nan
    df[column + '_dxd2'] = df[column + '_dxd2'].fillna(method='bfill')
    
    #(still a potential bug if both first and last row had a nan, we'll address with 
    #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
    df[column + '_dxd2'] = df[column + '_dxd2'].fillna(method='ffill')   
    
#     #we're going to take difference of average of last two rows with two rows preceding
#     df[column + '_dxd2'] = (df[column + '_dxd2'] + df[column + '_dxd2'].shift()) / 2 \
#                            - ((df[column + '_dxd2'].shift(periods=2) + df[column + '_dxd2'].shift(periods=3)) / 2)
    

    df[column + '_temp1'] = df[column + '_dxd2'].copy()
    df[column + '_temp2'] = df[column + '_dxd2'].copy()
    # df_train['number7_temp3'] = df_train['number7'].copy()

    for i in range(periods):
      df[column + '_temp1'] = df[column + '_temp1'] + df[column + '_temp1'].shift(periods=1)

    df[column + '_temp1'] = df[column + '_temp1'].shift(periods=-periods)

    for i in range(0,periods):
      df[column + '_temp2'] = df[column + '_temp2'] + df[column + '_temp2'].shift(periods=1)

    # df_train['number7_temp2'] = df_train['number7'].copy()

    df[column + '_dxd2'] = (df[column + '_temp1'] - df[column + '_temp2'])/2
    
    
    #first row will have a nan so just one more backfill
    df[column + '_dxd2'] = df[column + '_dxd2'].fillna(method='bfill')
    df[column + '_dxd2'] = df[column + '_dxd2'].fillna(method='ffill')
    
    #then one more infill with to address scenario when data wasn't numeric
    #get arbitrary cell value, if one is nan then all will be
    value = df[column + '_dxd2'].values[0]
    if value != value:
      value = 0

      df[column + '_dxd2'] = df[column + '_dxd2'].fillna(value)
    
    del df[column + '_temp1']
    del df[column + '_temp2']
    
    #create list of columns
    nmbrcolumns = [column + '_dxd2']


    #grab some driftreport metrics
    #note that if this function implemented for data streams at scale it may be appropriate
    #to consider creating an alternate to dxd2 without the driftreport metrics for postmunge efficiency
    positiveratio = df[df[column + '_dxd2'] >= 0].shape[0] / df[column + '_dxd2'].shape[0]
    negativeratio = df[df[column + '_dxd2'] < 0].shape[0] / df[column + '_dxd2'].shape[0]
    zeroratio = df[df[column + '_dxd2'] == 0].shape[0] / df[column + '_dxd2'].shape[0]
    minimum = df[column + '_dxd2'].min()
    maximum = df[column + '_dxd2'].max()
    mean = df[column + '_dxd2'].mean()
    std = df[column + '_dxd2'].std()
  
    nmbrnormalization_dict = {column + '_dxd2' : {'positiveratio' : positiveratio, \
                                                  'negativeratio' : negativeratio, \
                                                  'zeroratio' : zeroratio, \
                                                  'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'periods' : periods}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'dxd2', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return df, column_dict_list


  def process_MADn_class(self, mdf_train, mdf_test, column, category, \
                              postprocess_dict, params = {}):
    '''
    #process_MADn_class(mdf_train, mdf_test, column, category)
    #function to normalize data to mean of 0 and mean absolute deviation of 1
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_MADn'
    #note this is a "dualprocess" function since is applied to both train and test dataframes
    #expect this approach works better than z-score for when the numerical distribution isn't thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    #copy source column into new column
    mdf_train[column + '_MADn'] = mdf_train[column].copy()
    mdf_test[column + '_MADn'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_MADn'] = pd.to_numeric(mdf_train[column + '_MADn'], errors='coerce')
    mdf_test[column + '_MADn'] = pd.to_numeric(mdf_test[column + '_MADn'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_MADn'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_MADn'].min()

    #get mean of training data
    mean = mdf_train[column + '_MADn'].mean() 
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_MADn'] = mdf_train[column + '_MADn'].fillna(mean)
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'].fillna(mean)

    #subtract mean from column for both train and test
    mdf_train[column + '_MADn'] = mdf_train[column + '_MADn'] - mean
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'] - mean

    #get mean absolute deviation of training data
    MAD = mdf_train[column + '_MADn'].mad()
    
    #special case to avoid div by 0
    if MAD == 0:
      MAD = 1

    #divide column values by mad for both training and test data
    mdf_train[column + '_MADn'] = mdf_train[column + '_MADn'] / MAD
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'] / MAD

#     #change data type for memory savings
#     mdf_train[column + '_MADn'] = mdf_train[column + '_MADn'].astype(np.float32)
#     mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'].astype(np.float32)

    #create list of columns
    nmbrcolumns = [column + '_MADn']


    nmbrnormalization_dict = {column + '_MADn' : {'mean' : mean, 'MAD' : MAD, \
                                                  'maximum':maximum, 'minimum':minimum}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'MADn', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list

  def process_MAD3_class(self, mdf_train, mdf_test, column, category, \
                              postprocess_dict, params = {}):
    '''
    #process_MAD3_class(mdf_train, mdf_test, column, category)
    #function to normalize data by subtracting maximum and dividing by median absolute deviation
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_MADn'
    #note this is a "dualprocess" function since is applied to both train and test dataframes
    #expect this approach works better than z-score for when the numerical distribution isn't thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    #the use of maximum instead of mean for normalization based on comment from RWRI lectures 
    #documented in medium essay "Machine Learning and Miscelanea"
    '''
    
    #copy source column into new column
    mdf_train[column + '_MAD3'] = mdf_train[column].copy()
    mdf_test[column + '_MAD3'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_MAD3'] = pd.to_numeric(mdf_train[column + '_MAD3'], errors='coerce')
    mdf_test[column + '_MAD3'] = pd.to_numeric(mdf_test[column + '_MAD3'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_MAD3'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_MAD3'].min()

    #get mean of training data
    mean = mdf_train[column + '_MAD3'].mean()
    
    if mean != mean:
      mean = 0
    
    #replace missing data with training set mean
    if mean != mean:
      mean = 0
    mdf_train[column + '_MAD3'] = mdf_train[column + '_MAD3'].fillna(mean)
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'].fillna(mean)

    #get max of training data
    datamax = mdf_train[column + '_MAD3'].max()
    
    #get mean absolute deviation of training data
    MAD = mdf_train[column + '_MAD3'].mad()
    
    #special case to avoid div by 0
    if MAD == 0:
      MAD = 1
    
    #subtract max from column for both train and test
    mdf_train[column + '_MAD3'] = mdf_train[column + '_MAD3'] - datamax
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'] - datamax

    #divide column values by mad for both training and test data
    mdf_train[column + '_MAD3'] = mdf_train[column + '_MAD3'] / MAD
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'] / MAD

#     #change data type for memory savings
#     mdf_train[column + '_MAD3'] = mdf_train[column + '_MAD3'].astype(np.float32)
#     mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'].astype(np.float32)

    #create list of columns
    nmbrcolumns = [column + '_MAD3']


    nmbrnormalization_dict = {column + '_MAD3' : {'mean' : mean, 'MAD' : MAD, 'datamax' : datamax, \
                                                  'maximum':maximum, 'minimum':minimum}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'MAD3', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list


  def process_mnmx_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mnmx_class(mdf_train, mdf_test, column, category)
    #function to scale data to minimum of 0 and maximum of 1 \
    #based on min/max values from training set for this column
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_mnmx'
    #note this is a "dualprocess" function since is applied to both dataframes
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both
    #dataframe inputs
    '''
    
    #for cap ands floor, False means not applied, True means based on set's found max/min in train set
    
    #initialize parameters
    if 'cap' in params:
      cap = params['cap']
    else:
      cap = False
      
    if 'floor' in params:
      floor = params['floor']
    else:
      floor = False
    
    #copy source column into new column
    mdf_train[column + '_mnmx'] = mdf_train[column].copy()
    mdf_test[column + '_mnmx'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_mnmx'] = pd.to_numeric(mdf_train[column + '_mnmx'], errors='coerce')
    mdf_test[column + '_mnmx'] = pd.to_numeric(mdf_test[column + '_mnmx'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get standard deviation of training data
    std = mdf_train[column + '_mnmx'].std()

    #get mean of training data
    mean = mdf_train[column + '_mnmx'].mean()    

    #replace missing data with training set mean
    if mean != mean:
      mean = 0
    mdf_train[column + '_mnmx'] = mdf_train[column + '_mnmx'].fillna(mean)
    mdf_test[column + '_mnmx'] = mdf_test[column + '_mnmx'].fillna(mean)
    
    #get maximum value of training column
    maximum = mdf_train[column + '_mnmx'].max()
    
    #get minimum value of training column
    minimum = mdf_train[column + '_mnmx'].min()
    
    #if cap < maximum, maximum = cap
    if cap is not False and cap is not True:
      if cap < maximum:
        maximum = cap
    if floor is not False and floor is not True:
      if floor > minimum:
        minimum = floor
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0:
      maxminusmin = 1
    
    #perform min-max scaling to train and test sets using values from train
    mdf_train[column + '_mnmx'] = (mdf_train[column + '_mnmx'] - minimum) / \
                                  (maxminusmin)
    
    mdf_test[column + '_mnmx'] = (mdf_test[column + '_mnmx'] - minimum) / \
                                 (maxminusmin)

    #cap and floor application
    if cap is True:
      cap = maximum
    if floor is True:
      floor = minimum
    
    if cap is not False:
      #replace values in test > cap with cap
      mdf_train.loc[mdf_train[column + '_mnmx'] > (cap - minimum)/maxminusmin, (column + '_mnmx')] \
      = (cap - minimum)/maxminusmin
      
      mdf_test.loc[mdf_test[column + '_mnmx'] > (cap - minimum)/maxminusmin, (column + '_mnmx')] \
      = (cap - minimum)/maxminusmin
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_train.loc[mdf_train[column + '_mnmx'] < (floor - minimum)/maxminusmin, (column + '_mnmx')] \
      = (floor - minimum)/maxminusmin
      
      mdf_test.loc[mdf_test[column + '_mnmx'] < (floor - minimum)/maxminusmin, (column + '_mnmx')] \
      = (floor - minimum)/maxminusmin

    
    #create list of columns
    nmbrcolumns = [column + '_mnmx']


    nmbrnormalization_dict = {column + '_mnmx' : {'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'maxminusmin' : maxminusmin, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'cap' : cap, \
                                                  'floor' : floor}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'mnmx', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list



  def process_mnm3_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mnmx_class(mdf_train, mdf_test, column, category)
    #function to scale data to minimum of 0 and maximum of 1 \
    #after replacing extreme values above the 0.99 quantile with
    #the value of 0.99 quantile and extreme values below the 0.01
    #quantile with the value of 0.01 quantile
    #(accepts parameters qmax and qmin to customize these 0.99/0.01 values)
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_mnmx'
    #note this is a "dualprocess" function since is applied to both dataframes
    '''

    #initialize parameters
    if 'qmax' in params:
      qmax = params['qmax']
    else:
      qmax = 0.99
      
    if 'qmin' in params:
      qmin = params['qmin']
    else:
      qmin = 0.01
    
    
    #copy source column into new column
    mdf_train[column + '_mnm3'] = mdf_train[column].copy()
    mdf_test[column + '_mnm3'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_mnm3'] = pd.to_numeric(mdf_train[column + '_mnm3'], errors='coerce')
    mdf_test[column + '_mnm3'] = pd.to_numeric(mdf_test[column + '_mnm3'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get standard deviation of training data
    std = mdf_train[column + '_mnm3'].std()


    #get maximum value of training column
    quantilemax = mdf_train[column + '_mnm3'].quantile(qmax)
    
    if quantilemax != quantilemax:
      quantilemax = 0

    #get minimum value of training column
    quantilemin = mdf_train[column + '_mnm3'].quantile(qmin)
    
    if quantilemin != quantilemin:
      quantilemin = 0

    #replace values > quantilemax with quantilemax
    mdf_train.loc[mdf_train[column + '_mnm3'] > quantilemax, (column + '_mnm3')] \
    = quantilemax
    mdf_test.loc[mdf_test[column + '_mnm3'] > quantilemax, (column + '_mnm3')] \
    = quantilemax
    #replace values < quantile10 with quantile10
    mdf_train.loc[mdf_train[column + '_mnm3'] < quantilemin, (column + '_mnm3')] \
    = quantilemin
    mdf_test.loc[mdf_test[column + '_mnm3'] < quantilemin, (column + '_mnm3')] \
    = quantilemin


    #note this step is now performed after the quantile evaluation / replacement

    #get mean of training data
    mean = mdf_train[column + '_mnm3'].mean()    

    if mean != mean:
      mean = 0
    
    #replace missing data with training set mean
    mdf_train[column + '_mnm3'] = mdf_train[column + '_mnm3'].fillna(mean)
    mdf_test[column + '_mnm3'] = mdf_test[column + '_mnm3'].fillna(mean)
    
    #avoid outlier div by zero when max = min
    maxminusmin = quantilemax - quantilemin
    if maxminusmin == 0:
      maxminusmin = 1

    #perform min-max scaling to train and test sets using values from train
    mdf_train[column + '_mnm3'] = (mdf_train[column + '_mnm3'] - quantilemin) / \
                                  (maxminusmin)

    mdf_test[column + '_mnm3'] = (mdf_test[column + '_mnm3'] - quantilemin) / \
                                 (maxminusmin)

#     #change data type for memory savings
#     mdf_train[column + '_mnm3'] = mdf_train[column + '_mnm3'].astype(np.float32)
#     mdf_test[column + '_mnm3'] = mdf_test[column + '_mnm3'].astype(np.float32)

    
    #create list of columns
    nmbrcolumns = [column + '_mnm3']


    nmbrnormalization_dict = {column + '_mnm3' : {'quantilemin' : quantilemin, \
                                                  'quantilemax' : quantilemax, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'qmax' : qmax, \
                                                  'qmin' : qmin }}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'mnm3', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())



    return mdf_train, mdf_test, column_dict_list


  def process_mnm6_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mnm6_class(mdf_train, mdf_test, column, category)
    #function to scale data to minimum of 0 and maximum of 1 \
    #based on min/max values from training set for this column
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_mnmx'
    #note this is a "dualprocess" function since is applied to both dataframes
    
    #note that this differs from mnmx in that a floor is placed on the test set at min(train)
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both
    #dataframe inputs
    '''
    
    #copy source column into new column
    mdf_train[column + '_mnm6'] = mdf_train[column].copy()
    mdf_test[column + '_mnm6'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_mnm6'] = pd.to_numeric(mdf_train[column + '_mnm6'], errors='coerce')
    mdf_test[column + '_mnm6'] = pd.to_numeric(mdf_test[column + '_mnm6'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get standard deviation of training data
    std = mdf_train[column + '_mnm6'].std()

    #get mean of training data
    mean = mdf_train[column + '_mnm6'].mean()    

    if mean != mean:
      mean = 0
    
    #replace missing data with training set mean
    mdf_train[column + '_mnm6'] = mdf_train[column + '_mnm6'].fillna(mean)
    mdf_test[column + '_mnm6'] = mdf_test[column + '_mnm6'].fillna(mean)
    
    #get maximum value of training column
    maximum = mdf_train[column + '_mnm6'].max()
    
    #get minimum value of training column
    minimum = mdf_train[column + '_mnm6'].min()
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0:
      maxminusmin = 1
    
    #perform min-max scaling to train and test sets using values from train
    mdf_train[column + '_mnm6'] = (mdf_train[column + '_mnm6'] - minimum) / \
                                  (maxminusmin)
    
    mdf_test[column + '_mnm6'] = (mdf_test[column + '_mnm6'] - minimum) / \
                                 (maxminusmin)
    
    #replace values in test < 0 with 0
    mdf_test.loc[mdf_train[column + '_mnm6'] < 0, (column + '_mnm6')] \
    = 0

#     #change data type for memory savings
#     mdf_train[column + '_mnm6'] = mdf_train[column + '_mnm6'].astype(np.float32)
#     mdf_test[column + '_mnm6'] = mdf_test[column + '_mnm6'].astype(np.float32)

    
    #create list of columns
    nmbrcolumns = [column + '_mnm6']


    nmbrnormalization_dict = {column + '_mnm6' : {'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'mean' : mean, \
                                                  'std' : std}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'mnm6', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_retn_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    """
    #process_retn_class(mdf_train, mdf_test, column, category)
    #function to scale data as follows:
    
    # if max >= 0 and min <= 0:
  
    #   #scaling based on 
    #   x = x / (max - min)

    # elif max >= 0 and min >= 0:

    #   #traditional min/max
    #   x = (x - min) / (max - min)
    
    # elif max <= 0 and min <= 0:

    #   #max/min (retains negative values)
    #   x = (x - max) / (max - min)
    
    #replaces missing or improperly formatted data with mean of remaining values
    
    #returns same dataframes with new column of name column + '_retn'
    #note this is a "dualprocess" function since is applied to both dataframes
    
    #note with parameters divisor can also be set as standard deviation
    #also aprameters accepted for cap/floor/mulitplier/offset
    #where cap/floor based on pretransform values
    #multiplier/offset based on posttransform values, muoltiplier applied betfore offset
    """
    
    #initialize parameters
    
    #accepts divisor parameters of 'minmax' or 'std'
    if 'divisor' in params:
      divisor = params['divisor']
    else:
      divisor = 'minmax'
    
    if 'offset' in params:
      offset = params['offset']
    else:
      offset = 0
      
    if 'multiplier' in params:
      multiplier = params['multiplier']
    else:
      multiplier = 1
    
    if 'cap' in params:
      cap = params['cap']
    else:
      cap = False
      
    if 'floor' in params:
      floor = params['floor']
    else:
      floor = False
    
    
    #copy source column into new column
    mdf_train[column + '_retn'] = mdf_train[column].copy()
    mdf_test[column + '_retn'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_retn'] = pd.to_numeric(mdf_train[column + '_retn'], errors='coerce')
    mdf_test[column + '_retn'] = pd.to_numeric(mdf_test[column + '_retn'], errors='coerce')
    
    #a few more metrics collected for driftreport
    #get standard deviation of training data
    std = mdf_train[column + '_retn'].std()
    
    #get maximum value of training column
    maximum = mdf_train[column + '_retn'].max()
    
    #get minimum value of training column
    minimum = mdf_train[column + '_retn'].min()
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0 or maxminusmin != maxminusmin:
      maxminusmin = 1
      
    if std != std or std == 0:
      std = 1
      
    #if cap < maximum, maximum = cap
    if cap is not False and cap is not True:
      if cap < maximum:
        maximum = cap
    if floor is not False and floor is not True:
      if floor > minimum:
        minimum = floor
        
    #cap and floor application
    if cap is True:
      cap = maximum
    if floor is True:
      floor = minimum
      
    if cap is not False:
      #replace values in test > cap with cap
      mdf_train.loc[mdf_train[column + '_retn'] > cap, (column + '_retn')] \
      = cap
      
      mdf_test.loc[mdf_test[column + '_retn'] > cap, (column + '_retn')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_train.loc[mdf_train[column + '_retn'] < floor, (column + '_retn')] \
      = floor
      
      mdf_test.loc[mdf_test[column + '_retn'] < floor, (column + '_retn')] \
      = floor
      
    #get mean of training data
    mean = mdf_train[column + '_retn'].mean()    

    #replace missing data with training set mean
    if mean != mean:
      mean = 0
    mdf_train[column + '_retn'] = mdf_train[column + '_retn'].fillna(mean)
    mdf_test[column + '_retn'] = mdf_test[column + '_retn'].fillna(mean)
    
    #edge case (only neccesary so scalingapproach is assigned)
    if maximum != maximum:
      maximum = 0
    if minimum != minimum:
      minimum = 0
    
    #divisor
    if divisor not in ['minmax', 'std']:
      print("Error: retn transform parameter 'divisor' only accepts entries of 'minmax' or 'std'")
    if divisor == 'minmax':
      divisor = maxminusmin
    else:
      divisor = std
      
    if divisor == 0 or divisor != divisor:
      divisor = 1
    
    
    #driftreport metric scalingapproach returned as 'retn' or 'mnmx' or 'mxmn'
    #where mnmx is for cases where all values in train set are positive
    #mxmn is for cases where all values in train set are negative
    
    if maximum >= 0 and minimum <= 0:
      
      mdf_train[column + '_retn'] = (mdf_train[column + '_retn']) / \
                                    (divisor) * multiplier + offset
      
      mdf_test[column + '_retn'] = (mdf_test[column + '_retn']) / \
                                    (divisor) * multiplier + offset
      
      scalingapproach = 'retn'
      
    elif maximum >= 0 and minimum >= 0:
    
      #perform min-max scaling to train and test sets using values from train
      mdf_train[column + '_retn'] = (mdf_train[column + '_retn'] - minimum) / \
                                    (divisor) * multiplier + offset

      mdf_test[column + '_retn'] = (mdf_test[column + '_retn'] - minimum) / \
                                   (divisor) * multiplier + offset
      
      scalingapproach = 'mnmx'
      
    elif maximum <= 0 and minimum <= 0:
    
      #perform min-max scaling to train and test sets using values from train
      mdf_train[column + '_retn'] = (mdf_train[column + '_retn'] - maximum) / \
                                    (divisor) * multiplier + offset

      mdf_test[column + '_retn'] = (mdf_test[column + '_retn'] - maximum) / \
                                   (divisor) * multiplier + offset
      
      scalingapproach = 'mxmn'
    
    #create list of columns
    nmbrcolumns = [column + '_retn']


    nmbrnormalization_dict = {column + '_retn' : {'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'scalingapproach' : scalingapproach, \
                                                  'offset' : offset, \
                                                  'multiplier': multiplier, \
                                                  'cap' : cap, \
                                                  'floor' : floor, \
                                                  'divisor' : divisor }}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'retn', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list

  
  
  def process_mean_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_mean_class(mdf_train, mdf_test, column, category)
    #function to scale data to minimum of 0 and maximum of 1 \
    #based on min/max values from training set for this column
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #replaces missing or improperly formatted data with mean of remaining values
    #returns same dataframes with new column of name column + '_mnmx'
    #note this is a "dualprocess" function since is applied to both dataframes
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both
    #dataframe inputs
    '''
    
    #initialize parameters
    if 'offset' in params:
      offset = params['offset']
    else:
      offset = 0
      
    if 'multiplier' in params:
      multiplier = params['multiplier']
    else:
      multiplier = 1
    
    if 'cap' in params:
      cap = params['cap']
    else:
      cap = False
      
    if 'floor' in params:
      floor = params['floor']
    else:
      floor = False
    
    #copy source column into new column
    mdf_train[column + '_mean'] = mdf_train[column].copy()
    mdf_test[column + '_mean'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_mean'] = pd.to_numeric(mdf_train[column + '_mean'], errors='coerce')
    mdf_test[column + '_mean'] = pd.to_numeric(mdf_test[column + '_mean'], errors='coerce')
    
    #get maximum value of training column
    maximum = mdf_train[column + '_mean'].max()
    
    #get minimum value of training column
    minimum = mdf_train[column + '_mean'].min()
    
    #if cap < maximum, maximum = cap
    if cap is not False and cap is not True:
      if cap < maximum:
        maximum = cap
    if floor is not False and floor is not True:
      if floor > minimum:
        minimum = floor
        
    #cap and floor application
    if cap is True:
      cap = maximum
    if floor is True:
      floor = minimum
      
    if cap is not False:
      #replace values in test > cap with cap
      mdf_train.loc[mdf_train[column + '_mean'] > cap, (column + '_mean')] \
      = cap
      
      mdf_test.loc[mdf_test[column + '_mean'] > cap, (column + '_mean')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_train.loc[mdf_train[column + '_mean'] < floor, (column + '_mean')] \
      = floor
      
      mdf_test.loc[mdf_test[column + '_mean'] < floor, (column + '_mean')] \
      = floor
    
    
    #a few more metrics collected for driftreport
    #get standard deviation of training data
    std = mdf_train[column + '_mean'].std()

    #get mean of training data
    mean = mdf_train[column + '_mean'].mean()    

    #replace missing data with training set mean
    if mean != mean:
      mean = 0
    mdf_train[column + '_mean'] = mdf_train[column + '_mean'].fillna(mean)
    mdf_test[column + '_mean'] = mdf_test[column + '_mean'].fillna(mean)
    

    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0 or maxminusmin != maxminusmin:
      maxminusmin = 1
    
    #perform min-max scaling to train and test sets using values from train
    mdf_train[column + '_mean'] = (mdf_train[column + '_mean'] - mean) / \
                                  (maxminusmin) * multiplier + offset
    
    mdf_test[column + '_mean'] = (mdf_test[column + '_mean'] - mean) / \
                                 (maxminusmin) * multiplier + offset

#     #change data type for memory savings
#     mdf_train[column + '_mnmx'] = mdf_train[column + '_mnmx'].astype(np.float32)
#     mdf_test[column + '_mnmx'] = mdf_test[column + '_mnmx'].astype(np.float32)


    
    #create list of columns
    nmbrcolumns = [column + '_mean']


    nmbrnormalization_dict = {column + '_mean' : {'minimum' : minimum, \
                                                  'maximum' : maximum, \
                                                  'maxminusmin' : maxminusmin, \
                                                  'mean' : mean, \
                                                  'std' : std, \
                                                  'offset' : offset, \
                                                  'multiplier': multiplier, \
                                                  'cap' : cap, \
                                                  'floor' : floor}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'mean', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list


  def process_binary_class(self, mdf_train, mdf_test, column, category, \
                           postprocess_dict, params = {}):
    '''
    #process_binary_class(mdf, column, missing)
    #converts binary classification values to 0 or 1
    #takes as arguement a pandas dataframe (mdf_train, mdf_test), \
    #the name of the column string ('column') \
    #and the category from parent columkn (category)
    #fills missing valules with most common value
    #returns same dataframes with new column of name column + '_bnry'
    #note this is a "dualprocess" function since is applied to both dataframes
    '''
    
    #copy column to column + '_bnry'
    mdf_train[column + '_bnry'] = mdf_train[column].copy()
    mdf_test[column + '_bnry'] = mdf_test[column].copy()

    #create plug value for missing cells as most common value
    valuecounts = pd.DataFrame(mdf_train[column + '_bnry'].value_counts())
    valuecounts = valuecounts.rename_axis('zzzinfill').sort_values(by = [column + '_bnry', 'zzzinfill'], ascending = [False, True])
    valuecounts = list(valuecounts.index)
    
    if len(valuecounts) > 0:

      if len(valuecounts) > 1:
        binary_missing_plug = valuecounts[0]
      else:
        #making an executive decision here to deviate from standardinfill of most common value
        #for this edge case where a column evaluated as binary has only single value and NaN's
        binary_missing_plug = 'zzzinfill'


      #test for nan
      if binary_missing_plug != binary_missing_plug:
        binary_missing_plug = valuecounts[1]

      #edge case when applying this transform to set with >2 values
      #this only comes up when caluclating driftreport in postmunge
      extravalues = []
      if len(valuecounts) > 2:
        i=0
        for value in valuecounts:
          if i>1:
            extravalues.append(value)
          i+=1


      #replace nan in valuecounts with binary_missing_plug so we can sort
      valuecounts = [x if x == x else binary_missing_plug for x in valuecounts]
  #     #convert everything to string for sort
  #     valuecounts = [str(x) for x in valuecounts]

      #note LabelBinarizer encodes alphabetically, with 1 assigned to first and 0 to second
      #we'll take different approach of going by most common value to 1 unless 0 or 1
      #are already in the set then we'll defer to keeping those designations in place
      #there's some added complexity here to deal with edge case of passing this function
      #to a set with >2 values as we might run into when caluclating drift in postmunge

  #     valuecounts.sort()
  #     valuecounts = sorted(valuecounts)
      #in case this includes both strings and integers for instance we'll sort this way
  #     valuecounts = sorted(valuecounts, key=lambda p: str(p))

      #we'll save these in the normalization dictionary for future reference
      onevalue = valuecounts[0]
      if len(valuecounts) > 1:
        zerovalue = valuecounts[1]
      else:
        zerovalue = 'zzzinfill'

      #special case for when the source column is already encoded as 0/1

      if len(valuecounts) <= 2:

        if 0 in valuecounts:
          zerovalue = 0
          if 1 in valuecounts:
            onevalue = 1
          else:
            if valuecounts[0] == 0:
              if len(valuecounts) > 1:
                onevalue = valuecounts[1]
              else:
                onevalue = 'zzzinfill'

        if 1 in valuecounts:
          if 0 not in valuecounts:
            if valuecounts[0] != 1:
              onevalue = 1
              zerovalue = valuecounts[0]


      #edge case same as above but when values of 0 or 1. are in set and 
      #len(valuecounts) > 2
      if len(valuecounts) > 2:
        valuecounts2 = valuecounts[:2]

        if 0 in valuecounts2:
          zerovalue = 0
          if 1 in valuecounts2:
            onevalue = 1
          else:
            if valuecounts2[0] == 0:
              if len(valuecounts) > 1:
                onevalue = valuecounts2[1]
              else:
                onevalue = 'zzzinfill'

        if 1 in valuecounts2:
          if 0 not in valuecounts2:
            if valuecounts2[0] != 1:
              onevalue = 1
              zerovalue = valuecounts2[0]


      #edge case that might come up in drift report
      if binary_missing_plug not in [onevalue, zerovalue]:
        binary_missing_plug = onevalue

      #edge case when applying this transform to set with >2 values
      #this only comes up when caluclating driftreport in postmunge
      if len(valuecounts) > 2:
        for value in extravalues:
          mdf_train[column + '_bnry'] = \
          np.where(mdf_train[column + '_bnry'] == value, binary_missing_plug, mdf_train[column + '_bnry'])
          mdf_test[column + '_bnry'] = \
          np.where(mdf_test[column + '_bnry'] == value, binary_missing_plug, mdf_test[column + '_bnry'])


      #replace missing data with specified classification
      mdf_train[column + '_bnry'] = mdf_train[column + '_bnry'].fillna(binary_missing_plug)
      mdf_test[column + '_bnry'] = mdf_test[column + '_bnry'].fillna(binary_missing_plug)


      #this addressess issue where nunique for mdftest > than that for mdf_train
      #note is currently an oportunity for improvement that NArows won't identify these poinsts as candiadates
      #for user specified infill, and as currently addressed will default to infill with most common value
      #in the mean time a workaround could be for user to manually replace extra values with nan prior to
      #postmunge application such as if they want to apply ML infill
      #this will only be an issue when nunique for df_train == 2, and nunique for df_test > 2
      #if len(mdf_test[column + '_bnry'].unique()) > 2:
      uniqueintest = mdf_test[column + '_bnry'].unique()
      for unique in uniqueintest:
        if unique not in [onevalue, zerovalue]:
          mdf_test[column + '_bnry'] = \
          np.where(mdf_test[column + '_bnry'] == unique, binary_missing_plug, mdf_test[column + '_bnry'])

      #convert column to binary 0/1 classification (replaces scikit LabelBinarizer)
      mdf_train[column + '_bnry'] = np.where(mdf_train[column + '_bnry'] == onevalue, 1, 0)
      mdf_test[column + '_bnry'] = np.where(mdf_test[column + '_bnry'] == onevalue, 1, 0)

      #create list of columns
      bnrycolumns = [column + '_bnry']

      #change data types to 8-bit (1 byte) integers for memory savings
      mdf_train[column + '_bnry'] = mdf_train[column + '_bnry'].astype(np.int8)
      mdf_test[column + '_bnry'] = mdf_test[column + '_bnry'].astype(np.int8)

      #a few more metrics collected for driftreport
      oneratio = mdf_train[column + '_bnry'].sum() / mdf_train[column + '_bnry'].shape[0]
      zeroratio = (mdf_train[column + '_bnry'].shape[0] - mdf_train[column + '_bnry'].sum() )\
                  / mdf_train[column + '_bnry'].shape[0]

      #create list of columns associated with categorical transform (blank for now)
      categorylist = []
    
    else:
      mdf_train[column + '_bnry'] = 0
      mdf_test[column + '_bnry'] = 0
      
      binary_missing_plug = 0
      onevalue = 1
      zerovalue = 0
      extravalues = 0
      oneratio = 0
      zeroratio = 0
      bnrycolumns = [column + '_bnry']

  #     bnrynormalization_dict = {column + '_bnry' : {'missing' : binary_missing_plug, \
  #                                                   'onevalue' : onevalue, \
  #                                                   'zerovalue' : zerovalue}}
    
    bnrynormalization_dict = {column + '_bnry' : {'missing' : binary_missing_plug, \
                                                  1 : onevalue, \
                                                  0 : zerovalue, \
                                                  'extravalues' : extravalues, \
                                                  'oneratio' : oneratio, \
                                                  'zeroratio' : zeroratio}}

    #store some values in the column_dict{} for use later in ML infill methods
    column_dict_list = []

    for bc in bnrycolumns:

      column_dict = { bc : {'category' : 'bnry', \
                           'origcategory' : category, \
                           'normalization_dict' : bnrynormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : bnrycolumns, \
                           'categorylist' : [bc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    #return mdf, bnrycolumns, categorylist, column_dict_list
    return mdf_train, mdf_test, column_dict_list
  

  def process_binary2_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_binary2_class(mdf, column, missing)
    #converts binary classification values to 0 or 1
    #takes as arguement a pandas dataframe (mdf_train, mdf_test), \
    #the name of the column string ('column') \
    #and the category from parent columkn (category)
    #fills missing valules with least common value (different than bnry)
    #returns same dataframes with new column of name column + '_bnry'
    #note this is a "dualprocess" function since is applied to both dataframes
    '''
    
    #copy column to column + '_bnry'
    mdf_train[column + '_bnr2'] = mdf_train[column].copy()
    mdf_test[column + '_bnr2'] = mdf_test[column].copy()

    #create plug value for missing cells as most common value
    valuecounts = pd.DataFrame(mdf_train[column + '_bnr2'].value_counts())
    valuecounts = valuecounts.rename_axis('zzzinfill').sort_values(by = [column + '_bnr2', 'zzzinfill'], ascending = [False, True])
    valuecounts = list(valuecounts.index)
    
    if len(valuecounts) > 0:

      if len(valuecounts) > 1:
        #binary_missing_plug = valuecounts[0]
        binary_missing_plug = valuecounts[1]
      else:
        #making an executive decision here to deviate from standardinfill of most common value
        #for this edge case where a column evaluated as binary has only single value and NaN's
        binary_missing_plug = 'zzzinfill'


      #test for nan
      if binary_missing_plug != binary_missing_plug:
        #binary_missing_plug = valuecounts[1]
        binary_missing_plug = valuecounts[0]

      #edge case when applying this transform to set with >2 values
      #this only comes up when caluclating driftreport in postmunge
      extravalues = []
      if len(valuecounts) > 2:
        i=0
        for value in valuecounts:
          if i>1:
            extravalues.append(value)
          i+=1


      #replace nan in valuecounts with binary_missing_plug so we can sort
      valuecounts = [x if x == x else binary_missing_plug for x in valuecounts]
  #     #convert everything to string for sort
  #     valuecounts = [str(x) for x in valuecounts]

      #note LabelBinarizer encodes alphabetically, with 1 assigned to first and 0 to second
      #we'll take different approach of going by most common value to 1 unless 0 or 1
      #are already in the set then we'll defer to keeping those designations in place
      #there's some added complexity here to deal with edge case of passing this function
      #to a set with >2 values as we might run into when caluclating drift in postmunge

  #     valuecounts.sort()
  #     valuecounts = sorted(valuecounts)
      #in case this includes both strings and integers for instance we'll sort this way
  #     valuecounts = sorted(valuecounts, key=lambda p: str(p))

      #we'll save these in the normalization dictionary for future reference
      onevalue = valuecounts[0]
      if len(valuecounts) > 1:
        zerovalue = valuecounts[1]
      else:
        zerovalue = 'zzzinfill'

      #special case for when the source column is already encoded as 0/1

      if len(valuecounts) <= 2:

        if 0 in valuecounts:
          zerovalue = 0
          if 1 in valuecounts:
            onevalue = 1
          else:
            if valuecounts[0] == 0:
              if len(valuecounts) > 1:
                onevalue = valuecounts[1]
              else:
                onevalue = 'zzzinfill'

        if 1 in valuecounts:
          if 0 not in valuecounts:
            if valuecounts[0] != 1:
              onevalue = 1
              zerovalue = valuecounts[0]


      #edge case same as above but when values of 0 or 1. are in set and 
      #len(valuecounts) > 2
      if len(valuecounts) > 2:
        valuecounts2 = valuecounts[:2]

        if 0 in valuecounts2:
          zerovalue = 0
          if 1 in valuecounts2:
            onevalue = 1
          else:
            if valuecounts2[0] == 0:
              if len(valuecounts) > 1:
                onevalue = valuecounts2[1]
              else:
                onevalue = 'zzzinfill'

        if 1 in valuecounts2:
          if 0 not in valuecounts2:
            if valuecounts2[0] != 1:
              onevalue = 1
              zerovalue = valuecounts2[0]


      #edge case that might come up in drift report
      if binary_missing_plug not in [onevalue, zerovalue]:
        #binary_missing_plug = onevalue
        binary_missing_plug = zerovalue

      #edge case when applying this transform to set with >2 values
      #this only comes up when caluclating driftreport in postmunge
      if len(valuecounts) > 2:
        for value in extravalues:
          mdf_train[column + '_bnr2'] = \
          np.where(mdf_train[column + '_bnr2'] == value, binary_missing_plug, mdf_train[column + '_bnr2'])
          mdf_test[column + '_bnr2'] = \
          np.where(mdf_test[column + '_bnr2'] == value, binary_missing_plug, mdf_test[column + '_bnr2'])


      #replace missing data with specified classification
      mdf_train[column + '_bnr2'] = mdf_train[column + '_bnr2'].fillna(binary_missing_plug)
      mdf_test[column + '_bnr2'] = mdf_test[column + '_bnr2'].fillna(binary_missing_plug)


      #this addressess issue where nunique for mdftest > than that for mdf_train
      #note is currently an oportunity for improvement that NArows won't identify these poinsts as candiadates
      #for user specified infill, and as currently addressed will default to infill with most common value
      #in the mean time a workaround could be for user to manually replace extra values with nan prior to
      #postmunge application such as if they want to apply ML infill
      #this will only be an issue when nunique for df_train == 2, and nunique for df_test > 2
      #if len(mdf_test[column + '_bnry'].unique()) > 2:
      uniqueintest = mdf_test[column + '_bnr2'].unique()
      for unique in uniqueintest:
        if unique not in [onevalue, zerovalue]:
          mdf_test[column + '_bnr2'] = \
          np.where(mdf_test[column + '_bnr2'] == unique, binary_missing_plug, mdf_test[column + '_bnr2'])

      #convert column to binary 0/1 classification (replaces scikit LabelBinarizer)
      mdf_train[column + '_bnr2'] = np.where(mdf_train[column + '_bnr2'] == onevalue, 1, 0)
      mdf_test[column + '_bnr2'] = np.where(mdf_test[column + '_bnr2'] == onevalue, 1, 0)

      #create list of columns
      bnrycolumns = [column + '_bnr2']

      #change data types to 8-bit (1 byte) integers for memory savings
      mdf_train[column + '_bnr2'] = mdf_train[column + '_bnr2'].astype(np.int8)
      mdf_test[column + '_bnr2'] = mdf_test[column + '_bnr2'].astype(np.int8)

      #a few more metrics collected for driftreport
      oneratio = mdf_train[column + '_bnr2'].sum() / mdf_train[column + '_bnr2'].shape[0]
      zeroratio = (mdf_train[column + '_bnr2'].shape[0] - mdf_train[column + '_bnr2'].sum() )\
                  / mdf_train[column + '_bnr2'].shape[0]

      #create list of columns associated with categorical transform (blank for now)
      categorylist = []
    
    else:
      mdf_train[column + '_bnr2'] = 0
      mdf_test[column + '_bnr2'] = 0
      
      binary_missing_plug = 0
      onevalue = 1
      zerovalue = 0
      extravalues = 0
      oneratio = 0
      zeroratio = 0
      bnrycolumns = [column + '_bnr2']

  #     bnrynormalization_dict = {column + '_bnry' : {'missing' : binary_missing_plug, \
  #                                                   'onevalue' : onevalue, \
  #                                                   'zerovalue' : zerovalue}}
    
    bnrynormalization_dict = {column + '_bnr2' : {'missing' : binary_missing_plug, \
                                                  1 : onevalue, \
                                                  0 : zerovalue, \
                                                  'extravalues' : extravalues, \
                                                  'oneratio' : oneratio, \
                                                  'zeroratio' : zeroratio}}

    #store some values in the column_dict{} for use later in ML infill methods
    column_dict_list = []

    for bc in bnrycolumns:

      column_dict = { bc : {'category' : 'bnr2', \
                           'origcategory' : category, \
                           'normalization_dict' : bnrynormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : bnrycolumns, \
                           'categorylist' : [bc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    #return mdf, bnrycolumns, categorylist, column_dict_list
    return mdf_train, mdf_test, column_dict_list
  

  def process_text_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_text_class(mdf_train, mdf_test, column, category)
    #preprocess column with text categories
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column')
    #and the name of the category from parent column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    #doesn't delete the original column from master dataframe but
    #creates onehot encodings
    #with columns named after column_ + text categories
    #any categories missing from the training set removed from test set
    #any category present in training but missing from test set given a column of zeros for consistent formatting
    #ensures order of all new columns consistent between both sets
    #returns two transformed dataframe (mdf_train, mdf_test) \
    #and a list of the new column names (textcolumns)
    
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    tempcolumn = column + '_:;:_temp'
    
    #store original column for later retrieval
    mdf_train[tempcolumn] = mdf_train[column].copy()
    mdf_test[tempcolumn] = mdf_test[column].copy()

    #convert column to category
    mdf_train[tempcolumn] = mdf_train[tempcolumn].astype('category')
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype('category')

    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_train[tempcolumn].cat.categories:
      mdf_train[tempcolumn] = mdf_train[tempcolumn].cat.add_categories(['zzzinfill'])
    if 'zzzinfill' not in mdf_test[tempcolumn].cat.categories:
      mdf_test[tempcolumn] = mdf_test[tempcolumn].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_train[tempcolumn] = mdf_train[tempcolumn].fillna('zzzinfill')
    mdf_test[tempcolumn] = mdf_test[tempcolumn].fillna('zzzinfill')

    #replace numerical with string equivalent
    mdf_train[tempcolumn] = mdf_train[tempcolumn].astype(str)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype(str)


    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    labels_train = mdf_train[tempcolumn].unique()
    labels_train.sort(axis=0)
    orig_labels_train = list(labels_train.copy())
    labels_test = mdf_test[tempcolumn].unique()
    labels_test.sort(axis=0)


    #pandas one hot encoder
    df_train_cat = pd.get_dummies(mdf_train[tempcolumn])
    df_test_cat = pd.get_dummies(mdf_test[tempcolumn])

    #append column header name to each category listing
    #note the iteration is over a numpy array hence the [...] approach
    labels_train[...] = column + '_' + labels_train[...]
    labels_test[...] = column + '_' + labels_test[...]
    
    #convert sparse array to pandas dataframe with column labels
    df_train_cat.columns = labels_train
    df_test_cat.columns = labels_test

    #Get missing columns in test set that are present in training set
    missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )

    #Add a missing column in test set with default value equal to 0
    for c in missing_cols:
        df_test_cat[c] = 0
    #Ensure the order of column in the test set is in the same order than in train set
    #Note this also removes categories in test set that aren't present in training set
    df_test_cat = df_test_cat[df_train_cat.columns]


    #concatinate the sparse set with the rest of our training data
    mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)
    mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)


    del mdf_train[tempcolumn]    
    del mdf_test[tempcolumn]
    
    #delete _NArw column, this will be processed seperately in the processfamily function
    #delete support NArw2 column
#     columnNArw = column + '_NArw'
    columnNAr2 = column + '_zzzinfill'
    if columnNAr2 in list(mdf_train):
      del mdf_train[columnNAr2]
    if columnNAr2 in list(mdf_test):
      del mdf_test[columnNAr2]
    if 'zzzinfill' in orig_labels_train:
      orig_labels_train.remove('zzzinfill')

    
#     del mdf_train[column + '_NAr2']    
#     del mdf_test[column + '_NAr2']
    
    
    #create output of a list of the created column names
    NAcolumn = columnNAr2
    labels_train = list(df_train_cat)
    if NAcolumn in labels_train:
      labels_train.remove(NAcolumn)
    textcolumns = labels_train
    
    #now we'll creaate a dicitonary of the columns : categories for later reference
    #reminder here is list of. unque values from original column
    #labels_train
    
    
    normalizationdictvalues = labels_train
    normalizationdictkeys = textcolumns
    
    normalizationdictkeys.sort()
    normalizationdictvalues.sort()
    
    #textlabelsdict = dict(zip(normalizationdictkeys, normalizationdictvalues))
    textlabelsdict = dict(zip(normalizationdictvalues, orig_labels_train))
    
    
    
    
    #change data types to 8-bit (1 byte) integers for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)


    #store some values in the text_dict{} for use later in ML infill methods
    column_dict_list = []

    categorylist = textcolumns.copy()
#     categorylist.remove(columnNArw)

    for tc in textcolumns:
    
      #new parameter collected for driftreport
      tc_ratio = tc + '_ratio'
      tcratio = mdf_train[tc].sum() / mdf_train[tc].shape[0]

      textnormalization_dict = {tc : {'textlabelsdict_text' : textlabelsdict, \
                                      tc_ratio : tcratio}}
      
      column_dict = {tc : {'category' : 'text', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : textcolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())

    
    return mdf_train, mdf_test, column_dict_list

  
  def process_lngt_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton that length of string for each entry
    #such as a heuristic for information content
    #default infill is len(str(np.nan)) = 3
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #create new column
    df[column + '_lngt'] = df[column].copy()
    
    df[column + '_lngt'] = df[column + '_lngt'].astype(str).apply(len)
    
    #grab a fe4w driftreport metrics:
    #get maximum value of training column
    maximum = df[column + '_lngt'].max()
    
    #get minimum value of training column
    minimum = df[column + '_lngt'].min()
    
    #get minimum value of training column
    mean = df[column + '_lngt'].mean()
    
    #get standard deviation of training column
    std = df[column + '_lngt'].std()

    #create list of columns
    columns = [column + '_lngt']

    #create normalization dictionary
    normalization_dict = {column + '_lngt' : {'maximum' : maximum, \
                                              'minimum' : minimum, \
                                              'mean' : mean, \
                                              'std' : std }}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []
    
    for nc in columns:

      column_dict = { nc : {'category' : 'lngt', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : columns, \
                           'categorylist' : columns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list
  


  
  def process_UPCS_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton that converts columns to uppercase strings
    #such as to allow consistnet encoding if data has upper/lower case discrepencies
    #default infill is a distinct entry as string NAN
    #note that with assigninfill this can be converted to other infill methods
    #returns same dataframe with new column of name column + '_UPCS'
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #create new column
    df[column + '_UPCS'] = df[column].copy()
    
    #convert column to string
    df[column + '_UPCS'] = df[column + '_UPCS'].astype(str)
    
    #convert to uppercase string
    df[column + '_UPCS'] = df[column + '_UPCS'].str.upper()

    #create list of columns
    UPCScolumns = [column + '_UPCS']

    #create normalization dictionary
    normalization_dict = {column + '_UPCS' : {}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []
    
    for nc in UPCScolumns:

      column_dict = { nc : {'category' : 'UPCS', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : UPCScolumns, \
                           'categorylist' : UPCScolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list


  def process_splt_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_splt_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 
    #'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    '''
    
    #overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]
    
    if 'minsplit' in params:
      minsplit = params['minsplit'] - 1
    else:
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
      
    if 'concurrent_activations' in params:
      concurrent_activations = params['concurrent_activations']
    else:
      concurrent_activations = False
      
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]
                  
                  if concurrent_activations is False:

                    if extract == extract3:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                  elif concurrent_activations is True:
                    
                    if extract == extract3 and unique in overlap_dict[key]:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                if extract_already_in_overlap_dict is True:
                  
                  break
                    

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for key in train_keys:

      test_overlap_dict.update({key:[]})

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
              
              if concurrent_activations is False:

                break
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:

      newcolumn = column + '_splt_' + dict_key
      
      mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

      mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      
      mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

      newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'splt_newcolumns_splt'   : newcolumns, \
                                      'minsplit' : minsplit, \
                                      'concurrent_activations' : concurrent_activations}}
      
      column_dict = {tc : {'category' : 'splt', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []
    
    return mdf_train, mdf_test, column_dict_list
  
  

  
  
  def process_spl2_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_spl2_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries 
    #with their redecued overlap
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' 
    #replaced cells with north in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted 
    #to follow with an ordl encoding
    '''
    
#     overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
        
      minsplit = params['minsplit'] - 1
    
    else:
      
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
  
  
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]

                  if extract == extract3:

                    extract_already_in_overlap_dict = True
                    
                    break
                    
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for key in train_keys:

      test_overlap_dict.update({key:[]})

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
              
              break
                        
    
    
    #so that was all comparable to splt, now for spl2 we'll create a new 
    #dictionary structred as
    #{original unique value : overlap extract for replacement}
    
    #since one original unique value may have entries as multiple overlaps, 
    #we'll prioritize overlaps with
    #longer string lengths and then alphabetical
    
    spl2_overlap_dict = {}
    
    overlap_key_list = list(overlap_dict)
    
    overlap_key_list.sort()
    overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in overlap_key_list:
      
      for entry in overlap_dict[overlap_key]:
        
        if entry not in spl2_overlap_dict:
          
          spl2_overlap_dict.update({entry : overlap_key})
          
    
    
    #then we'll do same for test set
    
    spl2_test_overlap_dict = {}
    
    test_overlap_key_list = list(test_overlap_dict)
    
    test_overlap_key_list.sort()
    test_overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in test_overlap_key_list:
      
      for entry in test_overlap_dict[overlap_key]:
        
        if entry not in spl2_test_overlap_dict:
          
          spl2_test_overlap_dict.update({entry : overlap_key})
    
    
    
    
    newcolumns = []

#     for dict_key in overlap_dict:

    newcolumn = column + '_spl2'
  
    mdf_train[newcolumn] = mdf_train[column].copy()
    mdf_test[newcolumn] = mdf_test[column].copy()
    
    mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
    mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl2_overlap_dict)

#       mdf_train[newcolumn] = mdf_train[column].isin(overlap_dict[dict_key])
#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)

#       mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

    newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'spl2_newcolumns'   : newcolumns, 
                                      'spl2_overlap_dict' : spl2_overlap_dict, \
                                      'spl2_test_overlap_dict' : spl2_test_overlap_dict, \
                                      'minsplit' : minsplit}}
      
      column_dict = {tc : {'category' : 'spl2', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_spl5_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_spl5_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries 
    #with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' 
    #replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted 
    #to follow with an ordl encoding
    '''
    

    
#     overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
        
      minsplit = params['minsplit'] - 1
    
    else:
      
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]

                  if extract == extract3:

                    extract_already_in_overlap_dict = True
                    
                    break
                    
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break

                        
    #now for mdf_test we'll only consider those overlaps already 
    #identified from train set
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for key in train_keys:

      test_overlap_dict.update({key:[]})

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
              
              break
                        
    
    
    #so that was all comparable to splt, now for spl2 we'll create a new 
    #dictionary structred as
    #{original unique value : overlap extract for replacement}
    
    #since one original unique value may have entries as multiple overlaps, 
    #we'll prioritize overlaps with
    #longer string lengths and then alphabetical
    
    spl2_overlap_dict = {}
    
    overlap_key_list = list(overlap_dict)
    
    overlap_key_list.sort()
    overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in overlap_key_list:
      
      for entry in overlap_dict[overlap_key]:
        
        if entry not in spl2_overlap_dict:
          
          spl2_overlap_dict.update({entry : overlap_key})
    
    #here's where we identify values to set to 0 for spl5
    spl5_zero_dict = {}
    for entry in unique_list:
      if entry not in spl2_overlap_dict:
        spl5_zero_dict.update({entry : 0})
          
    
    
    #then we'll do same for test set
    
    spl2_test_overlap_dict = {}
    
    test_overlap_key_list = list(test_overlap_dict)
    
    test_overlap_key_list.sort()
    test_overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in test_overlap_key_list:
      
      for entry in test_overlap_dict[overlap_key]:
        
        if entry not in spl2_test_overlap_dict:
          
          spl2_test_overlap_dict.update({entry : overlap_key})
    
    #here's where we identify values to set to 0 for spl5
    spl5_test_zero_dict = {}
    for entry in unique_list_test:
      if entry not in spl2_test_overlap_dict:
        spl5_test_zero_dict.update({entry : 0})
    
    
    newcolumns = []

#     for dict_key in overlap_dict:

    newcolumn = column + '_spl5'
    
    mdf_train[newcolumn] = mdf_train[column].copy()
    mdf_test[newcolumn] = mdf_test[column].copy()
    
    mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
    mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl2_overlap_dict)
    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl5_zero_dict)

#       mdf_train[newcolumn] = mdf_train[column].isin(overlap_dict[dict_key])
#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

    newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'spl2_newcolumns'   : newcolumns, 
                                      'spl2_overlap_dict' : spl2_overlap_dict, \
                                      'spl2_test_overlap_dict' : spl2_test_overlap_dict, \
                                      'spl5_zero_dict' : spl5_zero_dict, \
                                      'minsplit' : minsplit}}
      
      column_dict = {tc : {'category' : 'spl5', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list
  
  def process_spl7_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #(spl7 is same as spl5 but uses min overlap character legnth of 1 instead of 5)
    #process_spl5_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries 
    #with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' 
    #replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted 
    #to follow with an ordl encoding
    '''
    
#     overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]
    if 'minsplit' in params:
        
      minsplit = params['minsplit'] - 1
    
    else:
      
      minsplit = 2 - 1
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]

                  if extract == extract3:

                    extract_already_in_overlap_dict = True
                    
                    break
                    
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break

                        
    #now for mdf_test we'll only consider those overlaps already 
    #identified from train set
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for key in train_keys:

      test_overlap_dict.update({key:[]})

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
              
              break
                        
    
    
    #so that was all comparable to splt, now for spl2 we'll create a new 
    #dictionary structred as
    #{original unique value : overlap extract for replacement}
    
    #since one original unique value may have entries as multiple overlaps, 
    #we'll prioritize overlaps with
    #longer string lengths and then alphabetical
    
    spl2_overlap_dict = {}
    
    overlap_key_list = list(overlap_dict)
    
    overlap_key_list.sort()
    overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in overlap_key_list:
      
      for entry in overlap_dict[overlap_key]:
        
        if entry not in spl2_overlap_dict:
          
          spl2_overlap_dict.update({entry : overlap_key})
    
    #here's where we identify values to set to 0 for spl5
    spl5_zero_dict = {}
    for entry in unique_list:
      if entry not in spl2_overlap_dict:
        spl5_zero_dict.update({entry : 0})
          
    
    
    #then we'll do same for test set
    
    spl2_test_overlap_dict = {}
    
    test_overlap_key_list = list(test_overlap_dict)
    
    test_overlap_key_list.sort()
    test_overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in test_overlap_key_list:
      
      for entry in test_overlap_dict[overlap_key]:
        
        if entry not in spl2_test_overlap_dict:
          
          spl2_test_overlap_dict.update({entry : overlap_key})
    
    #here's where we identify values to set to 0 for spl5
    spl5_test_zero_dict = {}
    for entry in unique_list_test:
      if entry not in spl2_test_overlap_dict:
        spl5_test_zero_dict.update({entry : 0})
    
    
    newcolumns = []

#     for dict_key in overlap_dict:

    newcolumn = column + '_spl7'
    
    mdf_train[newcolumn] = mdf_train[column].copy()
    mdf_test[newcolumn] = mdf_test[column].copy()
    
    mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
    mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl2_overlap_dict)
    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl5_zero_dict)

#       mdf_train[newcolumn] = mdf_train[column].isin(overlap_dict[dict_key])
#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

    newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'spl2_newcolumns'   : newcolumns, 
                                      'spl2_overlap_dict' : spl2_overlap_dict, \
                                      'spl2_test_overlap_dict' : spl2_test_overlap_dict, \
                                      'spl5_zero_dict' : spl5_zero_dict, \
                                      'minsplit' : minsplit}}
      
      column_dict = {tc : {'category' : 'spl7', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list

  def process_spl8_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_spl8_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 
    #'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    #comparable to splt but
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    '''
    
    #overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
      minsplit = params['minsplit'] - 1
    else:
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
      
    if 'concurrent_activations' in params:
      concurrent_activations = params['concurrent_activations']
    else:
      concurrent_activations = False
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]
                    
                  if concurrent_activations is False:

                    if extract == extract3:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                  elif concurrent_activations is True:
                    
                    if extract == extract3 and unique in overlap_dict[key]:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    
    
#     unique_list_test = list(mdf_test[column].unique())

#     unique_list_test = list(map(str, unique_list_test))

#     test_overlap_dict = {}

#     train_keys = list(overlap_dict)

#     train_keys.sort(key = len, reverse=True)

#     for key in train_keys:

#       test_overlap_dict.update({key:[]})

#     for dict_key in train_keys:

#       for unique_test in unique_list_test:

#         len_key = len(dict_key)

#         if len(unique_test) >= len_key:

#           nbr_iterations4 = len(unique_test) - len_key

#           for l in range(nbr_iterations4 + 1):

#             extract4 = unique_test[l:(len_key+l)]

#             if extract4 == dict_key:

#               test_overlap_dict[dict_key].append(unique_test)
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:

      newcolumn = column + '_spl8_' + dict_key
      
      mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

      mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      
      #mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

      newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'splt_newcolumns_spl8'   : newcolumns, \
                                      'minsplit' : minsplit, \
                                      'concurrent_activations' : concurrent_activations}}
      
      column_dict = {tc : {'category' : 'spl8', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list
  
  def process_spl9_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_spl2_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries 
    #with their redecued overlap
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' 
    #replaced cells with north in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted 
    #to follow with an ordl encoding
    
    #spl9 is comparable to spl2 but makes assumption that set of unique values
    #in test set is same or subset of train set for more efficient processing
    '''
    
#     overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
      minsplit = params['minsplit'] - 1

    else:
      
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]

                  if extract == extract3:

                    extract_already_in_overlap_dict = True
                    
                    break
                    
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
#     unique_list_test = list(mdf_test[column].unique())

#     unique_list_test = list(map(str, unique_list_test))

#     test_overlap_dict = {}

#     train_keys = list(overlap_dict)

#     train_keys.sort(key = len, reverse=True)

#     for key in train_keys:

#       test_overlap_dict.update({key:[]})

#     for dict_key in train_keys:

#       for unique_test in unique_list_test:

#         len_key = len(dict_key)

#         if len(unique_test) >= len_key:

#           nbr_iterations4 = len(unique_test) - len_key

#           for l in range(nbr_iterations4 + 1):

#             extract4 = unique_test[l:(len_key+l)]

#             if extract4 == dict_key:

#               test_overlap_dict[dict_key].append(unique_test)
    
    #for spl9 we'll just copy train set overlap_dict
    test_overlap_dict = deepcopy(overlap_dict)
    
    #so that was all comparable to splt, now for spl2 we'll create a new 
    #dictionary structred as
    #{original unique value : overlap extract for replacement}
    
    #since one original unique value may have entries as multiple overlaps, 
    #we'll prioritize overlaps with
    #longer string lengths and then alphabetical
    
    spl2_overlap_dict = {}
    
    overlap_key_list = list(overlap_dict)
    
    overlap_key_list.sort()
    overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in overlap_key_list:
      
      for entry in overlap_dict[overlap_key]:
        
        if entry not in spl2_overlap_dict:
          
          spl2_overlap_dict.update({entry : overlap_key})
          
    
    
    #then we'll do same for test set
    
#     spl2_test_overlap_dict = {}
    
#     test_overlap_key_list = list(test_overlap_dict)
    
#     test_overlap_key_list.sort()
#     test_overlap_key_list.sort(key = len, reverse=True)
    
#     for overlap_key in test_overlap_key_list:
      
#       for entry in test_overlap_dict[overlap_key]:
        
#         if entry not in spl2_test_overlap_dict:
          
#           spl2_test_overlap_dict.update({entry : overlap_key})
    
    
    spl2_test_overlap_dict = deepcopy(spl2_overlap_dict)
    
    unique_list_test = list(mdf_test[column].unique())
    unique_list_test = list(map(str, unique_list_test))
    
    extra_unique_test = list(set(unique_list_test) - set(unique_list))
    
    for extra_unique in extra_unique_test:
      
      spl2_test_overlap_dict.update({str(extra_unique) : str(extra_unique)})
    
    
    
    newcolumns = []

#     for dict_key in overlap_dict:

    newcolumn = column + '_spl9'
  
    mdf_train[newcolumn] = mdf_train[column].copy()
    mdf_test[newcolumn] = mdf_test[column].copy()
    
    mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
    mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl2_overlap_dict)

#       mdf_train[newcolumn] = mdf_train[column].isin(overlap_dict[dict_key])
#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)

#       mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

    newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'spl2_newcolumns'   : newcolumns, 
                                      'spl2_overlap_dict' : spl2_overlap_dict, \
                                      'unique_list' : unique_list, \
                                      'spl2_test_overlap_dict' : spl2_test_overlap_dict, \
                                      'minsplit' : minsplit}}
      
      column_dict = {tc : {'category' : 'spl9', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list
  
  def process_sp10_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_spl5_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries 
    #with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' 
    #replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted 
    #to follow with an ordl encoding
    
    #sp10 is like spl5 but makes assumption that set of unique test values is
    #same or subset of train values for more efficient application in postmunge
    #that's spelled s p one zero
    '''
    
#     overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
        
      minsplit = params['minsplit'] - 1
    
    else:
      
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]

                  if extract == extract3:

                    extract_already_in_overlap_dict = True
                    
                    break
                    
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          break

                        
    #now for mdf_test we'll only consider those overlaps already 
    #identified from train set
    
#     unique_list_test = list(mdf_test[column].unique())

#     unique_list_test = list(map(str, unique_list_test))

#     test_overlap_dict = {}

#     train_keys = list(overlap_dict)

#     train_keys.sort(key = len, reverse=True)

#     for key in train_keys:

#       test_overlap_dict.update({key:[]})

#     for dict_key in train_keys:

#       for unique_test in unique_list_test:

#         len_key = len(dict_key)

#         if len(unique_test) >= len_key:

#           nbr_iterations4 = len(unique_test) - len_key

#           for l in range(nbr_iterations4 + 1):

#             extract4 = unique_test[l:(len_key+l)]

#             if extract4 == dict_key:

#               test_overlap_dict[dict_key].append(unique_test)

    #for sp10 we'll just copy train set overlap_dict
    test_overlap_dict = deepcopy(overlap_dict)
                        
    
    
    #so that was all comparable to splt, now for spl2 we'll create a new 
    #dictionary structred as
    #{original unique value : overlap extract for replacement}
    
    #since one original unique value may have entries as multiple overlaps, 
    #we'll prioritize overlaps with
    #longer string lengths and then alphabetical
    
    spl2_overlap_dict = {}
    
    overlap_key_list = list(overlap_dict)
    
    overlap_key_list.sort()
    overlap_key_list.sort(key = len, reverse=True)
    
    for overlap_key in overlap_key_list:
      
      for entry in overlap_dict[overlap_key]:
        
        if entry not in spl2_overlap_dict:
          
          spl2_overlap_dict.update({entry : overlap_key})
    
    #here's where we identify values to set to 0 for spl5
    spl5_zero_dict = {}
    for entry in unique_list:
      if entry not in spl2_overlap_dict:
        spl5_zero_dict.update({entry : 0})
          
    
    
    #then we'll do same for test set
    
#     spl2_test_overlap_dict = {}
    
#     test_overlap_key_list = list(test_overlap_dict)
    
#     test_overlap_key_list.sort()
#     test_overlap_key_list.sort(key = len, reverse=True)
    
#     for overlap_key in test_overlap_key_list:
      
#       for entry in test_overlap_dict[overlap_key]:
        
#         if entry not in spl2_test_overlap_dict:
          
#           spl2_test_overlap_dict.update({entry : overlap_key})
    
  
    spl2_test_overlap_dict = deepcopy(spl2_overlap_dict)
    
    unique_list_test = list(mdf_test[column].unique())
    unique_list_test = list(map(str, unique_list_test))
    
    
    #here's where we identify values to set to 0 for spl5
    spl5_test_zero_dict = {}
    for entry in unique_list_test:
      if entry not in spl2_test_overlap_dict:
        spl5_test_zero_dict.update({entry : 0})
    
    
    newcolumns = []

#     for dict_key in overlap_dict:

    newcolumn = column + '_sp10'
    
    mdf_train[newcolumn] = mdf_train[column].copy()
    mdf_test[newcolumn] = mdf_test[column].copy()
    
    mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
    mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl2_overlap_dict)
    mdf_train[newcolumn] = mdf_train[newcolumn].replace(spl5_zero_dict)

#       mdf_train[newcolumn] = mdf_train[column].isin(overlap_dict[dict_key])
#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
    mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

    newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'spl2_newcolumns'   : newcolumns, 
                                      'spl2_overlap_dict' : spl2_overlap_dict, \
                                      'spl2_test_overlap_dict' : spl2_test_overlap_dict, \
                                      'spl5_zero_dict' : spl5_zero_dict, \
                                      'unique_list' : unique_list, \
                                      'minsplit' : minsplit }}
      
      column_dict = {tc : {'category' : 'sp10', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list
  
  def process_sp15_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_splt_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 
    #'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    
    #sp15 is comparable to splt but multiple concurrent activations allowed
    #so requires a different MLinfilltype in processdict
    '''
    
    #overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]
    
    if 'minsplit' in params:
      minsplit = params['minsplit'] - 1
    else:
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
      
    #note that same MLinfilltype in processdict ('1010')
    #may be used for both configurations but applying concurrent_activations = False
    #with sp11 is less efficient then running splt
    if 'concurrent_activations' in params:
      concurrent_activations = params['concurrent_activations']
    else:
      concurrent_activations = True
      
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]
                  
                  if concurrent_activations is False:

                    if extract == extract3:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                  elif concurrent_activations is True:
                    
                    if extract == extract3 and unique in overlap_dict[key]:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                if extract_already_in_overlap_dict is True:
                  
                  break
                    

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for key in train_keys:

      test_overlap_dict.update({key:[]})

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
              
              if concurrent_activations is False:

                break
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:

      newcolumn = column + '_sp15_' + dict_key
      
      mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

      mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      
      mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

      newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'splt_newcolumns_sp15'   : newcolumns, \
                                      'minsplit' : minsplit, \
                                      'concurrent_activations' : concurrent_activations}}
      
      column_dict = {tc : {'category' : 'sp15', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []
    
    return mdf_train, mdf_test, column_dict_list
  
  def process_sp16_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_sp16_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 
    #'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    #comparable to splt but
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    
    #sp16 is comparable to spl8 but multiple concurrent activations allowed
    #so requires a different MLinfilltype in processdict
    '''
    
    #overlap_lengths = [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7 , 6, 5]

    if 'minsplit' in params:
      minsplit = params['minsplit'] - 1
    else:
      minsplit = 4
      
    if 'space_and_punctuation' in params:
      space_and_punctuation = params['space_and_punctuation']
    else:
      space_and_punctuation = True
      
    if 'excluded_characters' in params:
      excluded_characters = params['excluded_characters']
    else:
      excluded_characters = [' ', ',', '.', '?', '!', '(', ')']
      
    #note that same MLinfilltype in processdict ('1010')
    #may be used for both configurations but applying concurrent_activations = False
    #with sp12 is less efficient then running spl8
    if 'concurrent_activations' in params:
      concurrent_activations = params['concurrent_activations']
    else:
      concurrent_activations = True
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    overlap_dict = {}

    #we'll populate overlap_dict as
    #{extract_with_overlap : [list of associate categories with that overlap]}

    #we'll cycle through the overlap lengths and only record an overlap 
    #if it is not a subset of those already recorded
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:

        len_unique = len(unique)

        if len_unique >= overlap_length:

          nbr_iterations = len_unique - overlap_length

          for i in range(nbr_iterations + 1):

            extract = unique[i:(overlap_length+i)]

            extract_already_in_overlap_dict = False

            for key in overlap_dict:

              len_key = len(key)

              if len_key >= overlap_length:

                nbr_iterations3 = len_key - overlap_length

                for k in range(nbr_iterations3 + 1):

                  extract3 = key[k:(overlap_length+k)]
                    
                  if concurrent_activations is False:

                    if extract == extract3:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                  elif concurrent_activations is True:
                    
                    if extract == extract3 and unique in overlap_dict[key]:

                      extract_already_in_overlap_dict = True
                      
                      break
                      
                if extract_already_in_overlap_dict is True:
                  
                  break

            if extract_already_in_overlap_dict is False:

              for unique2 in unique_list:

                if unique2 != unique:

                  len_unique2 = len(unique2)

                  nbr_iterations2 = len_unique2 - overlap_length

                  for j in range(nbr_iterations2 + 1):

                    extract2 = unique2[j:(overlap_length+j)]

                    #________
                    
                    if space_and_punctuation is True:

                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break
                          
                    elif space_and_punctuation is False:
                      
                      for scrub_punctuation in excluded_characters:
                        
                        extract2 = extract2.replace(scrub_punctuation, '')
                        
                      #if any punctuation was scrubbed these two extracts will be different lengths
                      if extract2 == extract:

                        if extract in overlap_dict:

                          if unique2 not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique2)
                            
                            if concurrent_activations is False:

                              break

                          if unique not in overlap_dict[extract]:

                            overlap_dict[extract].append(unique)
                            
                            if concurrent_activations is False:

                              break

                        #else if we don't have a key for extract
                        else:

                          overlap_dict.update({extract : [unique, unique2]})
                          
                          if concurrent_activations is False:

                            break

                        
    #now for mdf_test we'll only consider those overlaps already identified from train set
    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    
    
#     unique_list_test = list(mdf_test[column].unique())

#     unique_list_test = list(map(str, unique_list_test))

#     test_overlap_dict = {}

#     train_keys = list(overlap_dict)

#     train_keys.sort(key = len, reverse=True)

#     for key in train_keys:

#       test_overlap_dict.update({key:[]})

#     for dict_key in train_keys:

#       for unique_test in unique_list_test:

#         len_key = len(dict_key)

#         if len(unique_test) >= len_key:

#           nbr_iterations4 = len(unique_test) - len_key

#           for l in range(nbr_iterations4 + 1):

#             extract4 = unique_test[l:(len_key+l)]

#             if extract4 == dict_key:

#               test_overlap_dict[dict_key].append(unique_test)
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:

      newcolumn = column + '_sp16_' + dict_key
      
      mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

      mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      
      #mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

      newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'splt_newcolumns_sp16'   : newcolumns, \
                                      'minsplit' : minsplit, \
                                      'concurrent_activations' : concurrent_activations}}
      
      column_dict = {tc : {'category' : 'sp16', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []

    
    return mdf_train, mdf_test, column_dict_list

  
  def process_srch_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    """
    #process_srch_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #note this differs from original srch in that makes use of pandas str.contains
    #which is expected to be more efficient for unbounded sets
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'
    
    #note that search parameter can include lists of search terms embedded in the list
    #which embedded lists will be aggregated to a single activation
    #for example if we want single activation for female names could pass search = [['Ms.', 'Miss', 'Mrs']] etc
    
    #note this returns all zeros in a column if search value not found
    
    #note returned coluymns are named by search term, e
    #e.g. column + '_srch_' + str(search)
    
    #note that search terms are converted to strings and compared to columns cast as strings

    #missing values are ignored by default
    """
        
    if 'search' in params:
      search = params['search']
    else:
      search = []
      
    if 'case' in params:
      case = params['case']
    else:
      case = True
      
      
    #we'll create mirror to account for any embdded lists of search terms for aggregation
    search_preflattening = search.copy()
    #this is kind of hacky just to reuse code below resetting this list to repopulate
    search = []
    aggregated_dict = {}
    
    for entry in search_preflattening:
      if type(entry) != type([]):
        search.append(str(entry))
      else:
        aggregated_dict.update({str(entry[-1]):[]})
        for entry2 in entry[0:-1]:
          search.append(entry2)
          aggregated_dict[str(entry[-1])].append(str(entry2))
        for entry2 in entry[-1:]:
          search.append(entry2)
    
    
    newcolumns = []
    search_dict = {}
    for searchitem in search:
      search_dict.update({column + '_srch_' + str(searchitem) : str(searchitem)})
      
    for newcolumn in search_dict:
      mdf_train[newcolumn] = \
      np.where(mdf_train[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)
      
      mdf_test[newcolumn] = \
      np.where(mdf_test[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)
                        
    
    newcolumns = list(search_dict)
    
    
    #now we'll address any aggregations fo search terms
    #from search parameter passed with embedded list of search terms
          
    #then after populating activations, we'll put this below
    #inverse_search_dict has key of search term and value of column for activations
    inverse_search_dict = {value:key for key,value in search_dict.items()}
    newcolumns_before_aggregation = newcolumns.copy()
    
    #now we consolidate activations
    #note that this only runs when aggregated_dict was populated with an embedded list of search terms
    for aggregated_dict_key in aggregated_dict:
      aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]
      
      for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
        target_for_aggregation_column = inverse_search_dict[target_for_aggregation]
        
        mdf_train[aggregated_dict_key_column] = \
        np.where(mdf_train[target_for_aggregation_column] == 1, 1, mdf_train[aggregated_dict_key_column])
        mdf_test[aggregated_dict_key_column] = \
        np.where(mdf_test[target_for_aggregation_column] == 1, 1, mdf_test[aggregated_dict_key_column])
        
        del mdf_train[target_for_aggregation_column]
        del mdf_test[target_for_aggregation_column]
        
        newcolumns.remove(target_for_aggregation_column)
    
    


    for newcolumn in newcolumns:

      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'search_dict' : search_dict, \
                                      'inverse_search_dict' : inverse_search_dict, \
                                      'srch_newcolumns_srch'   : newcolumns, \
                                      'newcolumns_before_aggregation' : newcolumns_before_aggregation, \
                                      'search' : search, \
                                      'search_preflattening' : search_preflattening, \
                                      'aggregated_dict' : aggregated_dict, \
                                      'case' : case}}
      
      column_dict = {tc : {'category' : 'srch', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []
    
    return mdf_train, mdf_test, column_dict_list

  
  
  def process_src2_class(self, mdf_train, mdf_test, column, category, \
                        postprocess_dict, params = {}):
    """
    #process_src2_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'

    #missing values are ignored by default
    
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    """
        
    if 'search' in params:
      search = params['search']
    else:
      search = []
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
#     maxlength = max(len(x) for x in unique_list)
    
#     overlap_lengths = list(range(maxlength - 1, minsplit, -1))


    #we'll create mirror to account for any embdded lists of search terms for aggregation
    search_preflattening = search.copy()
    #this is kind of hacky just to reuse code below resetting this list to repopulate
    search = []
    aggregated_dict = {}
    
    for entry in search_preflattening:
      if type(entry) != type([]):
        search.append(str(entry))
      else:
        aggregated_dict.update({str(entry[-1]):[]})
        for entry2 in entry[0:-1]:
          search.append(entry2)
          aggregated_dict[str(entry[-1])].append(str(entry2))
        for entry2 in entry[-1:]:
          search.append(entry2)

    

    #we'll populate overlap_dict as
    #{search_string : [list of associate categories with that overlap found]}

    
    overlap_dict = {}
    
    for search_string in search:
      
      overlap_dict.update({search_string : []})
    
    
    
    for search_string in search:
      
      len_search_string = len(search_string)
    
      for unique in unique_list:
        
        len_unique = len(unique)
        
        if len_unique >= len_search_string:
          
          nbr_iterations = len_unique - len_search_string
          
          for i in range(nbr_iterations + 1):
            
            extract = unique[i:(len_search_string+i)]
            
            if extract in search:
              
              overlap_dict[extract].append(unique)

                        
#     #now for mdf_test
    
#     unique_list_test = list(mdf_test[column].unique())

#     unique_list_test = list(map(str, unique_list_test))

#     test_overlap_dict = {}
    
#     for search_string in search:
      
#       test_overlap_dict.update({search_string : []})
    

#     train_keys = list(overlap_dict)

#     train_keys.sort(key = len, reverse=True)

#     for dict_key in train_keys:

#       for unique_test in unique_list_test:

#         len_key = len(dict_key)

#         if len(unique_test) >= len_key:

#           nbr_iterations4 = len(unique_test) - len_key

#           for l in range(nbr_iterations4 + 1):

#             extract4 = unique_test[l:(len_key+l)]

#             if extract4 == dict_key:

#               test_overlap_dict[dict_key].append(unique_test)
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:
      
      if len(overlap_dict[dict_key]) > 0:

        newcolumn = column + '_src2_' + dict_key

        mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

        mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
#         mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

        mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
        
    
    #now in case there are any aggregated activations, inspired by approach in srch
    inverse_search_dict = dict(zip(search, newcolumns))
    newcolumns_before_aggregation = newcolumns.copy()
    
    #now we consolidate activations
    #note that this only runs when aggregated_dict was populated with an embedded list of search terms
    for aggregated_dict_key in aggregated_dict:
      aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]
      
      for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
        target_for_aggregation_column = inverse_search_dict[target_for_aggregation]
        
        mdf_train[aggregated_dict_key_column] = \
        np.where(mdf_train[target_for_aggregation_column] == 1, 1, mdf_train[aggregated_dict_key_column])
        mdf_test[aggregated_dict_key_column] = \
        np.where(mdf_test[target_for_aggregation_column] == 1, 1, mdf_test[aggregated_dict_key_column])
        
        del mdf_train[target_for_aggregation_column]
        del mdf_test[target_for_aggregation_column]
        
        newcolumns.remove(target_for_aggregation_column)
        
    for newcolumn in newcolumns:
      mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)
      
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'src2_newcolumns_src2'   : newcolumns, \
                                      'newcolumns_before_aggregation' : newcolumns_before_aggregation, \
                                      'search' : search, \
                                      'inverse_search_dict' : inverse_search_dict, \
                                      'aggregated_dict' : aggregated_dict, \
                                      'search_preflattening' : search_preflattening}}
      
      column_dict = {tc : {'category' : 'src2', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []
    
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_src3_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    """
    #process_src3_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'

    #missing values are ignored by default
    
    #where srch is preferred for unbounded range of unique values
    
    #and src2 preferred when have bounded range of unique values for both train & test
    
    #and speculation is that src3 may be preferred when have a bounded
    #range of unique values but still want capacity to handle values in 
    #test set not found in train set
    """
        
    if 'search' in params:
      search = params['search']
    else:
      search = []
    
    #first we find overlaps from mdf_train
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
#     maxlength = max(len(x) for x in unique_list)
    
#     overlap_lengths = list(range(maxlength - 1, minsplit, -1))

    

    #we'll populate overlap_dict as
    #{search_string : [list of associate categories with that overlap found]}

    
    overlap_dict = {}
    
    for search_string in search:
      
      overlap_dict.update({search_string : []})
    
    
    
    for search_string in search:
      
      len_search_string = len(search_string)
    
      for unique in unique_list:
        
        len_unique = len(unique)
        
        if len_unique >= len_search_string:
          
          nbr_iterations = len_unique - len_search_string
          
          for i in range(nbr_iterations + 1):
            
            extract = unique[i:(len_search_string+i)]
            
            if extract in search:
              
              overlap_dict[extract].append(unique)


                        
    #now for mdf_test
    
    unique_list_test = list(mdf_test[column].unique())

    unique_list_test = list(map(str, unique_list_test))

    test_overlap_dict = {}
    
    for search_string in search:
      
      test_overlap_dict.update({search_string : []})
    

    train_keys = list(overlap_dict)

    train_keys.sort(key = len, reverse=True)

    for dict_key in train_keys:

      for unique_test in unique_list_test:

        len_key = len(dict_key)

        if len(unique_test) >= len_key:

          nbr_iterations4 = len(unique_test) - len_key

          for l in range(nbr_iterations4 + 1):

            extract4 = unique_test[l:(len_key+l)]

            if extract4 == dict_key:

              test_overlap_dict[dict_key].append(unique_test)
                        
                        
    
    newcolumns = []

    for dict_key in overlap_dict:
      
      if len(overlap_dict[dict_key]) > 0:

        newcolumn = column + '_src3_' + dict_key

        mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

        mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        mdf_train[newcolumn] = mdf_train[newcolumn].isin(overlap_dict[dict_key])
        mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)

        mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
    
    
    
    column_dict_list = []

    for tc in newcolumns:

      textnormalization_dict = {tc : {'overlap_dict' : overlap_dict, \
                                      'srch_newcolumns_src3'   : newcolumns, \
                                      'search' : search}}
      
      column_dict = {tc : {'category' : 'src3', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : newcolumns, \
                           'categorylist' : newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
    if len(newcolumns) == 0:
      
      column_dict_list = []
    
    return mdf_train, mdf_test, column_dict_list
  

  def process_src4_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    """
    #process_src4_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #note this differs from original srch in that makes use of pandas str.contains
    #which is expected to be more efficient for unbounded sets
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'
    
    #note this returns all zeros in a column if search value not found
    
    #note returned coluymns are named by search term, e
    #e.g. column + '_srch_' + str(search)
    
    #note that search terms are converted to strings and compared to columns cast as strings

    #missing values are ignored by default
    
    #src4 builds on the srch by converting to an ordinal activation
    #with 0 reserved for no activations
    #note that if an entry was activated for multiple search terms
    #the order of entries in search parameter will dictate the final encoding
    #(e.g. entries at end of list are prioritized over beginning)
    """
        
    if 'search' in params:
      search = params['search']
    else:
      search = []
      
    if 'case' in params:
      case = params['case']
    else:
      case = True
      
    #we'll create mirror to account for any embdded lists of search terms for aggregation
    search_preflattening = search.copy()
    #this is kind of hacky just to reuse code below resetting this list to repopulate
    search = []
    aggregated_dict = {}
    
    for entry in search_preflattening:
      if type(entry) != type([]):
        search.append(str(entry))
      else:
        aggregated_dict.update({str(entry[-1]):[]})
        for entry2 in entry[0:-1]:
          search.append(entry2)
          aggregated_dict[str(entry[-1])].append(str(entry2))
        for entry2 in entry[-1:]:
          search.append(entry2)
      
    
    newcolumns = []
    search_dict = {}
    for searchitem in search:
      search_dict.update({column + '_src4_' + str(searchitem) : str(searchitem)})
      
    for newcolumn in search_dict:
      mdf_train[newcolumn] = \
      np.where(mdf_train[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)
      
      mdf_test[newcolumn] = \
      np.where(mdf_test[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)
                        
    
    newcolumns = list(search_dict)

#     for newcolumn in newcolumns:

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(np.int8)
#       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)
      
    #ok now let's convert to ordinal for src4
    ordl_dict1 = {}
    ordl_dict2 = {}
    
    #reserve zero for no activations
    i = 1
    for newcolumn in newcolumns:
      ordl_dict1.update({i : newcolumn})
      ordl_dict2.update({newcolumn : i})
      i += 1
      
    mdf_train[column + '_src4'] = 0
    mdf_test[column + '_src4'] = 0
    
    for newcolumn in newcolumns:
      mdf_train[column + '_src4'] = \
      np.where(mdf_train[newcolumn] == 1, ordl_dict2[newcolumn], mdf_train[column + '_src4'])
      mdf_test[column + '_src4'] = \
      np.where(mdf_test[newcolumn] == 1, ordl_dict2[newcolumn], mdf_test[column + '_src4'])
      del mdf_train[newcolumn]
      del mdf_test[newcolumn]
      
      
    #now we'll address any aggregations fo search terms
    #from search parameter passed with embedded list of search terms
          
    #then after populating activations, we'll put this below
    #inverse_search_dict has key of search term and value of column for activations
    inverse_search_dict = {value:key for key,value in search_dict.items()}
#     newcolumns_before_aggregation = newcolumns.copy()
      
    #now we consolidate activations
    #note that this only runs when aggregated_dict was populated with an embedded list of search terms
    for aggregated_dict_key in aggregated_dict:
      aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]
      aggregated_dict_key_encoding = ordl_dict2[aggregated_dict_key_column]
      
      for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
        target_for_aggregation_column = inverse_search_dict[target_for_aggregation]
        target_for_aggregation_encoding = ordl_dict2[target_for_aggregation_column]
        
        mdf_train[column + '_src4'] = \
        np.where(mdf_train[column + '_src4'] == target_for_aggregation_encoding, aggregated_dict_key_encoding, mdf_train[column + '_src4'])
        mdf_test[column + '_src4'] = \
        np.where(mdf_test[column + '_src4'] == target_for_aggregation_encoding, aggregated_dict_key_encoding, mdf_test[column + '_src4'])

    
    #we'll base the integer type on number of ordinal entries
    if len(ordl_dict1) < 254:
      mdf_train[column + '_src4'] = mdf_train[column + '_src4'].astype(np.uint8)
      mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint8)
    elif len(ordl_dict1) < 65530:
      mdf_train[column + '_src4'] = mdf_train[column + '_src4'].astype(np.uint16)
      mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint16)
    else:
      mdf_train[column + '_src4'] = mdf_train[column + '_src4'].astype(np.uint32)
      mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint32)
    
    column_dict_list = []
    
    #newcolumns are based on the original srch transform
    #src4_newcolumns are after consolidating to ordinal encoding (single entry)
    src4_newcolumns = [column + '_src4']

    for tc in src4_newcolumns:

      textnormalization_dict = {tc : {'search_dict' : search_dict, \
                                      'inverse_search_dict' : inverse_search_dict, \
                                      'srch_newcolumns_src4' : newcolumns, \
                                      'src4_newcolumns' : src4_newcolumns, \
                                      'search' : search, \
                                      'search_preflattening' : search_preflattening, \
                                      'aggregated_dict' : aggregated_dict, \
                                      'case' : case, \
                                      'ordl_dict1' : ordl_dict1, \
                                      'ordl_dict2' : ordl_dict2}}
      
      column_dict = {tc : {'category' : 'src4', \
                           'origcategory' : category, \
                           'normalization_dict' : textnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : src4_newcolumns, \
                           'categorylist' : src4_newcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())

    
    return mdf_train, mdf_test, column_dict_list
  
  def process_strn_class(self, df, column, category, postprocess_dict, params = {}):
    """
    #process_strn_class(df, column, category, postprocess_dict)
    #parses string entries and if any strings present returns longest string
    #i.e. character subsets excluding numerical entries
    #entries without strings present subject to infill
    """
    
    unique_list = list(df[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]
                  
                  has_number = False
                  
                  for j in range(len(extract)):
                    
                    if self.is_number(extract[j]):
                      
                      has_number = True

  #                 extract_already_in_overlap_dict = False

                  if has_number is False:

                    overlap_dict.update({unique : extract})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False
  
  
                  has_number = False
                  
                  for j in range(len(extract)):
                    
                    if self.is_number(extract[j]):
                      
                      has_number = True

  #                 extract_already_in_overlap_dict = False

                  if has_number is False:
      
                    in_dict = True

                    overlap_dict.update({unique : extract})

                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    df[column + '_strn'] = df[column].astype(str)
    df[column + '_strn'] = df[column + '_strn'].replace(overlap_dict)
    

    #replace missing data with training set mean as default infill
    df[column + '_strn'] = df[column + '_strn'].fillna('zzzinfill')
    
    
#     #a few more metrics collected for driftreport
#     #get maximum value of training column
#     maximum = df[column + '_nmrc'].max()
#     #get minimum value of training column
#     minimum = df[column + '_nmrc'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_strn']


    nmbrnormalization_dict = {column + '_strn' : {'overlap_dict' : overlap_dict}}
#                                                   'mean' : mean, \
#                                                   'maximum' : maximum, \
#                                                   'minimum' : minimum }}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'strn', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return df, column_dict_list

  
  def process_nmrc_class(self, df, column, category, postprocess_dict, params = {}):
    """
    #process_nmrc_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    """
    
    unique_list = list(df[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    df[column + '_nmrc'] = df[column].astype(str)
    df[column + '_nmrc'] = df[column + '_nmrc'].replace(overlap_dict)
    


    #get mean of training data
    mean = df[column + '_nmrc'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    df[column + '_nmrc'] = df[column + '_nmrc'].fillna(mean)
    
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = df[column + '_nmrc'].max()
    #get minimum value of training column
    minimum = df[column + '_nmrc'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmrc']


    nmbrnormalization_dict = {column + '_nmrc' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum }}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmrc', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return df, column_dict_list

  def process_nmr4_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    """
    #process_nmr4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #assumes set of entries in test data is same or subset of train data for
    #more efficient postmunge than vs nmrc
    """
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    mdf_train[column + '_nmr4'] = mdf_train[column].astype(str)
    mdf_train[column + '_nmr4'] = mdf_train[column + '_nmr4'].replace(overlap_dict)
    
    #do same for test data
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    for test_unique in extra_test_unique:
      test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmr4'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmr4'] = mdf_test[column + '_nmr4'].replace(test_overlap_dict)
    

    #get mean of training data
    mean = mdf_train[column + '_nmr4'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    mdf_train[column + '_nmr4'] = mdf_train[column + '_nmr4'].fillna(mean)
    mdf_test[column + '_nmr4'] = mdf_test[column + '_nmr4'].fillna(mean)
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_nmr4'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_nmr4'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmr4']


    nmbrnormalization_dict = {column + '_nmr4' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum, \
                                                  'unique_list' : unique_list}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmr4', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return mdf_train, mdf_test, column_dict_list
  
  def process_nmr7_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    """
    #process_nmr4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #comparable to nmr4 but instead of making blanket assumption that unique values in
    #test set are found in train set, implements parsing for test set entries not found in train set
    """
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    mdf_train[column + '_nmr7'] = mdf_train[column].astype(str)
    mdf_train[column + '_nmr7'] = mdf_train[column + '_nmr7'].replace(overlap_dict)
    
    #do same for test data
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    
    
#     unique_list = list(mdf_train[column].unique())

#     unique_list = list(map(str, unique_list))
    
    testmaxlength = max(len(x) for x in unique_list)
    
    if testmaxlength > maxlength:
      maxlength = testmaxlength
    
    overlap_lengths = list(range(maxlength, 0, -1))

#     overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in extra_test_unique:
        
        if unique not in test_overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    test_overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    test_overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                test_overlap_dict.update({unique : np.nan})
    
    
#     for test_unique in extra_test_unique:
#       test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmr7'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmr7'] = mdf_test[column + '_nmr7'].replace(test_overlap_dict)
    

    #get mean of training data
    mean = mdf_train[column + '_nmr7'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    mdf_train[column + '_nmr7'] = mdf_train[column + '_nmr7'].fillna(mean)
    mdf_test[column + '_nmr7'] = mdf_test[column + '_nmr7'].fillna(mean)
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_nmr7'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_nmr7'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmr7']


    nmbrnormalization_dict = {column + '_nmr7' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum, \
                                                  'unique_list' : unique_list, \
                                                  'maxlength' : maxlength}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmr7', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_nmcm_class(self, df, column, category, postprocess_dict, params = {}):
    """
    #process_nmcm_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #the check for numbers strips commas and returned numbers have commas stripped
    #entries without numbers present subject to infill
    """
    
    unique_list = list(df[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    df[column + '_nmcm'] = df[column].astype(str)
    df[column + '_nmcm'] = df[column + '_nmcm'].replace(overlap_dict)
    


    #get mean of training data
    mean = df[column + '_nmcm'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    df[column + '_nmcm'] = df[column + '_nmcm'].fillna(mean)
    
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = df[column + '_nmcm'].max()
    #get minimum value of training column
    minimum = df[column + '_nmcm'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmcm']


    nmbrnormalization_dict = {column + '_nmcm' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum }}


    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmcm', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return df, column_dict_list
  
  def process_nmc4_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    """
    #process_nmc4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #the check for numbers strips commas and returned numbers have commas stripped
    #entries without numbers present subject to infill
    #assumes set of entries in test data is same or subset of train data for
    #more efficient postmunge than vs nmc
    """
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    mdf_train[column + '_nmc4'] = mdf_train[column].astype(str)
    mdf_train[column + '_nmc4'] = mdf_train[column + '_nmc4'].replace(overlap_dict)
    
    
    #do same for test data
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    test_unique_list = list(map(str, test_unique_list))
    for test_unique in extra_test_unique:
      test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmc4'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmc4'] = mdf_test[column + '_nmc4'].replace(test_overlap_dict)
    

    #get mean of training data
    mean = mdf_train[column + '_nmc4'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    mdf_train[column + '_nmc4'] = mdf_train[column + '_nmc4'].fillna(mean)
    mdf_test[column + '_nmc4'] = mdf_test[column + '_nmc4'].fillna(mean)
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_nmc4'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_nmc4'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmc4']


    nmbrnormalization_dict = {column + '_nmc4' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum, \
                                                  'unique_list' : unique_list}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmc4', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return mdf_train, mdf_test, column_dict_list
  
  def process_nmc7_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    """
    #process_nmc7_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #the check for numbers strips commas and returned numbers have commas stripped
    #entries without numbers present subject to infill
    #assumes set of entries in test data is same or subset of train data for
    #more efficient postmunge than vs nmc
    #comparable to nmc4 but instead of making blanket assumption that unique values in
    #test set are found in train set, implements parsing for test set entries not found in train set
    """
    
    unique_list = list(mdf_train[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    in_dict = True

                    overlap_dict.update({unique : float(extract.replace(',',''))})
                  
              if in_dict is False:

                overlap_dict.update({unique : np.nan})
    
    mdf_train[column + '_nmc7'] = mdf_train[column].astype(str)
    mdf_train[column + '_nmc7'] = mdf_train[column + '_nmc7'].replace(overlap_dict)
    
    
    #do same for test data
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
#     for test_unique in extra_test_unique:
#       test_overlap_dict.update({str(test_unique) : np.nan})


#     unique_list = list(mdf_train[column].unique())

#     unique_list = list(map(str, unique_list))
    
    testmaxlength = max(len(x) for x in unique_list)
    
    if testmaxlength > maxlength:
      maxlength = testmaxlength
    
    overlap_lengths = list(range(maxlength, 0, -1))

#     overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in extra_test_unique:
        
        if unique not in test_overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    test_overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    test_overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                test_overlap_dict.update({unique : np.nan})
    
    
#     for test_unique in extra_test_unique:
#       test_overlap_dict.update({str(test_unique) : np.nan})
    

    
    mdf_test[column + '_nmc7'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmc7'] = mdf_test[column + '_nmc7'].replace(test_overlap_dict)
    

    #get mean of training data
    mean = mdf_train[column + '_nmc7'].mean()
    if mean != mean:
      mean = 0

    #replace missing data with training set mean as default infill
    mdf_train[column + '_nmc7'] = mdf_train[column + '_nmc7'].fillna(mean)
    mdf_test[column + '_nmc7'] = mdf_test[column + '_nmc7'].fillna(mean)
    
    #a few more metrics collected for driftreport
    #get maximum value of training column
    maximum = mdf_train[column + '_nmc7'].max()
    #get minimum value of training column
    minimum = mdf_train[column + '_nmc7'].min()
    
    
    #create list of columns
    nmbrcolumns = [column + '_nmc7']


    nmbrnormalization_dict = {column + '_nmc7' : {'overlap_dict' : overlap_dict, \
                                                  'mean' : mean, \
                                                  'maximum' : maximum, \
                                                  'minimum' : minimum, \
                                                  'unique_list' : unique_list, \
                                                  'maxlength' : maxlength}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    
    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'nmc7', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
        
    return mdf_train, mdf_test, column_dict_list
  
    
  def process_ordl_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_ordl_class(mdf_train, mdf_test, column, category)
    #preprocess column with categories into ordinal (sequentuial integer) sets
    #corresponding to (sorted) categories
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories presetn in test set not present in train set use this 'zzz' category
    '''
    
    #create new column for trasnformation
    mdf_train[column + '_ordl'] = mdf_train[column].copy()
    mdf_test[column + '_ordl'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype('category')
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')

    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_train[column + '_ordl'].cat.categories:
      mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].cat.add_categories(['zzzinfill'])
    if 'zzzinfill' not in mdf_test[column + '_ordl'].cat.categories:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].fillna('zzzinfill')
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].fillna('zzzinfill')

    #replace numerical with string equivalent
    mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(str)
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(str)
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    labels_train = list(mdf_train[column + '_ordl'].unique())
    labels_train.sort()
    labels_test = list(mdf_test[column + '_ordl'].unique())
    labels_test.sort()

    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
      labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
    
    listlength = len(labels_train)
    
    #____
    #quick check if there are any overlaps between binary encodings and prior unique values in the column
    #as would interfere with the replacement operation
    #(I know this is an outlier scenario, just trying to be thorough)
    
    overlap_list = []
    overlap_replace = {}
    for value in labels_train:
      if value in range(listlength):
        overlap_list.append(value)
        
        #here's what we'll replace with, the string suffix is arbitrary and intended as not likely to be in set
        overlap_replace.update({value : value + 'encoding_overlap'})
        
    
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_list) > 0:
      mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].replace(overlap_replace)
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(overlap_replace)
      
      #then we'll redo the encodings
      
      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array
      labels_train = list(mdf_train[column + '_ordl'].unique())
      labels_train.sort()
      labels_test = list(mdf_test[column + '_ordl'].unique())
      labels_test.sort()
      
    #clear up memory
    del overlap_list
    
    #____
    
    
    #get length of the list, then zip a dictionary from list and range(length)
    #the range values will be our ordinal points to replace the categories
    listlength = len(labels_train)
    ordinal_dict = dict(zip(labels_train, range(listlength)))
    
    #replace the cateogries in train set via ordinal trasnformation
    mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].replace(ordinal_dict)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(ordinal_dict)
    
    #just want to make sure these arent' being saved as floats for memory considerations
    if len(ordinal_dict) < 254:
      mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(np.uint8)
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint8)
    elif len(ordinal_dict) < 65530:
      mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(np.uint16)
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint16)
    else:
      mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(np.uint32)
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint32)
    
#     #convert column to category
#     mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype('category')
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')

#     #change data type for memory savings
#     mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(np.int32)
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.int32)

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for key in ordinal_dict:
      sumcalc = (mdf_train[column+'_ordl'] == ordinal_dict[key]).sum() 
      ratio = sumcalc / mdf_train[column+'_ordl'].shape[0]
      ordl_activations_dict.update({key:ratio})
    
    categorylist = [column + '_ordl']  
        
    column_dict_list = []
    
    for tc in categorylist:
        
      normalization_dict = {tc : {'ordinal_dict' : ordinal_dict, \
                                  'ordinal_overlap_replace' : overlap_replace, \
                                  'ordl_activations_dict' : ordl_activations_dict}}
    
      column_dict = {tc : {'category' : 'ordl', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : categorylist, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
    
    return mdf_train, mdf_test, column_dict_list
  

  def process_ord3_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_ord3_class(mdf_train, mdf_test, column, category)
    #preprocess column with categories into ordinal (sequentuial integer) sets
    #corresponding to categories sorted by frequency of occurance
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories presetn in test set not present in train set use this 'zzz' category
    '''
    
    #create new column for trasnformation
    mdf_train[column + '_ord3'] = mdf_train[column].copy()
    mdf_test[column + '_ord3'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].astype('category')
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype('category')

    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_train[column + '_ord3'].cat.categories:
      mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].cat.add_categories(['zzzinfill'])
    if 'zzzinfill' not in mdf_test[column + '_ord3'].cat.categories:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].fillna('zzzinfill')
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].fillna('zzzinfill')

    #replace numerical with string equivalent
    mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].astype(str)
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(str)
    
    #extract categories for column labels
    #with values sorted by frequency of occurance from most to least
    labels_train = pd.DataFrame(mdf_train[column + '_ord3'].value_counts())
    labels_train = labels_train.rename_axis('zzzinfill').sort_values(by = [column + '_ord3', 'zzzinfill'], ascending = [False, True])
    labels_train = list(labels_train.index)
    
#     labels_train = list(mdf_train[column + '_ordl'].unique())
#     labels_train.sort()
    labels_test = list(mdf_test[column + '_ord3'].unique())
    labels_test.sort()

    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
#       labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
    
    listlength = len(labels_train)
    
    #____
    #quick check if there are any overlaps between binary encodings and prior unique values in the column
    #as would interfere with the replacement operation
    #(I know this is an outlier scenario, just trying to be thorough)
    
    overlap_list = []
    overlap_replace = {}
    for value in labels_train:
      if value in range(listlength):
        overlap_list.append(value)
        
        #here's what we'll replace with, the string suffix is arbitrary and intended as not likely to be in set
        overlap_replace.update({value : value + 'encoding_overlap'})
        
    
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_list) > 0:
      mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].replace(overlap_replace)
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(overlap_replace)
      
      #then we'll redo the encodings
      
      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array
      labels_train = pd.DataFrame(mdf_train[column + '_ord3'].value_counts())
      labels_train = labels_train.rename_axis('zzzinfill').sort_values(by = [column + '_ord3', 'zzzinfill'], ascending = [False, True])
      labels_train = list(labels_train.index)
      
#       labels_train = list(mdf_train[column + '_ord2'].unique())
#       labels_train.sort()
      labels_test = list(mdf_test[column + '_ord3'].unique())
      labels_test.sort()
      
    #clear up memory
    del overlap_list
    
    #____
    
    
    #get length of the list, then zip a dictionary from list and range(length)
    #the range values will be our ordinal points to replace the categories
    listlength = len(labels_train)
    ordinal_dict = dict(zip(labels_train, range(listlength)))
    
    #replace the cateogries in train set via ordinal trasnformation
    mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].replace(ordinal_dict)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(ordinal_dict)
    
    #just want to make sure these arent' being saved as floats for memory considerations
    if len(ordinal_dict) < 254:
      mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].astype(np.uint8)
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint8)
    elif len(ordinal_dict) < 65530:
      mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].astype(np.uint16)
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint16)
    else:
      mdf_train[column + '_ord3'] = mdf_train[column + '_ord3'].astype(np.uint32)
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint32)
    
#     #convert column to category
#     mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype('category')
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')

#     #change data type for memory savings
#     mdf_train[column + '_ordl'] = mdf_train[column + '_ordl'].astype(np.int32)
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.int32)

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for key in ordinal_dict:
      sumcalc = (mdf_train[column+'_ord3'] == ordinal_dict[key]).sum() 
      ratio = sumcalc / mdf_train[column+'_ord3'].shape[0]
      ordl_activations_dict.update({key:ratio})
    
    categorylist = [column + '_ord3']  
        
    column_dict_list = []
    
    for tc in categorylist:
        
      normalization_dict = {tc : {'ordinal_dict' : ordinal_dict, \
                                  'ordinal_overlap_replace' : overlap_replace, \
                                  'ordl_activations_dict' : ordl_activations_dict}}
    
      column_dict = {tc : {'category' : 'ord3', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : categorylist, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
    
    return mdf_train, mdf_test, column_dict_list
  
  def process_ucct_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_ucct_class(mdf_train, mdf_test, column, category)
    #preprocess column with categories into unique class count sets
    #normalized by total row count
    #e.g. for each class in train set, 
    #counts instances and divides by total train set row count
    #(so values will fall in range 0-1)
    #test sets recive comparable encoding
    '''
    
    #create new column for trasnformation
    mdf_train[column + '_ucct'] = mdf_train[column].copy()
    mdf_test[column + '_ucct'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].astype('category')
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].astype('category')

    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_train[column + '_ucct'].cat.categories:
      mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].cat.add_categories(['zzzinfill'])
    if 'zzzinfill' not in mdf_test[column + '_ucct'].cat.categories:
      mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].fillna('zzzinfill')
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].fillna('zzzinfill')

    #replace numerical with string equivalent
    mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].astype(str)
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].astype(str)
    
    #extract categories for column labels
    #with values sorted by frequency of occurance from most to least
    labels_train = pd.DataFrame(mdf_train[column + '_ucct'].value_counts())
    labels_train = labels_train.rename_axis('zzzinfill').sort_values(by = [column + '_ucct', 'zzzinfill'], ascending = [False, True])
    labels_train = list(labels_train.index)
    
#     labels_train = list(mdf_train[column + '_ordl'].unique())
#     labels_train.sort()
    labels_test = list(mdf_test[column + '_ucct'].unique())
    labels_test.sort()

    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
#       labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
    
    listlength = len(labels_train)
    
    #____
    #quick check if there are any overlaps between binary encodings and prior unique values in the column
    #as would interfere with the replacement operation
    #(I know this is an outlier scenario, just trying to be thorough)
    
    overlap_list = []
    overlap_replace = {}
    for value in labels_train:
      if value in range(listlength):
        overlap_list.append(value)
        
        #here's what we'll replace with, the string suffix is arbitrary and intended as not likely to be in set
        overlap_replace.update({value : value + 'encoding_overlap'})
        
    
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_list) > 0:
      mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].replace(overlap_replace)
      mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(overlap_replace)
      
      #then we'll redo the encodings
      
      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array
      labels_train = pd.DataFrame(mdf_train[column + '_ucct'].value_counts())
      labels_train = labels_train.rename_axis('zzzinfill').sort_values(by = [column + '_ucct', 'zzzinfill'], ascending = [False, True])
      labels_train = list(labels_train.index)
      
#       labels_train = list(mdf_train[column + '_ord2'].unique())
#       labels_train.sort()
      labels_test = list(mdf_test[column + '_ucct'].unique())
      labels_test.sort()
      
    #clear up memory
    del overlap_list
    
    #____
    
    
    #assemble the ordinal_dict
    #with key of class and value of normalized unique class count
    ordinal_dict = {}
    rowcount = mdf_train.shape[0]
    
    for item in labels_train:
      item_count = mdf_train[mdf_train[column + '_ucct'] == item].shape[0]
      ordinal_dict.update({item: item_count / rowcount})
    
    
    #replace the cateogries in train set via ordinal trasnformation
    mdf_train[column + '_ucct'] = mdf_train[column + '_ucct'].replace(ordinal_dict)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(ordinal_dict)
    

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for key in ordinal_dict:
      sumcalc = (mdf_train[column+'_ucct'] == ordinal_dict[key]).sum() 
      ratio = sumcalc / mdf_train[column+'_ucct'].shape[0]
      ordl_activations_dict.update({key:ratio})
    
    categorylist = [column + '_ucct']  
        
    column_dict_list = []
    
    for tc in categorylist:
        
      normalization_dict = {tc : {'ordinal_dict' : ordinal_dict, \
                                  'ordinal_overlap_replace' : overlap_replace, \
                                  'ordl_activations_dict' : ordl_activations_dict}}
    
      column_dict = {tc : {'category' : 'ucct', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : categorylist, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
    
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_1010_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_1010_class(mdf_train, mdf_test, column, category)
    #preprocess column with categories into binary encoded sets
    #corresponding to (sorted) categories of >2 values
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories present in test set not present in train set uses this 'zzzinfill' category
    '''
    
    #create new column for trasnformation
    mdf_train[column + '_1010'] = mdf_train[column].copy()
    mdf_test[column + '_1010'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_train[column + '_1010'] = mdf_train[column + '_1010'].astype('category')
    mdf_test[column + '_1010'] = mdf_test[column + '_1010'].astype('category')

    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_train[column + '_1010'].cat.categories:
      mdf_train[column + '_1010'] = mdf_train[column + '_1010'].cat.add_categories(['zzzinfill'])
    if 'zzzinfill' not in mdf_test[column + '_1010'].cat.categories:
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_train[column + '_1010'] = mdf_train[column + '_1010'].fillna('zzzinfill')
    mdf_test[column + '_1010'] = mdf_test[column + '_1010'].fillna('zzzinfill')

    #replace numerical with string equivalent
    mdf_train[column + '_1010'] = mdf_train[column + '_1010'].astype(str)
    mdf_test[column + '_1010'] = mdf_test[column + '_1010'].astype(str)
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    labels_train = list(mdf_train[column + '_1010'].unique())
    labels_train.sort()
    labels_test = list(mdf_test[column + '_1010'].unique())
    labels_test.sort()

    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
      labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
    
    #get length of the list
    listlength = len(labels_train)
    
    #calculate number of columns we'll need
    #currently using numk;py since already imported, this could also be done with math library
    binary_column_count = int(np.ceil(np.log2(listlength)))
    
    #initialize dictionaryt to store encodings
    binary_encoding_dict = {}
    encoding_list = []
    
    for i in range(listlength):
      
      #this converts the integer i to binary encoding
      #where f is an f string for inserting the column coount into the string to designate length of encoding
      #0 is to pad out the encoding with 0's for the length
      #and b is telling it to convert to binary 
      #note this returns a string
      encoding = format(i, f"0{binary_column_count}b")
      
      if i < len(labels_train):

        #store the encoding in a dictionary
        binary_encoding_dict.update({labels_train[i] : encoding})

        #store the encoding in a list for checking in next step
        encoding_list.append(encoding)

    #____
    #quick check if there are any overlaps between binary encodings and prior unique values in the column
    #as would interfere with the replacement operation
    #(I know this is an outlier scenario, just trying to be thorough)
    
    overlap_list = []
    overlap_replace = {}
    for value in labels_train:
      if value in encoding_list:
        overlap_list.append(value)
        
        #here's what we'll replace with, the string suffix is arbitrary and intended as not likely to be in set
        overlap_replace.update({value : value + 'encoding_overlap'})
        
    
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_list) > 0:
      mdf_train[column + '_1010'] = mdf_train[column + '_1010'].replace(overlap_replace)
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(overlap_replace)
      
      #then we'll redo the encodings
      
      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array
      labels_train = list(mdf_train[column + '_1010'].unique())
      labels_train.sort()
      labels_test = list(mdf_test[column + '_1010'].unique())
      labels_test.sort()
      
      #initialize dictionaryt to store encodings
      binary_encoding_dict = {}
      encoding_list = []

      for i in range(listlength):

        #this converts the integer i to binary encoding
        #where f is an f string for inserting the column coount into the string to designate length of encoding
        #0 is to pad out the encoding with 0's for the length
        #and b is telling it to convert to binary 
        #note this returns a string
        encoding = format(i, f"0{binary_column_count}b")
        
        if i < len(labels_train):

          #store the encoding in a dictionary
          binary_encoding_dict.update({labels_train[i] : encoding})

          #store the encoding in a list for checking in next step
          encoding_list.append(encoding)

      
    #clear up memory
    del encoding_list
    del overlap_list
    
    #new driftreport metric _1010_activations_dict
    _1010_activations_dict = {}
    for key in binary_encoding_dict:
      sumcalc = (mdf_train[column+'_1010'] == key).sum() 
      ratio = sumcalc / mdf_train[column+'_1010'].shape[0]
      _1010_activations_dict.update({key:ratio})
    
    #____
    
    #replace the cateogries in train set via ordinal trasnformation
    mdf_train[column + '_1010'] = mdf_train[column + '_1010'].replace(binary_encoding_dict)      
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(testplug_dict)    
    
    #now we'll apply the 1010 transformation to the test set
    mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(binary_encoding_dict)    

    
    #ok let's create a list of columns to store each entry of the binary encoding
    _1010_columnlist = []
    
    for i in range(binary_column_count):
      
      _1010_columnlist.append(column + '_1010_' + str(i))
      
    #now let's store the encoding
    i=0
    for _1010_column in _1010_columnlist:
      
      mdf_train[_1010_column] = mdf_train[column + '_1010'].str.slice(i,i+1).astype(np.int8)
      
      mdf_test[_1010_column] = mdf_test[column + '_1010'].str.slice(i,i+1).astype(np.int8)
      
      i+=1

      
    #now delete the support column
    del mdf_train[column + '_1010']
    del mdf_test[column + '_1010']
    
    
    #now store the column_dict entries
    
    categorylist = _1010_columnlist
        
    column_dict_list = []
    
    for tc in categorylist:
        
      normalization_dict = {tc : {'_1010_binary_encoding_dict' : binary_encoding_dict, \
                                  '_1010_overlap_replace' : overlap_replace, \
                                  '_1010_binary_column_count' : binary_column_count, \
                                  '_1010_activations_dict' : _1010_activations_dict}}
    
      column_dict = {tc : {'category' : '1010', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : categorylist, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    
    
    return mdf_train, mdf_test, column_dict_list
  


  def process_bshr_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton depending on input format of datetime data 
    #that creates a boolean column indicating 1 for rows
    #corresponding to traditional business hours in source column
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #initialize parameters
    if 'start' in params:
      start = params['start']
    else:
      start = 9
      
    if 'end' in params:
      end = params['end']
    else:
      end = 17
    
    #convert improperly formatted values to datetime in new column
    df[column+'_bshr'] = pd.to_datetime(df[column], errors = 'coerce')
    
    #This is kind of hack for whole hour increments, if we were needing
    #to evlauate hour ranges between seperate days a different metod
    #would be required
    #For now we'll defer to Dollly Parton
    df[column+'_bshr'] = df[column+'_bshr'].dt.hour
    df[column+'_bshr'] = df[column+'_bshr'].between(start, end)
    
    #reduce memory footprint
    df[column+'_bshr'] = df[column+'_bshr'].astype(np.int8)
    
    
    #create list of columns
    datecolumns = [column + '_bshr']

    #grab some driftreport metrics
    activationratio = df[column + '_bshr'].sum() / df[column + '_bshr'].shape[0]

    #create normalization dictionary
    normalization_dict = {column + '_bshr' : {'activationratio' : activationratio}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for dc in datecolumns:

      column_dict = { dc : {'category' : 'bshr', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : [dc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False, \
                           'downstream':[]}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list



  def process_wkdy_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton depending on input format of datetime data 
    #that creates a boolean column indicating 1 for rows
    #corresponding to weekdays in source column
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #convert improperly formatted values to datetime in new column
    df[column+'_wkdy'] = pd.to_datetime(df[column], errors = 'coerce')
    
    #This is kind of hack for whole hour increments, if we were needing
    #to evlauate hour ranges between seperate days a different metod
    #would be required
    #For now we'll defer to Dollly Parton
    df[column+'_wkdy'] = pd.DatetimeIndex(df[column+'_wkdy']).dayofweek
    
    df[column+'_wkdy'] = df[column+'_wkdy'].between(0,4)
    
    #reduce memory footprint
    df[column+'_wkdy'] = df[column+'_wkdy'].astype(np.int8)
    
    
    #create list of columns
    datecolumns = [column+'_wkdy']

    #grab some driftreport metrics
    activationratio = df[column + '_wkdy'].sum() / df[column + '_wkdy'].shape[0]

    #create normalization dictionary
    normalization_dict = {column + '_wkdy' : {'activationratio' : activationratio}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for dc in datecolumns:

      column_dict = { dc : {'category' : 'wkdy', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : [dc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False, \
                           'downstream':[]}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list




  def process_hldy_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton depending on input format of datetime data 
    #that creates a boolean column indicating 1 for rows
    #corresponding to US Federal Holidays in source column
    #note this is a "singleprocess" function since is applied to single dataframe
    '''
    
    #initialize parameters
    if 'holiday_list' in params:
      holiday_list = params['holiday_list']
    else:
      holiday_list = []
    
    if len(holiday_list) > 0:
    
      #reformat holiday_list
      holiday_list = pd.to_datetime(pd.DataFrame(holiday_list)[0], errors = 'coerce')

      #reform holiday_list again
      timestamp_list = []

      for row in range(holiday_list.shape[0]):
        timestamp = pd.Timestamp(holiday_list[row])
        timestamp_list += [timestamp]
      timestamp_list
      
    else:
      timestamp_list = []
    
    
    #convert improperly formatted values to datetime in new column
    df[column+'_hldy'] = pd.to_datetime(df[column], errors = 'coerce')
    
    df[column+'_hldy'] = df[column+'_hldy'].dt.date
    
    df[column+'_hldy'] = pd.to_datetime(df[column+'_hldy'], errors = 'coerce')
    
    
    #grab list of holidays from import
    holidays = USFederalHolidayCalendar().holidays().tolist()

    holidays += timestamp_list
    
    #activate boolean identifier for holidays
    df[column+'_hldy'] = df[column+'_hldy'].isin(holidays)

    #reduce memory footprint
    df[column+'_hldy'] = df[column+'_hldy'].astype(np.int8)
    
    #create list of columns
    datecolumns = [column + '_hldy']

    #grab some driftreport metrics
    activationratio = df[column + '_hldy'].sum() / df[column + '_hldy'].shape[0]

    #create normalization dictionary
    normalization_dict = {column + '_hldy' : {'activationratio' : activationratio}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for dc in datecolumns:

      column_dict = { dc : {'category' : 'hldy', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : [dc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False, \
                           'downstream':[]}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list
  
  
  def process_wkds_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton depending on input format of datetime data 
    #that creates a categorical column 
    #corresponding to weekdays in source column
    #note this is a "singleprocess" function since is applied to single dataframe
    #defdault infill is eight days a week
    '''
    
    #convert improperly formatted values to datetime in new column
    df[column+'_wkds'] = pd.to_datetime(df[column], errors = 'coerce')
    
    #This is kind of hack for whole hour increments, if we were needing
    #to evlauate hour ranges between seperate days a different metod
    #would be required
    #For now we'll defer to Dollly Parton
    df[column+'_wkds'] = pd.DatetimeIndex(df[column+'_wkds']).dayofweek
    
#     df[column+'_wkdy'] = df[column+'_wkdy'].between(0,4)

    #we'll use convention for default infill of eight days a week
    df[column + '_wkds'] = df[column + '_wkds'].fillna(7)
    
    #reduce memory footprint
    df[column+'_wkds'] = df[column+'_wkds'].astype(np.int8)
    
    
    #create list of columns
    datecolumns = [column+'_wkds']

    #grab some driftreport metrics
    numberofrows = df[column + '_wkds'].shape[0]
    mon_ratio = df[df[column + '_wkds'] == 0].shape[0] / numberofrows
    tue_ratio = df[df[column + '_wkds'] == 1].shape[0] / numberofrows
    wed_ratio = df[df[column + '_wkds'] == 2].shape[0] / numberofrows
    thr_ratio = df[df[column + '_wkds'] == 3].shape[0] / numberofrows
    fri_ratio = df[df[column + '_wkds'] == 4].shape[0] / numberofrows
    sat_ratio = df[df[column + '_wkds'] == 5].shape[0] / numberofrows
    sun_ratio = df[df[column + '_wkds'] == 6].shape[0] / numberofrows
    infill_ratio = df[df[column + '_wkds'] == 7].shape[0] / numberofrows
  
  
    #create normalization dictionary
    normalization_dict = {column+'_wkds' : {'mon_ratio' : mon_ratio, \
                                            'tue_ratio' : tue_ratio, \
                                            'wed_ratio' : wed_ratio, \
                                            'thr_ratio' : thr_ratio, \
                                            'fri_ratio' : fri_ratio, \
                                            'sat_ratio' : sat_ratio, \
                                            'sun_ratio' : sun_ratio, \
                                            'infill_ratio' : infill_ratio}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for dc in datecolumns:

      column_dict = { dc : {'category' : 'wkds', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : [dc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False, \
                           'downstream':[]}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list
  
  def process_mnts_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #processing funciton depending on input format of datetime data 
    #that creates a categorical column 
    #corresponding to months in source column
    #note this is a "singleprocess" function since is applied to single dataframe
    #default infill is 0
    '''
    
    #convert improperly formatted values to datetime in new column
    df[column+'_mnts'] = pd.to_datetime(df[column], errors = 'coerce')
    
    #This is kind of hack for whole hour increments, if we were needing
    #to evlauate hour ranges between seperate days a different metod
    #would be required
    #For now we'll defer to Dollly Parton
    df[column+'_mnts'] = pd.DatetimeIndex(df[column+'_mnts']).month
    
#     df[column+'_wkdy'] = df[column+'_wkdy'].between(0,4)

    #we'll use convention for default infill of eight days a week
    #jan-dec is 1-12, 0 is default infill
    df[column + '_mnts'] = df[column + '_mnts'].fillna(0)
    
    #reduce memory footprint
    df[column+'_mnts'] = df[column+'_mnts'].astype(np.int8)
    
    
    #create list of columns
    datecolumns = [column+'_mnts']

    #grab some driftreport metrics
    numberofrows = df[column + '_mnts'].shape[0]
    infill_ratio = df[df[column + '_mnts'] == 0].shape[0] / numberofrows
    jan_ratio = df[df[column + '_mnts'] == 1].shape[0] / numberofrows
    feb_ratio = df[df[column + '_mnts'] == 2].shape[0] / numberofrows
    mar_ratio = df[df[column + '_mnts'] == 3].shape[0] / numberofrows
    apr_ratio = df[df[column + '_mnts'] == 4].shape[0] / numberofrows
    may_ratio = df[df[column + '_mnts'] == 5].shape[0] / numberofrows
    jun_ratio = df[df[column + '_mnts'] == 6].shape[0] / numberofrows
    jul_ratio = df[df[column + '_mnts'] == 7].shape[0] / numberofrows
    aug_ratio = df[df[column + '_mnts'] == 8].shape[0] / numberofrows
    sep_ratio = df[df[column + '_mnts'] == 9].shape[0] / numberofrows
    oct_ratio = df[df[column + '_mnts'] == 10].shape[0] / numberofrows
    nov_ratio = df[df[column + '_mnts'] == 11].shape[0] / numberofrows
    dec_ratio = df[df[column + '_mnts'] == 12].shape[0] / numberofrows
  
    #create normalization dictionary
    normalization_dict = {column+'_mnts' : {'infill_ratio' : infill_ratio, \
                                            'jan_ratio' : jan_ratio, \
                                            'feb_ratio' : feb_ratio, \
                                            'mar_ratio' : mar_ratio, \
                                            'apr_ratio' : apr_ratio, \
                                            'may_ratio' : may_ratio, \
                                            'jun_ratio' : jun_ratio, \
                                            'jul_ratio' : jul_ratio, \
                                            'aug_ratio' : aug_ratio, \
                                            'sep_ratio' : sep_ratio, \
                                            'oct_ratio' : oct_ratio, \
                                            'nov_ratio' : nov_ratio, \
                                            'dec_ratio' : dec_ratio}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for dc in datecolumns:

      column_dict = { dc : {'category' : '_mnts', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : [dc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False, \
                           'downstream':[]}}

      column_dict_list.append(column_dict.copy())

    return df, column_dict_list
  
  def process_year_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_year_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for year
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_year'] = mdf_train[column].copy()
    mdf_test[column + '_year'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_year'] = pd.to_datetime(mdf_train[column + '_year'], errors = 'coerce')
    mdf_test[column + '_year'] = pd.to_datetime(mdf_test[column + '_year'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanyear = mdf_train[column + '_year'].dt.year.mean()
    
    if meanyear != meanyear:
      meanyear = 0

    #get standard deviation of training data
    stdyear = mdf_train[column + '_year'].dt.year.std()
    
    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdyear == 0:
      stdyear = 1
    if stdyear != stdyear:
      stdyear = 1

    #create new columns for each category in train set
    mdf_train[column + '_year'] = mdf_train[column + '_year'].dt.year
    mdf_test[column + '_year'] = mdf_test[column + '_year'].dt.year


    #replace missing data with training set mean
    mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(meanyear)
    mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)

    #subtract mean from column for both train and test
    mdf_train[column + '_year'] = mdf_train[column + '_year'] - meanyear
    mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear


    #divide column values by std for both training and test data
    mdf_train[column + '_year'] = mdf_train[column + '_year'] / stdyear

    mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear


#     #now replace NaN with 0
#     mdf_train[column + '_year'] = mdf_train[column + '_year'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)

    #output of a list of the created column names
    datecolumns = [column + '_year']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_year'] = mdf_train[column + '_year'].astype(np.float32)
#     mdf_test[column + '_year'] = mdf_test[column + '_year'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanyear' : meanyear,\
             'stdyear' : stdyear}}

      column_dict = {dc : {'category' : 'year', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  def process_mnth_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mnth_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for months
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mnth'] = mdf_train[column].copy()
    mdf_test[column + '_mnth'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mnth'] = pd.to_datetime(mdf_train[column + '_mnth'], errors = 'coerce')
    mdf_test[column + '_mnth'] = pd.to_datetime(mdf_test[column + '_mnth'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanmonth = mdf_train[column + '_mnth'].dt.month.mean()
    
    if meanmonth != meanmonth:
      meanmonth = 0

    #get standard deviation of training data
    stdmonth = mdf_train[column + '_mnth'].dt.month.std()

    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdmonth == 0:
      stdmonth = 1
    if stdmonth != stdmonth:
      stdmonth = 1

    #create new columns for each category in train set
    mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'].dt.month

    #do same for test set
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].dt.month


    #replace missing data with training set mean
    mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'].fillna(meanmonth)

    #do same for test set
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].fillna(meanmonth)

    #subtract mean from column for both train and test
    mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'] - meanmonth

    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'] - meanmonth


    #divide column values by std for both training and test data
    mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'] / stdmonth

    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'] / stdmonth


#     #now replace NaN with 0
#     mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].fillna(0)

#     #change data type for memory savings
#     mdf_train[column + '_mnth'] = mdf_train[column + '_mnth'].astype(np.float32)
#     mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].astype(np.float32)

    #output of a list of the created column names
    datecolumns = [column + '_mnth']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']


    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanmonth' : meanmonth,\
             'stdmonth' : stdmonth}}

      column_dict = {dc : {'category' : 'mnth', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mnsn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mnsn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for months
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mnsn'] = mdf_train[column].copy()
    mdf_test[column + '_mnsn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mnsn'] = pd.to_datetime(mdf_train[column + '_mnsn'], errors = 'coerce')
    mdf_test[column + '_mnsn'] = pd.to_datetime(mdf_test[column + '_mnsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries
    mdf_train[column + '_mnsn'] = mdf_train[column + '_mnsn'].dt.month
    mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].dt.month
    
    #apply sin transform
    mdf_train[column + '_mnsn'] = np.sin(mdf_train[column + '_mnsn'] * 2 * np.pi / 12 )
    mdf_test[column + '_mnsn'] = np.sin(mdf_test[column + '_mnsn'] * 2 * np.pi / 12 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mnsn = mdf_train[column + '_mnsn'].mean()
    
    if mean_mnsn != mean_mnsn:
      mean_mnsn = 0

    #replace missing data with training set mean
    mdf_train[column + '_mnsn'] = mdf_train[column + '_mnsn'].fillna(mean_mnsn)
    mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].fillna(mean_mnsn)

    #output of a list of the created column names
    datecolumns = [column + '_mnsn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mnsn'] = mdf_train[column + '_mnsn'].astype(np.float32)
#     mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mnsn' : mean_mnsn}}

      column_dict = {dc : {'category' : 'mnsn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mncs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mncs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for months
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mncs'] = mdf_train[column].copy()
    mdf_test[column + '_mncs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mncs'] = pd.to_datetime(mdf_train[column + '_mncs'], errors = 'coerce')
    mdf_test[column + '_mncs'] = pd.to_datetime(mdf_test[column + '_mncs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries
    mdf_train[column + '_mncs'] = mdf_train[column + '_mncs'].dt.month
    mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].dt.month
    
    #apply cos transform
    mdf_train[column + '_mncs'] = np.cos(mdf_train[column + '_mncs'] * 2 * np.pi / 12 )
    mdf_test[column + '_mncs'] = np.cos(mdf_test[column + '_mncs'] * 2 * np.pi / 12 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mncs = mdf_train[column + '_mncs'].mean()

    if mean_mncs != mean_mncs:
      mean_mncs = 0

    #replace missing data with training set mean
    mdf_train[column + '_mncs'] = mdf_train[column + '_mncs'].fillna(mean_mncs)
    mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].fillna(mean_mncs)

    #output of a list of the created column names
    datecolumns = [column + '_mncs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mncs'] = mdf_train[column + '_mncs'].astype(np.float32)
#     mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mncs' : mean_mncs}}

      column_dict = {dc : {'category' : 'mncs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mdsn_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_mdsn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined columns for months and days
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mdsn'] = mdf_train[column].copy()
    mdf_test[column + '_mdsn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mdsn'] = pd.to_datetime(mdf_train[column + '_mdsn'], errors = 'coerce')
    mdf_test[column + '_mdsn'] = pd.to_datetime(mdf_test[column + '_mdsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month

    #convert months to number of days in a temp column to support periodicity trasnform

    mdf_train[column + '_mdsn' + '_temp'] = mdf_train[column + '_mdsn'].copy()
    mdf_train[column + '_mdsn' + '_temp_leap'] = mdf_train[column + '_mdsn'].copy()

    mdf_train[column + '_mdsn' + '_temp'] = mdf_train[column + '_mdsn' + '_temp'].dt.month
    mdf_train[column + '_mdsn' + '_temp_leap'] = mdf_train[column + '_mdsn' + '_temp_leap'].dt.is_leap_year

    mdf_train[column + '_mdsn' + '_temp_leap'] = \
    np.where(mdf_train[column + '_mdsn' + '_temp_leap'], 29, 28)
    
    mdf_train[column + '_mdsn' + '_temp'] = \
    np.where(mdf_train[column + '_mdsn' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_train[column + '_mdsn' + '_temp'].values)

    mdf_train[column + '_mdsn' + '_temp'] = \
    np.where(mdf_train[column + '_mdsn' + '_temp'].isin([4,6,9,11]), 30, mdf_train[column + '_mdsn' + '_temp'].values)

    mdf_train[column + '_mdsn' + '_temp'] = \
    np.where(mdf_train[column + '_mdsn' + '_temp'].isin([2]), mdf_train[column + '_mdsn' + '_temp_leap'], \
    mdf_train[column + '_mdsn' + '_temp'].values)

    mdf_train[column + '_mdsn' + '_temp'] = \
    np.where(mdf_train[column + '_mdsn' + '_temp'].isin([28,29,30,31]), mdf_train[column + '_mdsn' + '_temp'].values, 30.42)

    #do same for test set

    mdf_test[column + '_mdsn' + '_temp'] = mdf_test[column + '_mdsn'].copy()
    mdf_test[column + '_mdsn' + '_temp_leap'] = mdf_test[column + '_mdsn'].copy()

    mdf_test[column + '_mdsn' + '_temp'] = mdf_test[column + '_mdsn' + '_temp'].dt.month
    mdf_test[column + '_mdsn' + '_temp_leap'] = mdf_test[column + '_mdsn' + '_temp_leap'].dt.is_leap_year

    mdf_test[column + '_mdsn' + '_temp_leap'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp_leap'], 29, 28)
    
    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([4,6,9,11]), 30, mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([2]), mdf_test[column + '_mdsn' + '_temp_leap'], \
    mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([28,29,30,31]), mdf_test[column + '_mdsn' + '_temp'].values, 30.42)



    #apply sin transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    mdf_train[column + '_mdsn'] = np.sin((mdf_train[column + '_mdsn'].dt.month + mdf_train[column + '_mdsn'].dt.day / \
    mdf_train[column + '_mdsn' + '_temp']) * 2 * np.pi / 12 )
    mdf_test[column + '_mdsn'] = np.sin((mdf_test[column + '_mdsn'].dt.month + mdf_test[column + '_mdsn'].dt.day / \
    mdf_test[column + '_mdsn' + '_temp']) * 2 * np.pi / 12 )

    #delete the support columns 
    del mdf_train[column + '_mdsn' + '_temp']
    del mdf_test[column + '_mdsn' + '_temp']

    del mdf_train[column + '_mdsn' + '_temp_leap']
    del mdf_test[column + '_mdsn' + '_temp_leap']
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mdsn = mdf_train[column + '_mdsn'].mean()

    if mean_mdsn != mean_mdsn:
      mean_mdsn = 0

    #replace missing data with training set mean
    mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].fillna(mean_mdsn)
    mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].fillna(mean_mdsn)

    #output of a list of the created column names
    datecolumns = [column + '_mdsn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].astype(np.float32)
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mdsn' : mean_mdsn}}

      column_dict = {dc : {'category' : 'mdsn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mdcs_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #process_mdcs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined columns for months and days
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mdcs'] = mdf_train[column].copy()
    mdf_test[column + '_mdcs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mdcs'] = pd.to_datetime(mdf_train[column + '_mdcs'], errors = 'coerce')
    mdf_test[column + '_mdcs'] = pd.to_datetime(mdf_test[column + '_mdcs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month

    #convert months to number of days in a temp column to support periodicity trasnform

    mdf_train[column + '_mdcs' + '_temp'] = mdf_train[column + '_mdcs'].copy()
    mdf_train[column + '_mdcs' + '_temp_leap'] = mdf_train[column + '_mdcs'].copy()

    mdf_train[column + '_mdcs' + '_temp'] = mdf_train[column + '_mdcs' + '_temp'].dt.month
    mdf_train[column + '_mdcs' + '_temp_leap'] = mdf_train[column + '_mdcs' + '_temp_leap'].dt.is_leap_year

    mdf_train[column + '_mdcs' + '_temp_leap'] = \
    np.where(mdf_train[column + '_mdcs' + '_temp_leap'], 29, 28)
    
    mdf_train[column + '_mdcs' + '_temp'] = \
    np.where(mdf_train[column + '_mdcs' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_train[column + '_mdcs' + '_temp'].values)

    mdf_train[column + '_mdcs' + '_temp'] = \
    np.where(mdf_train[column + '_mdcs' + '_temp'].isin([4,6,9,11]), 30, mdf_train[column + '_mdcs' + '_temp'].values)

    mdf_train[column + '_mdcs' + '_temp'] = \
    np.where(mdf_train[column + '_mdcs' + '_temp'].isin([2]), mdf_train[column + '_mdcs' + '_temp_leap'], \
    mdf_train[column + '_mdcs' + '_temp'].values)

    mdf_train[column + '_mdcs' + '_temp'] = \
    np.where(mdf_train[column + '_mdcs' + '_temp'].isin([28,29,30,31]), mdf_train[column + '_mdcs' + '_temp'].values, 30.42)

    #do same for test set

    mdf_test[column + '_mdcs' + '_temp'] = mdf_test[column + '_mdcs'].copy()
    mdf_test[column + '_mdcs' + '_temp_leap'] = mdf_test[column + '_mdcs'].copy()

    mdf_test[column + '_mdcs' + '_temp'] = mdf_test[column + '_mdcs' + '_temp'].dt.month
    mdf_test[column + '_mdcs' + '_temp_leap'] = mdf_test[column + '_mdcs' + '_temp_leap'].dt.is_leap_year

    mdf_test[column + '_mdcs' + '_temp_leap'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp_leap'], 29, 28)
    
    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([4,6,9,11]), 30, mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([2]), mdf_test[column + '_mdcs' + '_temp_leap'], \
    mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([28,29,30,31]), mdf_test[column + '_mdcs' + '_temp'].values, 30.42)
    
    #apply cos transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    mdf_train[column + '_mdcs'] = np.cos((mdf_train[column + '_mdcs'].dt.month + mdf_train[column + '_mdcs'].dt.day / \
    mdf_train[column + '_mdcs' + '_temp']) * 2 * np.pi / 12 )
    mdf_test[column + '_mdcs'] = np.cos((mdf_test[column + '_mdcs'].dt.month + mdf_test[column + '_mdcs'].dt.day / \
    mdf_test[column + '_mdcs' + '_temp']) * 2 * np.pi / 12 )

    #delete the support columns 
    del mdf_train[column + '_mdcs' + '_temp']
    del mdf_test[column + '_mdcs' + '_temp']

    del mdf_train[column + '_mdcs' + '_temp_leap']
    del mdf_test[column + '_mdcs' + '_temp_leap']
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mdcs = mdf_train[column + '_mdcs'].mean()

    if mean_mdcs != mean_mdcs:
      mean_mdcs = 0

    #replace missing data with training set mean
    mdf_train[column + '_mdcs'] = mdf_train[column + '_mdcs'].fillna(mean_mdcs)
    mdf_test[column + '_mdcs'] = mdf_test[column + '_mdcs'].fillna(mean_mdcs)

    #output of a list of the created column names
    datecolumns = [column + '_mdcs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mdcs'] = mdf_train[column + '_mdcs'].astype(np.float32)
#     mdf_test[column + '_mdcs'] = mdf_test[column + '_mdcs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mdcs' : mean_mdcs}}

      column_dict = {dc : {'category' : 'mdcs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_days_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_days_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for days
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_days'] = mdf_train[column].copy()
    mdf_test[column + '_days'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_days'] = pd.to_datetime(mdf_train[column + '_days'], errors = 'coerce')
    mdf_test[column + '_days'] = pd.to_datetime(mdf_test[column + '_days'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanday = mdf_train[column + '_days'].dt.day.mean()
    
    if meanday != meanday:
      meanday = 0

    #get standard deviation of training data
    stdday = mdf_train[column + '_days'].dt.day.std()

    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdday == 0:
      stdday = 1
    if stdday != stdday:
      stdday = 1

    #create new columns for each category in train set
    mdf_train[column + '_days'] = mdf_train[column + '_days'].dt.day
    mdf_test[column + '_days'] = mdf_test[column + '_days'].dt.day


    #replace missing data with training set mean
    mdf_train[column + '_days'] = mdf_train[column + '_days'].fillna(meanday)
    mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(meanday)

    #subtract mean from column for both train and test
    mdf_train[column + '_days'] = mdf_train[column + '_days'] - meanday
    mdf_test[column + '_days'] = mdf_test[column + '_days'] - meanday


    #divide column values by std for both training and test data
    mdf_train[column + '_days'] = mdf_train[column + '_days'] / stdday
    mdf_test[column + '_days'] = mdf_test[column + '_days'] / stdday


#     #now replace NaN with 0
#     mdf_train[column + '_days'] = mdf_train[column + '_days'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(0)

#     #change data type for memory savings
#     mdf_train[column + '_days'] = mdf_train[column + '_days'].astype(np.float32)
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].astype(np.float32)

    #output of a list of the created column names
    datecolumns = [column + '_days']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']


    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanday' : meanday,\
             'stdday' : stdday}}

      column_dict = {dc : {'category' : 'days', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_dysn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_dysn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for days
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_dysn'] = mdf_train[column].copy()
    mdf_test[column + '_dysn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_dysn'] = pd.to_datetime(mdf_train[column + '_dysn'], errors = 'coerce')
    mdf_test[column + '_dysn'] = pd.to_datetime(mdf_test[column + '_dysn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries
    mdf_train[column + '_dysn'] = mdf_train[column + '_dysn'].dt.day
    mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].dt.day

    #apply sin transform
    #average number of days in a month is 30.42
    #days in a week is 7
    mdf_train[column + '_dysn'] = np.sin(mdf_train[column + '_dysn'] * 2 * np.pi / 7 )
    mdf_test[column + '_dysn'] = np.sin(mdf_test[column + '_dysn'] * 2 * np.pi / 7 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_dysn = mdf_train[column + '_dysn'].mean()

    if mean_dysn != mean_dysn:
      mean_dysn = 0

    #replace missing data with training set mean
    mdf_train[column + '_dysn'] = mdf_train[column + '_dysn'].fillna(mean_dysn)
    mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].fillna(mean_dysn)

    #output of a list of the created column names
    datecolumns = [column + '_dysn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_dysn'] = mdf_train[column + '_dysn'].astype(np.float32)
#     mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_dysn' : mean_dysn}}

      column_dict = {dc : {'category' : 'dysn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_dycs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_dycs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for days
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_dycs'] = mdf_train[column].copy()
    mdf_test[column + '_dycs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_dycs'] = pd.to_datetime(mdf_train[column + '_dycs'], errors = 'coerce')
    mdf_test[column + '_dycs'] = pd.to_datetime(mdf_test[column + '_dycs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries
    mdf_train[column + '_dycs'] = mdf_train[column + '_dycs'].dt.day
    mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].dt.day

    #apply sin transform
    #average number of days in a month is 30.42
    #days in. a week is 7
    mdf_train[column + '_dycs'] = np.cos(mdf_train[column + '_dycs'] * 2 * np.pi / 7 )
    mdf_test[column + '_dycs'] = np.cos(mdf_test[column + '_dycs'] * 2 * np.pi / 7 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_dycs = mdf_train[column + '_dycs'].mean()

    if mean_dycs != mean_dycs:
      mean_dycs = 0

    #replace missing data with training set mean
    mdf_train[column + '_dycs'] = mdf_train[column + '_dycs'].fillna(mean_dycs)
    mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].fillna(mean_dycs)

    #output of a list of the created column names
    datecolumns = [column + '_dycs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_dycs'] = mdf_train[column + '_dycs'].astype(np.float32)
#     mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_dycs' : mean_dycs}}

      column_dict = {dc : {'category' : 'dycs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  def process_dhms_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mdsn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for days, hours and minutes
    #with sin transform for 1 day period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_dhms'] = mdf_train[column].copy()
    mdf_test[column + '_dhms'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_dhms'] = pd.to_datetime(mdf_train[column + '_dhms'], errors = 'coerce')
    mdf_test[column + '_dhms'] = pd.to_datetime(mdf_test[column + '_dhms'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    #7 days in. a week
    mdf_train[column + '_dhms'] = np.sin((mdf_train[column + '_dhms'].dt.day + mdf_train[column + '_dhms'].dt.hour / 24 + mdf_train[column + '_dhms'].dt.minute / 24 / 60) * 2 * np.pi / 7 )
    mdf_test[column + '_dhms'] = np.sin((mdf_test[column + '_dhms'].dt.day + mdf_test[column + '_dhms'].dt.hour / 24 + mdf_test[column + '_dhms'].dt.minute / 24 / 60) * 2 * np.pi / 7 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_dhms = mdf_train[column + '_dhms'].mean()

    if mean_dhms != mean_dhms:
      mean_dhms = 0

    #replace missing data with training set mean
    mdf_train[column + '_dhms'] = mdf_train[column + '_dhms'].fillna(mean_dhms)
    mdf_test[column + '_dhms'] = mdf_test[column + '_dhms'].fillna(mean_dhms)

    #output of a list of the created column names
    datecolumns = [column + '_dhms']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_dhms'] = mdf_train[column + '_dhms'].astype(np.float32)
#     mdf_test[column + '_dhms'] = mdf_test[column + '_dhms'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_dhms' : mean_dhms}}

      column_dict = {dc : {'category' : 'dhms', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_dhmc_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_dhmc_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for days, hours and minutes
    #with cos transform for 1 day period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_dhmc'] = mdf_train[column].copy()
    mdf_test[column + '_dhmc'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_dhmc'] = pd.to_datetime(mdf_train[column + '_dhmc'], errors = 'coerce')
    mdf_test[column + '_dhmc'] = pd.to_datetime(mdf_test[column + '_dhmc'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply cos transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    #7 days in a week
    mdf_train[column + '_dhmc'] = np.cos((mdf_train[column + '_dhmc'].dt.day + mdf_train[column + '_dhmc'].dt.hour / 24 + mdf_train[column + '_dhmc'].dt.minute / 24 / 60) * 2 * np.pi / 7 )
    mdf_test[column + '_dhmc'] = np.cos((mdf_test[column + '_dhmc'].dt.day + mdf_test[column + '_dhmc'].dt.hour / 24 + mdf_test[column + '_dhmc'].dt.minute / 24 / 60) * 2 * np.pi / 7 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_dhmc = mdf_train[column + '_dhmc'].mean()

    if mean_dhmc != mean_dhmc:
      mean_dhmc = 0

    #replace missing data with training set mean
    mdf_train[column + '_dhmc'] = mdf_train[column + '_dhmc'].fillna(mean_dhmc)
    mdf_test[column + '_dhmc'] = mdf_test[column + '_dhmc'].fillna(mean_dhmc)

    #output of a list of the created column names
    datecolumns = [column + '_dhmc']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_dhmc'] = mdf_train[column + '_dhmc'].astype(np.float32)
#     mdf_test[column + '_dhmc'] = mdf_test[column + '_dhmc'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_dhmc' : mean_dhmc}}

      column_dict = {dc : {'category' : 'dhmc', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_hour_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_hour_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for hours
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_hour'] = mdf_train[column].copy()
    mdf_test[column + '_hour'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_hour'] = pd.to_datetime(mdf_train[column + '_hour'], errors = 'coerce')
    mdf_test[column + '_hour'] = pd.to_datetime(mdf_test[column + '_hour'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanhour = mdf_train[column + '_hour'].dt.hour.mean()

    if meanhour != meanhour:
      meanhour = 0
    
    #get standard deviation of training data
    stdhour = mdf_train[column + '_hour'].dt.hour.std()

    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdhour == 0:
      stdhour = 1
    if stdhour != stdhour:
      stdhour = 1

    #create new columns for each category in train set
    mdf_train[column + '_hour'] = mdf_train[column + '_hour'].dt.hour
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'].dt.hour


    #replace missing data with training set mean
    mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(meanhour)
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)

    #subtract mean from column for both train and test
    mdf_train[column + '_hour'] = mdf_train[column + '_hour'] - meanhour

    mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour


    #divide column values by std for both training and test data
    mdf_train[column + '_hour'] = mdf_train[column + '_hour'] / stdhour

    mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour


#     #now replace NaN with 0
#     mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)

    #output of a list of the created column names
    datecolumns = [column + '_hour']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_hour'] = mdf_train[column + '_hour'].astype(np.float32)
#     mdf_test[column + '_hour'] = mdf_test[column + '_hour'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanhour' : meanhour,\
             'stdhour' : stdhour}}

      column_dict = {dc : {'category' : 'hour', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_hrsn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_hrsn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for hours
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_hrsn'] = mdf_train[column].copy()
    mdf_test[column + '_hrsn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_hrsn'] = pd.to_datetime(mdf_train[column + '_hrsn'], errors = 'coerce')
    mdf_test[column + '_hrsn'] = pd.to_datetime(mdf_test[column + '_hrsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab hour entries
    mdf_train[column + '_hrsn'] = mdf_train[column + '_hrsn'].dt.hour
    mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].dt.hour

    #apply sin transform
    #average number of hours in a day is ~24
    mdf_train[column + '_hrsn'] = np.sin(mdf_train[column + '_hrsn'] * 2 * np.pi / 24 )
    mdf_test[column + '_hrsn'] = np.sin(mdf_test[column + '_hrsn'] * 2 * np.pi / 24 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_hrsn = mdf_train[column + '_hrsn'].mean()

    if mean_hrsn != mean_hrsn:
      mean_hrsn = 0

    #replace missing data with training set mean
    mdf_train[column + '_hrsn'] = mdf_train[column + '_hrsn'].fillna(mean_hrsn)
    mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].fillna(mean_hrsn)

    #output of a list of the created column names
    datecolumns = [column + '_hrsn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_hrsn'] = mdf_train[column + '_hrsn'].astype(np.float32)
#     mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_hrsn' : mean_hrsn}}

      column_dict = {dc : {'category' : 'hrsn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_hrcs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_hrcs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for hours
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_hrcs'] = mdf_train[column].copy()
    mdf_test[column + '_hrcs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_hrcs'] = pd.to_datetime(mdf_train[column + '_hrcs'], errors = 'coerce')
    mdf_test[column + '_hrcs'] = pd.to_datetime(mdf_test[column + '_hrcs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries
    mdf_train[column + '_hrcs'] = mdf_train[column + '_hrcs'].dt.hour
    mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].dt.hour

    #apply cos transform
    #average number of hours in a day is ~24
    mdf_train[column + '_hrcs'] = np.cos(mdf_train[column + '_hrcs'] * 2 * np.pi / 24 )
    mdf_test[column + '_hrcs'] = np.cos(mdf_test[column + '_hrcs'] * 2 * np.pi / 24 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_hrcs = mdf_train[column + '_hrcs'].mean()

    if mean_hrcs != mean_hrcs:
      mean_hrcs = 0

    #replace missing data with training set mean
    mdf_train[column + '_hrcs'] = mdf_train[column + '_hrcs'].fillna(mean_hrcs)
    mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].fillna(mean_hrcs)

    #output of a list of the created column names
    datecolumns = [column + '_hrcs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_hrcs'] = mdf_train[column + '_hrcs'].astype(np.float32)
#     mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_hrcs' : mean_hrcs}}

      column_dict = {dc : {'category' : 'hrcs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_hmss_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_hmss_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for hours, minutes, and seconds
    #with sin transform for 1hr period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_hmss'] = mdf_train[column].copy()
    mdf_test[column + '_hmss'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_hmss'] = pd.to_datetime(mdf_train[column + '_hmss'], errors = 'coerce')
    mdf_test[column + '_hmss'] = pd.to_datetime(mdf_test[column + '_hmss'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    #24 hours in. a day
    mdf_train[column + '_hmss'] = np.sin((mdf_train[column + '_hmss'].dt.hour + mdf_train[column + '_hmss'].dt.minute / 60 + mdf_train[column + '_hmss'].dt.second / 60 / 60) * 2 * np.pi / 24 )
    mdf_test[column + '_hmss'] = np.sin((mdf_test[column + '_hmss'].dt.hour + mdf_test[column + '_hmss'].dt.minute / 60 + mdf_test[column + '_hmss'].dt.second / 60 / 60) * 2 * np.pi / 24 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_hmss = mdf_train[column + '_hmss'].mean()

    if mean_hmss != mean_hmss:
      mean_hmss = 0

    #replace missing data with training set mean
    mdf_train[column + '_hmss'] = mdf_train[column + '_hmss'].fillna(mean_hmss)
    mdf_test[column + '_hmss'] = mdf_test[column + '_hmss'].fillna(mean_hmss)

    #output of a list of the created column names
    datecolumns = [column + '_hmss']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_hmss'] = mdf_train[column + '_hmss'].astype(np.float32)
#     mdf_test[column + '_hmss'] = mdf_test[column + '_hmss'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_hmss' : mean_hmss}}

      column_dict = {dc : {'category' : 'hmss', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_hmsc_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_hmsc_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for hours, minutes, and seconds
    #with cos transform for 1hr period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_hmsc'] = mdf_train[column].copy()
    mdf_test[column + '_hmsc'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_hmsc'] = pd.to_datetime(mdf_train[column + '_hmsc'], errors = 'coerce')
    mdf_test[column + '_hmsc'] = pd.to_datetime(mdf_test[column + '_hmsc'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply cos transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    #24 hours in a day
    mdf_train[column + '_hmsc'] = np.cos((mdf_train[column + '_hmsc'].dt.hour + mdf_train[column + '_hmsc'].dt.minute / 60 + mdf_train[column + '_hmsc'].dt.second / 60 / 60) * 2 * np.pi / 24 )
    mdf_test[column + '_hmsc'] = np.cos((mdf_test[column + '_hmsc'].dt.hour + mdf_test[column + '_hmsc'].dt.minute / 60 + mdf_test[column + '_hmsc'].dt.second / 60 / 60) * 2 * np.pi / 24 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_hmsc = mdf_train[column + '_hmsc'].mean()

    if mean_hmsc != mean_hmsc:
      mean_hmsc = 0

    #replace missing data with training set mean
    mdf_train[column + '_hmsc'] = mdf_train[column + '_hmsc'].fillna(mean_hmsc)
    mdf_test[column + '_hmsc'] = mdf_test[column + '_hmsc'].fillna(mean_hmsc)

    #output of a list of the created column names
    datecolumns = [column + '_hmsc']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_hmsc'] = mdf_train[column + '_hmsc'].astype(np.float32)
#     mdf_test[column + '_hmsc'] = mdf_test[column + '_hmsc'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_hmsc' : mean_hmsc}}

      column_dict = {dc : {'category' : 'hmsc', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mint_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mint_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for minutes
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mint'] = mdf_train[column].copy()
    mdf_test[column + '_mint'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mint'] = pd.to_datetime(mdf_train[column + '_mint'], errors = 'coerce')
    mdf_test[column + '_mint'] = pd.to_datetime(mdf_test[column + '_mint'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanmint = mdf_train[column + '_mint'].dt.minute.mean()

    if meanmint != meanmint:
      meanmint = 0
      
    #get standard deviation of training data
    stdmint = mdf_train[column + '_mint'].dt.minute.std()

    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdmint == 0:
      stdmint = 1
    if stdmint != stdmint:
      stdmint = 1

    #create new columns for each category in train set
    mdf_train[column + '_mint'] = mdf_train[column + '_mint'].dt.minute
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'].dt.minute


    #replace missing data with training set mean
    mdf_train[column + '_mint'] = mdf_train[column + '_mint'].fillna(meanmint)
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'].fillna(meanmint)

    #subtract mean from column for both train and test
    mdf_train[column + '_mint'] = mdf_train[column + '_mint'] - meanmint

    mdf_test[column + '_mint'] = mdf_test[column + '_mint'] - meanmint


    #divide column values by std for both training and test data
    mdf_train[column + '_mint'] = mdf_train[column + '_mint'] / stdmint

    mdf_test[column + '_mint'] = mdf_test[column + '_mint'] / stdmint


#     #now replace NaN with 0
#     mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)

    #output of a list of the created column names
    datecolumns = [column + '_mint']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mint'] = mdf_train[column + '_mint'].astype(np.float32)
#     mdf_test[column + '_mint'] = mdf_test[column + '_mint'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanmint' : meanmint,\
             'stdmint' : stdmint}}

      column_dict = {dc : {'category' : 'mint', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_misn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_misn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for minutes
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_misn'] = mdf_train[column].copy()
    mdf_test[column + '_misn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_misn'] = pd.to_datetime(mdf_train[column + '_misn'], errors = 'coerce')
    mdf_test[column + '_misn'] = pd.to_datetime(mdf_test[column + '_misn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab hour entries
    mdf_train[column + '_misn'] = mdf_train[column + '_misn'].dt.minute
    mdf_test[column + '_misn'] = mdf_test[column + '_misn'].dt.minute

    #apply sin transform
    #60 minutes in an hour, 24 hours in a. day
    mdf_train[column + '_misn'] = np.sin(mdf_train[column + '_misn'] * 2 * np.pi / 60 / 24 )
    mdf_test[column + '_misn'] = np.sin(mdf_test[column + '_misn'] * 2 * np.pi / 60 / 24 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_misn = mdf_train[column + '_misn'].mean()

    if mean_misn != mean_misn:
      mean_misn = 0

    #replace missing data with training set mean
    mdf_train[column + '_misn'] = mdf_train[column + '_misn'].fillna(mean_misn)
    mdf_test[column + '_misn'] = mdf_test[column + '_misn'].fillna(mean_misn)

    #output of a list of the created column names
    datecolumns = [column + '_misn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_misn'] = mdf_train[column + '_misn'].astype(np.float32)
#     mdf_test[column + '_misn'] = mdf_test[column + '_misn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_misn' : mean_misn}}

      column_dict = {dc : {'category' : 'misn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mics_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mics_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for minutes
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_mics'] = mdf_train[column].copy()
    mdf_test[column + '_mics'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mics'] = pd.to_datetime(mdf_train[column + '_mics'], errors = 'coerce')
    mdf_test[column + '_mics'] = pd.to_datetime(mdf_test[column + '_mics'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab hour entries
    mdf_train[column + '_mics'] = mdf_train[column + '_mics'].dt.minute
    mdf_test[column + '_mics'] = mdf_test[column + '_mics'].dt.minute

    #apply cos transform
    #60 minutes in an hour, 24 hours in a day
    mdf_train[column + '_mics'] = np.cos(mdf_train[column + '_mics'] * 2 * np.pi / 60 / 24 )
    mdf_test[column + '_mics'] = np.cos(mdf_test[column + '_mics'] * 2 * np.pi / 60 / 24 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mics = mdf_train[column + '_mics'].mean()

    if mean_mics != mean_mics:
      mean_mics = 0

    #replace missing data with training set mean
    mdf_train[column + '_mics'] = mdf_train[column + '_mics'].fillna(mean_mics)
    mdf_test[column + '_mics'] = mdf_test[column + '_mics'].fillna(mean_mics)

    #output of a list of the created column names
    datecolumns = [column + '_mics']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mics'] = mdf_train[column + '_mics'].astype(np.float32)
#     mdf_test[column + '_mics'] = mdf_test[column + '_mics'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mics' : mean_mics}}

      column_dict = {dc : {'category' : 'mics', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mssn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mssn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for minutes, and seconds
    #with sin transform for 1 min period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mssn'] = mdf_train[column].copy()
    mdf_test[column + '_mssn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mssn'] = pd.to_datetime(mdf_train[column + '_mssn'], errors = 'coerce')
    mdf_test[column + '_mssn'] = pd.to_datetime(mdf_test[column + '_mssn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    mdf_train[column + '_mssn'] = np.sin((mdf_train[column + '_mssn'].dt.minute + mdf_train[column + '_mssn'].dt.second / 60) * 2 * np.pi / 60 / 24 )
    mdf_test[column + '_mssn'] = np.sin((mdf_test[column + '_mssn'].dt.minute + mdf_test[column + '_mssn'].dt.second / 60) * 2 * np.pi / 60 / 24 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mssn = mdf_train[column + '_mssn'].mean()

    if mean_mssn != mean_mssn:
      mean_mssn = 0

    #replace missing data with training set mean
    mdf_train[column + '_mssn'] = mdf_train[column + '_mssn'].fillna(mean_mssn)
    mdf_test[column + '_mssn'] = mdf_test[column + '_mssn'].fillna(mean_mssn)

    #output of a list of the created column names
    datecolumns = [column + '_mssn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mssn'] = mdf_train[column + '_mssn'].astype(np.float32)
#     mdf_test[column + '_mssn'] = mdf_test[column + '_mssn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mssn' : mean_mssn}}

      column_dict = {dc : {'category' : 'mssn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mscs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mscs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates combined column for minutes, and seconds
    #with cos transform for 1 min period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_mscs'] = mdf_train[column].copy()
    mdf_test[column + '_mscs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_mscs'] = pd.to_datetime(mdf_train[column + '_mscs'], errors = 'coerce')
    mdf_test[column + '_mscs'] = pd.to_datetime(mdf_test[column + '_mscs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries
#     mdf_train[column + '_mdsn'] = mdf_train[column + '_mdsn'].dt.month
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply cos transform to combined day and month, note aversage of 30.42 days in a month, 12 months in a year
    mdf_train[column + '_mscs'] = np.cos((mdf_train[column + '_mscs'].dt.minute + mdf_train[column + '_mscs'].dt.second / 60) * 2 * np.pi / 60 / 24 )
    mdf_test[column + '_mscs'] = np.cos((mdf_test[column + '_mscs'].dt.minute + mdf_test[column + '_mscs'].dt.second / 60) * 2 * np.pi / 60 / 24 )
    
    
    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_mscs = mdf_train[column + '_mscs'].mean()

    if mean_mscs != mean_mscs:
      mean_mscs = 0

    #replace missing data with training set mean
    mdf_train[column + '_mscs'] = mdf_train[column + '_mscs'].fillna(mean_mscs)
    mdf_test[column + '_mscs'] = mdf_test[column + '_mscs'].fillna(mean_mscs)

    #output of a list of the created column names
    datecolumns = [column + '_mscs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_mscs'] = mdf_train[column + '_mscs'].astype(np.float32)
#     mdf_test[column + '_mscs'] = mdf_test[column + '_mscs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_mscs' : mean_mscs}}

      column_dict = {dc : {'category' : 'mscs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_scnd_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_scnd_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 
    
    #creates distinct columns for seconds
    #z score normalized to the mean and std, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''
    
    #store original column for later retrieval
    mdf_train[column + '_scnd'] = mdf_train[column].copy()
    mdf_test[column + '_scnd'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_scnd'] = pd.to_datetime(mdf_train[column + '_scnd'], errors = 'coerce')
    mdf_test[column + '_scnd'] = pd.to_datetime(mdf_test[column + '_scnd'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #get mean of various categories of datetime objects to use to plug in missing cells
    meanscnd = mdf_train[column + '_scnd'].dt.second.mean()
    
    if meanscnd != meanscnd:
      meanscnd = 0

    #get standard deviation of training data
    stdscnd = mdf_train[column + '_scnd'].dt.second.std()

    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if stdscnd == 0:
      stdscnd = 1
    if stdscnd != stdscnd:
      stdscnd = 1

    #create new columns for each category in train set
    mdf_train[column + '_scnd'] = mdf_train[column + '_scnd'].dt.second
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].dt.second


    #replace missing data with training set mean
    mdf_train[column + '_scnd'] = mdf_train[column + '_scnd'].fillna(meanscnd)
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].fillna(meanscnd)

    #subtract mean from column for both train and test
    mdf_train[column + '_scnd'] = mdf_train[column + '_scnd'] - meanscnd

    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'] - meanscnd


    #divide column values by std for both training and test data
    mdf_train[column + '_scnd'] = mdf_train[column + '_scnd'] / stdscnd

    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'] / stdscnd


#     #now replace NaN with 0
#     mdf_train[column + '_hour'] = mdf_train[column + '_hour'].fillna(0)

#     #do same for test set
#     mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(0)

    #output of a list of the created column names
    datecolumns = [column + '_scnd']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_scnd'] = mdf_train[column + '_scnd'].astype(np.float32)
#     mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'meanscnd' : meanscnd,\
             'stdscnd' : stdscnd}}

      column_dict = {dc : {'category' : 'scnd', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
      
      
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_scsn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_scsn_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for seconds
    #with sin transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_scsn'] = mdf_train[column].copy()
    mdf_test[column + '_scsn'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_scsn'] = pd.to_datetime(mdf_train[column + '_scsn'], errors = 'coerce')
    mdf_test[column + '_scsn'] = pd.to_datetime(mdf_test[column + '_scsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab hour entries
    mdf_train[column + '_scsn'] = mdf_train[column + '_scsn'].dt.second
    mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].dt.second

    #apply sin transform
    #60 seconds in a minute
    mdf_train[column + '_scsn'] = np.sin(mdf_train[column + '_scsn'] * 2 * np.pi / 60 )
    mdf_test[column + '_scsn'] = np.sin(mdf_test[column + '_scsn'] * 2 * np.pi / 60 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_scsn = mdf_train[column + '_scsn'].mean()

    if mean_scsn != mean_scsn:
      mean_scsn = 0

    #replace missing data with training set mean
    mdf_train[column + '_scsn'] = mdf_train[column + '_scsn'].fillna(mean_scsn)
    mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].fillna(mean_scsn)

    #output of a list of the created column names
    datecolumns = [column + '_scsn']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_scsn'] = mdf_train[column + '_scsn'].astype(np.float32)
#     mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_scsn' : mean_scsn}}

      column_dict = {dc : {'category' : 'scsn', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_sccs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_sccs_class(mdf_train, mdf_test, column, category)
    #preprocess column with time classifications
    #takes as arguement two pandas dataframe containing training and test data respectively 
    #(mdf_train, mdf_test), and the name of the column string ('column') and the
    #category fo the source column (category)
    #note this trains both training and test data simultaneously due to unique treatment if any category
    #missing from training set but not from test set to ensure consistent formatting 

    #creates distinct columns for seconds
    #with cos transform for 12 month period, with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns two transformed dataframe (mdf_train, mdf_test) and column_dict_list
    '''

    #store original column for later retrieval
    mdf_train[column + '_sccs'] = mdf_train[column].copy()
    mdf_test[column + '_sccs'] = mdf_test[column].copy()


    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_train[column + '_sccs'] = pd.to_datetime(mdf_train[column + '_sccs'], errors = 'coerce')
    mdf_test[column + '_sccs'] = pd.to_datetime(mdf_test[column + '_sccs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab hour entries
    mdf_train[column + '_sccs'] = mdf_train[column + '_sccs'].dt.second
    mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].dt.second

    #apply cos transform
    #60 seconds in a minute
    mdf_train[column + '_sccs'] = np.cos(mdf_train[column + '_sccs'] * 2 * np.pi / 60 )
    mdf_test[column + '_sccs'] = np.cos(mdf_test[column + '_sccs'] * 2 * np.pi / 60 )


    #get mean of various categories of datetime objects to use to plug in missing cells
    mean_sccs = mdf_train[column + '_sccs'].mean()

    if mean_sccs != mean_sccs:
      mean_sccs = 0

    #replace missing data with training set mean
    mdf_train[column + '_sccs'] = mdf_train[column + '_sccs'].fillna(mean_sccs)
    mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].fillna(mean_sccs)

    #output of a list of the created column names
    datecolumns = [column + '_sccs']

#     #this is to address an issue I found when parsing columns with only time no date
#     #which returned -inf vlaues, so if an issue will just delete the associated 
#     #column along with the entry in datecolumns
#     checkyear = np.isinf(mdf_train.iloc[0][column + '_year'])
#     if checkyear:
#       del mdf_train[column + '_year']
#       datecolumns.remove(column + '_year')
#       if column + '_year' in mdf_test.columns:
#         del mdf_test[column + '_year']

#     #change data type for memory savings
#     mdf_train[column + '_sccs'] = mdf_train[column + '_sccs'].astype(np.float32)
#     mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].astype(np.float32)

    #store some values in the date_dict{} for use later in ML infill methods

    column_dict_list = []

    categorylist = datecolumns


    for dc in categorylist:

      #save a dictionary of the associated column mean and std
      timenormalization_dict = \
      {dc : {'mean_sccs' : mean_sccs}}

      column_dict = {dc : {'category' : 'sccs', \
                           'origcategory' : category, \
                           'normalization_dict' : timenormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : datecolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list


  
    
  def process_bxcx_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    Applies Box-Cox transform to an all-positive numerical set.
    '''
    
    #df_train, nmbrcolumns, nmbrnormalization_dict, categorylist = \
    mdf_train, column_dict_list = \
    self.process_bxcx_support(mdf_train, column, category, 1, bxcx_lmbda = None, \
                              trnsfrm_mean = None)

    #grab the normalization_dict associated with the bxcx category
    columnkeybxcx = column + '_bxcx'
    for column_dict in column_dict_list:
      if columnkeybxcx in column_dict:
        bxcxnormalization_dict = column_dict[columnkeybxcx]['normalization_dict'][columnkeybxcx]

    #df_test, nmbrcolumns, _1, _2 = \
    mdf_test, _1 = \
    self.process_bxcx_support(mdf_test, column, category, 1, bxcx_lmbda = \
                             bxcxnormalization_dict['bxcx_lmbda'], \
                             trnsfrm_mean = bxcxnormalization_dict['trnsfrm_mean'])

    return mdf_train, mdf_test, column_dict_list
  


  def process_bxcx_support(self, df, column, category, bxcxerrorcorrect, bxcx_lmbda = None, trnsfrm_mean = None):
    '''                      
    #process_bxcx_class(df, column, bxcx_lmbda = None, trnsfrm_mean = None, trnsfrm_std = None)
    #function that takes as input a dataframe with numnerical column for purposes
    #of applying a box-cox transformation. If lmbda = None it will infer a suitable
    #lambda value by minimizing log likelihood using SciPy's stats boxcox call. If
    #we pass a mean or std value it will apply the mean for the initial infill and 
    #use the values to apply postprocess_numerical_class function. 
    #Returns transformed dataframe, a list nmbrcolumns of the associated columns,
    #and a normalization dictionary nmbrnormalization_dict which we'll use for our
    #postprocess_dict, and the parameter lmbda that was used
    #expect this approach works better than our prior numerical address when the 
    #distribution is less thin tailed
    '''
    
    bxcxcolumn = column + '_bxcx'
    
    #store original column for later reversion
    df[bxcxcolumn] = df[column].copy()

    #convert all values to either numeric or NaN
    df[bxcxcolumn] = pd.to_numeric(df[bxcxcolumn], errors='coerce')
    #convert non-positive values to nan
    df.loc[df[bxcxcolumn] <= 0, (bxcxcolumn)] = np.nan

    #get the mean value to apply to infill
    if trnsfrm_mean == None:
      #get mean of training data
      mean = df[bxcxcolumn].mean()  

    else:
      mean = trnsfrm_mean

    #replace missing data with training set mean
    df[bxcxcolumn] = df[bxcxcolumn].fillna(mean)
    
    #edge case to avoid stats.boxcox error
    if df[bxcxcolumn].nunique() == 1:
      df[bxcxcolumn] = 0
      
      #we'll use convention that if training data is set to 0 then so will all subsequent data
      bxcx_lmbda = False
      
    else:

      #apply box-cox transformation to generate a new column
      #note the returns are different based on whether we passed a lmbda value

      if bxcx_lmbda == None:

        df[bxcxcolumn], bxcx_lmbda = stats.boxcox(df[bxcxcolumn])
        df[bxcxcolumn] *= bxcxerrorcorrect
        
      elif bxcx_lmbda is False:
        
        df[bxcxcolumn] = 0

      else:

        df[bxcxcolumn] = stats.boxcox(df[bxcxcolumn], lmbda = bxcx_lmbda)
        df[bxcxcolumn] *= bxcxerrorcorrect

    #this is to address an error when bxcx transofrm produces overflow
    #I'm not sure of cause, showed up in the housing set)
    bxcxerrorcorrect = 1
    if max(df[bxcxcolumn]) > (2 ** 31 - 1):
      bxcxerrorcorrect = 0
      df[bxcxcolumn] = 0
      bxcxcolumn = bxcxcolumn
      print("overflow condition found in boxcox transofrm, column set to 0: ", bxcxcolumn)



#     #replace original column
#     del df[column]

#     df[column] = df[column + '_temp'].copy()

#     del df[column + '_temp']

#     #change data type for memory savings
#     df[column + '_bxcx'] = df[column + '_bxcx'].astype(np.float32)

    #output of a list of the created column names
    #nmbrcolumns = [column + '_nmbr', column + '_bxcx', column + '_NArw']
    nmbrcolumns = [bxcxcolumn]

    #create list of columns associated with categorical transform (blank for now)
    categorylist = []


    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:


      #save a dictionary of the associated column mean and std

      normalization_dict = {nc : {'trnsfrm_mean' : mean, \
                                  'bxcx_lmbda' : bxcx_lmbda, \
                                  'bxcxerrorcorrect' : bxcxerrorcorrect, \
                                  'mean' : mean}}

      column_dict = { nc : {'category' : 'bxcx', \
                           'origcategory' : category, \
                           'normalization_dict' : normalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())




    #return df, nmbrcolumns, nmbrnormalization_dict, categorylist
    return df, column_dict_list


  def process_log0_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_log0_class(mdf_train, mdf_test, column, category)
    #function to apply logatrithmic transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a logarithmic transform (base 10)
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_log0'
    '''
    
    #copy source column into new column
    mdf_train[column + '_log0'] = mdf_train[column].copy()
    mdf_test[column + '_log0'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_log0'] = pd.to_numeric(mdf_train[column + '_log0'], errors='coerce')
    mdf_test[column + '_log0'] = pd.to_numeric(mdf_test[column + '_log0'], errors='coerce')
    
#     #replace all zeros with nan for the log operation
#     zeroreplace = {0 : np.nan}
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].replace(zeroreplace)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].replace(zeroreplace)
    
    #replace all non-positive with nan for the log operation
    mdf_train.loc[mdf_train[column + '_log0'] <= 0, (column + '_log0')] = np.nan
    mdf_test.loc[mdf_test[column + '_log0'] <= 0, (column + '_log0')] = np.nan
    
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with mean
    mdf_train[column + '_log0'] = np.log10(mdf_train[column + '_log0'])
    mdf_test[column + '_log0'] = np.log10(mdf_test[column + '_log0'])
    
    #get mean of train set
    meanlog = mdf_train[column + '_log0'].mean()
    
    if meanlog != meanlog:
      meanlog = 0

    #replace missing data with training set mean
    mdf_train[column + '_log0'] = mdf_train[column + '_log0'].fillna(meanlog)
    mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(meanlog)


#     #replace missing data with 0
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].fillna(0)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].astype(np.float32)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    #create list of columns
    nmbrcolumns = [column + '_log0']


    nmbrnormalization_dict = {column + '_log0' : {'meanlog' : meanlog}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'log0', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  def process_logn_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_logn_class(mdf_train, mdf_test, column, category)
    #function to apply natural logatrithmic transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a logarithmic transform (base e)
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_logn'
    '''
    
    #copy source column into new column
    mdf_train[column + '_logn'] = mdf_train[column].copy()
    mdf_test[column + '_logn'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_logn'] = pd.to_numeric(mdf_train[column + '_logn'], errors='coerce')
    mdf_test[column + '_logn'] = pd.to_numeric(mdf_test[column + '_logn'], errors='coerce')
    
#     #replace all zeros with nan for the log operation
#     zeroreplace = {0 : np.nan}
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].replace(zeroreplace)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].replace(zeroreplace)
    
    #replace all non-positive with nan for the log operation
    mdf_train.loc[mdf_train[column + '_logn'] <= 0, (column + '_logn')] = np.nan
    mdf_test.loc[mdf_test[column + '_logn'] <= 0, (column + '_logn')] = np.nan
    
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with mean
    mdf_train[column + '_logn'] = np.log(mdf_train[column + '_logn'])
    mdf_test[column + '_logn'] = np.log(mdf_test[column + '_logn'])
    
    #get mean of train set
    meanlog = mdf_train[column + '_logn'].mean()
    
    if meanlog != meanlog:
      meanlog = 0

    #replace missing data with training set mean
    mdf_train[column + '_logn'] = mdf_train[column + '_logn'].fillna(meanlog)
    mdf_test[column + '_logn'] = mdf_test[column + '_logn'].fillna(meanlog)


#     #replace missing data with 0
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].fillna(0)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].astype(np.float32)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    #create list of columns
    nmbrcolumns = [column + '_logn']


    nmbrnormalization_dict = {column + '_logn' : {'meanlog' : meanlog}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'logn', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  def process_sqrt_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_sqrt_class(mdf_train, mdf_test, column, category)
    #function to apply square root transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a square root transform
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_log0'
    '''
    
    #copy source column into new column
    mdf_train[column + '_sqrt'] = mdf_train[column].copy()
    mdf_test[column + '_sqrt'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_sqrt'] = pd.to_numeric(mdf_train[column + '_sqrt'], errors='coerce')
    mdf_test[column + '_sqrt'] = pd.to_numeric(mdf_test[column + '_sqrt'], errors='coerce')
    
#     #replace all zeros with nan for the log operation
#     zeroreplace = {0 : np.nan}
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].replace(zeroreplace)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].replace(zeroreplace)
    
    #replace all non-positive with nan for the log operation
    mdf_train.loc[mdf_train[column + '_sqrt'] < 0, (column + '_sqrt')] = np.nan
    mdf_test.loc[mdf_test[column + '_sqrt'] < 0, (column + '_sqrt')] = np.nan
    
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with mean
    mdf_train[column + '_sqrt'] = np.sqrt(mdf_train[column + '_sqrt'])
    mdf_test[column + '_sqrt'] = np.sqrt(mdf_test[column + '_sqrt'])
    
    #get mean of train set
    meansqrt = mdf_train[column + '_sqrt'].mean()
    
    if meansqrt != meansqrt:
      meansqrt = 0

    #replace missing data with training set mean
    mdf_train[column + '_sqrt'] = mdf_train[column + '_sqrt'].fillna(meansqrt)
    mdf_test[column + '_sqrt'] = mdf_test[column + '_sqrt'].fillna(meansqrt)


#     #replace missing data with 0
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].fillna(0)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].astype(np.float32)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    #create list of columns
    nmbrcolumns = [column + '_sqrt']


    nmbrnormalization_dict = {column + '_sqrt' : {'meansqrt' : meansqrt}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'sqrt', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  def process_addd_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_addd_class(.)
    #function to apply addition transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'add' for amount of addition, otherwise defaults to adding 1
    #applies an addition transform
    #replaces non-numeric entries with set mean after addition
    #returns same dataframes with new column of name column + '_addd'
    '''
    
    if 'add' in params:
        
      add = params['add']
    
    else:
      
      add = 1
    
    #copy source column into new column
    mdf_train[column + '_addd'] = mdf_train[column].copy()
    mdf_test[column + '_addd'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_addd'] = pd.to_numeric(mdf_train[column + '_addd'], errors='coerce')
    mdf_test[column + '_addd'] = pd.to_numeric(mdf_test[column + '_addd'], errors='coerce')
    
    
    #apply addition
    mdf_train[column + '_addd'] = mdf_train[column + '_addd'] + add
    mdf_test[column + '_addd'] = mdf_test[column + '_addd'] + add
    
    #get mean of train set
    mean = mdf_train[column + '_addd'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_addd'] = mdf_train[column + '_addd'].fillna(mean)
    mdf_test[column + '_addd'] = mdf_test[column + '_addd'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_addd']


    nmbrnormalization_dict = {column + '_addd' : {'mean' : mean, \
                                                  'add' : add}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'addd', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_sbtr_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_sbtr_class(.)
    #function to apply subtraction transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'subtract' for amount of subtraction, otherwise defaults to subtracting 1
    #applies a subtraction transform
    #replaces non-numeric entries with set mean after subtraction
    #returns same dataframes with new column of name column + '_sbtr'
    '''
    
    if 'subtract' in params:
        
      subtract = params['subtract']
    
    else:
      
      subtract = 1
    
    #copy source column into new column
    mdf_train[column + '_sbtr'] = mdf_train[column].copy()
    mdf_test[column + '_sbtr'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_sbtr'] = pd.to_numeric(mdf_train[column + '_sbtr'], errors='coerce')
    mdf_test[column + '_sbtr'] = pd.to_numeric(mdf_test[column + '_sbtr'], errors='coerce')
    
    
    #apply subtraction
    mdf_train[column + '_sbtr'] = mdf_train[column + '_sbtr'] - subtract
    mdf_test[column + '_sbtr'] = mdf_test[column + '_sbtr'] - subtract
    
    #get mean of train set
    mean = mdf_train[column + '_sbtr'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_sbtr'] = mdf_train[column + '_sbtr'].fillna(mean)
    mdf_test[column + '_sbtr'] = mdf_test[column + '_sbtr'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_sbtr']


    nmbrnormalization_dict = {column + '_sbtr' : {'mean' : mean, \
                                                  'subtract' : subtract}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'sbtr', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_mltp_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_mltp_class(.)
    #function to apply multiplication transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'multiply' for amount of addition, otherwise defaults to multiplying 2
    #applies an multiplication transform
    #replaces non-numeric entries with set mean after addition
    #returns same dataframes with new column of name column + '_mltp'
    '''
    
    if 'multiply' in params:
        
      multiply = params['multiply']
    
    else:
      
      multiply = 2
    
    #copy source column into new column
    mdf_train[column + '_mltp'] = mdf_train[column].copy()
    mdf_test[column + '_mltp'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_mltp'] = pd.to_numeric(mdf_train[column + '_mltp'], errors='coerce')
    mdf_test[column + '_mltp'] = pd.to_numeric(mdf_test[column + '_mltp'], errors='coerce')
    
    
    #apply multiplication
    mdf_train[column + '_mltp'] = mdf_train[column + '_mltp'] * multiply
    mdf_test[column + '_mltp'] = mdf_test[column + '_mltp'] * multiply
    
    #get mean of train set
    mean = mdf_train[column + '_mltp'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_mltp'] = mdf_train[column + '_mltp'].fillna(mean)
    mdf_test[column + '_mltp'] = mdf_test[column + '_mltp'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_mltp']


    nmbrnormalization_dict = {column + '_mltp' : {'mean' : mean, \
                                                  'multiply' : multiply}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'mltp', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_divd_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_divd_class(.)
    #function to apply division transform
    #accepts parameter 'divide' for amount of division, otherwise defaults to dividing by 2
    #applies an division transform
    #replaces non-numeric entries with set mean after division
    #returns same dataframes with new column of name column + '_divd'
    '''
    
    if 'divide' in params:
        
      divide = params['divide']
    
    else:
      
      divide = 2
      
    #special case override to avoid div by 0
    if divide == 0:
      divide = 1
    
    #copy source column into new column
    mdf_train[column + '_divd'] = mdf_train[column].copy()
    mdf_test[column + '_divd'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_divd'] = pd.to_numeric(mdf_train[column + '_divd'], errors='coerce')
    mdf_test[column + '_divd'] = pd.to_numeric(mdf_test[column + '_divd'], errors='coerce')
    
    
    #apply multiplication
    mdf_train[column + '_divd'] = mdf_train[column + '_divd'] / divide
    mdf_test[column + '_divd'] = mdf_test[column + '_divd'] / divide
    
    #get mean of train set
    mean = mdf_train[column + '_divd'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_divd'] = mdf_train[column + '_divd'].fillna(mean)
    mdf_test[column + '_divd'] = mdf_test[column + '_divd'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_divd']


    nmbrnormalization_dict = {column + '_divd' : {'mean' : mean, \
                                                  'divide' : divide}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'divd', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_rais_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_rais_class(.)
    #function to apply raise to a power transform
    #accepts parameter 'raiser' for amount of power, otherwise defaults to square (raise by 2)
    #applies an raise transform
    #replaces non-numeric entries with set mean after raise
    #returns same dataframes with new column of name column + '_rais'
    '''
    
    if 'raiser' in params:
        
      raiser = params['raiser']
    
    else:
      
      raiser = 2
    
    #copy source column into new column
    mdf_train[column + '_rais'] = mdf_train[column].copy()
    mdf_test[column + '_rais'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_rais'] = pd.to_numeric(mdf_train[column + '_rais'], errors='coerce')
    mdf_test[column + '_rais'] = pd.to_numeric(mdf_test[column + '_rais'], errors='coerce')
    
    
    #apply addition
    mdf_train[column + '_rais'] = mdf_train[column + '_rais'] ** raiser
    mdf_test[column + '_rais'] = mdf_test[column + '_rais'] ** raiser
    
    #get mean of train set
    mean = mdf_train[column + '_rais'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_rais'] = mdf_train[column + '_rais'].fillna(mean)
    mdf_test[column + '_rais'] = mdf_test[column + '_rais'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_rais']


    nmbrnormalization_dict = {column + '_rais' : {'mean' : mean, \
                                                  'raiser' : raiser}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'rais', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_absl_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #process_absl_class(.)
    #function to apply absolute transform
    #does not accept paraemters
    #applies an absolute transform
    #replaces non-numeric entries with set mean after transform
    #returns same dataframes with new column of name column + '_absl'
    '''
    
    #copy source column into new column
    mdf_train[column + '_absl'] = mdf_train[column].copy()
    mdf_test[column + '_absl'] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[column + '_absl'] = pd.to_numeric(mdf_train[column + '_absl'], errors='coerce')
    mdf_test[column + '_absl'] = pd.to_numeric(mdf_test[column + '_absl'], errors='coerce')
    
    
    #apply addition
    mdf_train[column + '_absl'] = mdf_train[column + '_absl'].abs()
    mdf_test[column + '_absl'] = mdf_test[column + '_absl'].abs()
    
    #get mean of train set
    mean = mdf_train[column + '_absl'].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[column + '_absl'] = mdf_train[column + '_absl'].fillna(mean)
    mdf_test[column + '_absl'] = mdf_test[column + '_absl'].fillna(mean)


    #create list of columns
    nmbrcolumns = [column + '_absl']


    nmbrnormalization_dict = {column + '_absl' : {'mean' : mean}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'absl', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : nmbrcolumns, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return mdf_train, mdf_test, column_dict_list

  
  def process_pwrs_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #we'll use an initial plug value of 0
    '''
    
    tempcolumn = column + '_:;:_temp'

    #store original column for later reversion
    mdf_train[tempcolumn] = mdf_train[column].copy()
    mdf_test[tempcolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[tempcolumn] = pd.to_numeric(mdf_train[tempcolumn], errors='coerce')
    mdf_test[tempcolumn] = pd.to_numeric(mdf_test[tempcolumn], errors='coerce')
    
    #convert all values <= 0 to Nan
    mdf_train[tempcolumn] = \
    np.where(mdf_train[tempcolumn] <= 0, np.nan, mdf_train[tempcolumn].values)
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] <= 0, np.nan, mdf_test[tempcolumn].values)
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with meanlog
#     mdf_train[column] = np.floor(np.log10(mdf_train[column]))
#     mdf_test[column] = np.floor(np.log10(mdf_test[column]))
    mdf_train[tempcolumn] = \
    np.where(mdf_train[tempcolumn] != np.nan, np.floor(np.log10(mdf_train[tempcolumn])), mdf_train[tempcolumn].values)
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] != np.nan, np.floor(np.log10(mdf_test[tempcolumn])), mdf_test[tempcolumn].values)


    
    #get mean of train set
    meanlog = np.floor(mdf_train[tempcolumn].mean())
    
    #get max of train set
    maxlog = max(mdf_train[tempcolumn])
    
#     #replace missing data with training set mean
#     mdf_train[column + '_log0'] = mdf_train[column + '_log0'].fillna(meanlog)
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(meanlog)

    #replace missing data with 0
    mdf_train[tempcolumn] = mdf_train[tempcolumn].fillna(0)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].fillna(0)
    
    
    #replace numerical with string equivalent
    mdf_train[tempcolumn] = mdf_train[tempcolumn].astype(int).astype(str)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype(int).astype(str)
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    labels_train = mdf_train[tempcolumn].unique()
    labels_train.sort(axis=0)
    labels_test = mdf_test[tempcolumn].unique()
    labels_test.sort(axis=0)
    
    #pandas one hot encoder
    df_train_cat = pd.get_dummies(mdf_train[tempcolumn])
    df_test_cat = pd.get_dummies(mdf_test[tempcolumn])
    
    #append column header name to each category listing
    labels_train[...] = column + '_10^' + labels_train[...]
    labels_test[...] = column + '_10^' + labels_test[...]
    
    #convert sparse array to pandas dataframe with column labels
    df_train_cat.columns = labels_train
    df_test_cat.columns = labels_test
    
    #Get missing columns in test set that are present in training set
    missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )
    
    #Add a missing column in test set with default value equal to 0
    for c in missing_cols:
        df_test_cat[c] = 0
    #Ensure the order of column in the test set is in the same order than in train set
    #Note this also removes categories in test set that aren't present in training set
    df_test_cat = df_test_cat[df_train_cat.columns]
    
    #concatinate the sparse set with the rest of our training data
    mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)
    mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)
    

    del mdf_train[tempcolumn]    
    del mdf_test[tempcolumn]
    
    #create output of a list of the created column names
#     NAcolumn = columnNAr2
    labels_train = list(df_train_cat)
#     if NAcolumn in labels_train:
#       labels_train.remove(NAcolumn)
    powercolumns = labels_train
  
    #change data type for memory savings
    for powercolumn in powercolumns:
      mdf_train[powercolumn] = mdf_train[powercolumn].astype(np.int8)
      mdf_test[powercolumn] = mdf_test[powercolumn].astype(np.int8)
    
    normalizationdictvalues = labels_train
    normalizationdictkeys = powercolumns
    
    normalizationdictkeys.sort()
    normalizationdictvalues.sort()
    
    powerlabelsdict = dict(zip(normalizationdictkeys, normalizationdictvalues))
    
    #change data types to 8-bit (1 byte) integers for memory savings
    for powercolumn in powercolumns:
      mdf_train[powercolumn] = mdf_train[powercolumn].astype(np.int8)
      mdf_test[powercolumn] = mdf_test[powercolumn].astype(np.int8)
        
    #store some values in the text_dict{} for use later in ML infill methods
    column_dict_list = []
    
    categorylist = powercolumns.copy()
    
    for pc in powercolumns:
      
      #new parameter collected for driftreport
      tc_ratio = pc + '_ratio'
      tcratio = mdf_train[pc].sum() / mdf_train[pc].shape[0]

      powernormalization_dict = {pc : {'powerlabelsdict' : powerlabelsdict, \
                                       'meanlog' : meanlog, \
                                       'maxlog' : maxlog, \
                                       tc_ratio : tcratio}}
    
      column_dict = {pc : {'category' : 'pwrs', \
                           'origcategory' : category, \
                           'normalization_dict' : powernormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : powercolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}
        
      column_dict_list.append(column_dict.copy())
    
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_pwor_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #we'll use an initial plug value of 0
    '''
    
    pworcolumn = column + '_pwor'

    #store original column for later reversion
    mdf_train[pworcolumn] = mdf_train[column].copy()
    mdf_test[pworcolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[pworcolumn] = pd.to_numeric(mdf_train[pworcolumn], errors='coerce')
    mdf_test[pworcolumn] = pd.to_numeric(mdf_test[pworcolumn], errors='coerce')
    
    #convert all values <= 0 to Nan
    mdf_train[pworcolumn] = \
    np.where(mdf_train[pworcolumn] <= 0, np.nan, mdf_train[pworcolumn].values)
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] <= 0, np.nan, mdf_test[pworcolumn].values)

    
    #get mean of train set
    meanlog = np.floor(mdf_train[pworcolumn].mean())
    
    #get max of train set
    maxlog = max(mdf_train[pworcolumn])


    mdf_train[pworcolumn] = \
    np.where(mdf_train[pworcolumn] != np.nan, np.floor(np.log10(mdf_train[pworcolumn])), mdf_train[pworcolumn].values)
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] != np.nan, np.floor(np.log10(mdf_test[pworcolumn])), mdf_test[pworcolumn].values)
    
    #replace missing data with 0
    mdf_train[pworcolumn] = mdf_train[pworcolumn].fillna(0)
    mdf_test[pworcolumn] = mdf_test[pworcolumn].fillna(0)
    
        
    #store some values in the text_dict{} for use later in ML infill methods
    column_dict_list = []
    
    powercolumns = [pworcolumn]
    
    categorylist = powercolumns.copy()
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[pworcolumn].unique():
      sumcalc = (mdf_train[pworcolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[pworcolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})
    
    for pc in powercolumns:

      powernormalization_dict = {pc : {'meanlog' : meanlog, \
                                       'maxlog' : maxlog, \
                                       'ordl_activations_dict' : ordl_activations_dict}}
    
      column_dict = {pc : {'category' : 'pwor', \
                           'origcategory' : category, \
                           'normalization_dict' : powernormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : powercolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}
        
      column_dict_list.append(column_dict.copy())
    
    return mdf_train, mdf_test, column_dict_list
  

  def process_pwr2_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #postiive values encoded under column 'column' + '_10^#' where # is power of 10
    #negative values encoded under column 'column' + '_-10^#' where # is power of 10
    
    #infill for non numeric or 0, infill has no activation
    
    #if all values are infill no columns returned
    '''
    
    tempcolumn = column + '_:;:_temp'

    #store original column for later reversion
    mdf_train[tempcolumn] = mdf_train[column].copy()
    mdf_test[tempcolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[tempcolumn] = pd.to_numeric(mdf_train[tempcolumn], errors='coerce')
    mdf_test[tempcolumn] = pd.to_numeric(mdf_test[tempcolumn], errors='coerce')
    
    #create copy with negative values
    negtempcolumn = column + '_negtemp'
    mdf_train[negtempcolumn] = mdf_train[tempcolumn].copy()
    mdf_test[negtempcolumn] = mdf_test[tempcolumn].copy()
    
    #convert all values in negtempcolumn >= 0 to Nan
    mdf_train[negtempcolumn] = \
    np.where(mdf_train[negtempcolumn] >= 0, np.nan, mdf_train[negtempcolumn].values)
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] >= 0, np.nan, mdf_test[negtempcolumn].values)
    
    #convert all values <= 0 to Nan
    mdf_train[tempcolumn] = \
    np.where(mdf_train[tempcolumn] <= 0, np.nan, mdf_train[tempcolumn].values)
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] <= 0, np.nan, mdf_test[tempcolumn].values)
    
    #log transform column
    
    #take abs value of negtempcolumn
    mdf_train[negtempcolumn] = mdf_train[negtempcolumn].abs()
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].abs()
    
    mdf_train[negtempcolumn] = \
    np.where(mdf_train[negtempcolumn] != np.nan, np.floor(np.log10(mdf_train[negtempcolumn])), mdf_train[negtempcolumn].values)
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] != np.nan, np.floor(np.log10(mdf_test[negtempcolumn])), mdf_test[negtempcolumn].values)
    
    train_neg_dict = {}
    newunique_list = []
    negunique = mdf_train[negtempcolumn].unique()
    for unique in negunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_-10^' + str(int(unique))
      train_neg_dict.update({unique : newunique})
      newunique_list.append(newunique)
      
    test_neg_dict = {}
    negunique = mdf_test[negtempcolumn].unique()
    for unique in negunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_-10^' + str(int(unique))
      if newunique in newunique_list and newunique == newunique:
        test_neg_dict.update({unique : newunique})
      else:
        test_neg_dict.update({unique : np.nan})
        
    mdf_train[negtempcolumn] = mdf_train[negtempcolumn].replace(train_neg_dict)
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].replace(test_neg_dict)
    
    
    #now log trasnform positive values in column column 

    mdf_train[tempcolumn] = \
    np.where(mdf_train[tempcolumn] != np.nan, np.floor(np.log10(mdf_train[tempcolumn])), mdf_train[tempcolumn].values)
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] != np.nan, np.floor(np.log10(mdf_test[tempcolumn])), mdf_test[tempcolumn].values)

    
    train_pos_dict = {}
    newposunique_list = []
    posunique = mdf_train[tempcolumn].unique()
    for unique in posunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_10^' + str(int(unique))
      train_pos_dict.update({unique : newunique})
      newposunique_list.append(newunique)
      
    test_pos_dict = {}
    posunique = mdf_test[tempcolumn].unique()
    for unique in posunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_10^' + str(int(unique))
      if newunique in newposunique_list and newunique == newunique:
        test_pos_dict.update({unique : newunique})
      else:
        test_pos_dict.update({unique : np.nan})
    
    
    mdf_train[tempcolumn] = mdf_train[tempcolumn].replace(train_pos_dict)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].replace(test_pos_dict)

    
    
    #combine the two columns
    mdf_train[tempcolumn] = mdf_train[negtempcolumn].where(mdf_train[negtempcolumn] == mdf_train[negtempcolumn], mdf_train[tempcolumn])
    mdf_test[tempcolumn] = mdf_test[negtempcolumn].where(mdf_test[negtempcolumn] == mdf_test[negtempcolumn], mdf_test[tempcolumn])
    

    
    #pandas one hot encoder
    df_train_cat = pd.get_dummies(mdf_train[tempcolumn])
    df_test_cat = pd.get_dummies(mdf_test[tempcolumn])

    
    labels_train = list(df_train_cat)
    labels_test = list(df_test_cat)

    
    #Get missing columns in test set that are present in training set
    missing_cols = set( df_train_cat.columns ) - set( df_test_cat.columns )
    
    #Add a missing column in test set with default value equal to 0
    for c in missing_cols:
        df_test_cat[c] = 0
    #Ensure the order of column in the test set is in the same order than in train set
    #Note this also removes categories in test set that aren't present in training set
    df_test_cat = df_test_cat[df_train_cat.columns]
    
    #concatinate the sparse set with the rest of our training data
    mdf_train = pd.concat([df_train_cat, mdf_train], axis=1)
    mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)
    
    #replace original column from training data
    
    del mdf_train[negtempcolumn]    
    del mdf_test[negtempcolumn]
    
    del mdf_train[tempcolumn]    
    del mdf_test[tempcolumn]
    
    
    #create output of a list of the created column names
#     NAcolumn = columnNAr2
    labels_train = list(df_train_cat)
#     if NAcolumn in labels_train:
#       labels_train.remove(NAcolumn)
    powercolumns = labels_train
  
    #change data type for memory savings
    for powercolumn in powercolumns:
      mdf_train[powercolumn] = mdf_train[powercolumn].astype(np.int8)
      mdf_test[powercolumn] = mdf_test[powercolumn].astype(np.int8)
    
    normalizationdictvalues = labels_train
    normalizationdictkeys = powercolumns
    
    normalizationdictkeys.sort()
    normalizationdictvalues.sort()
    
    powerlabelsdict = dict(zip(normalizationdictkeys, normalizationdictvalues))
    
        
    #store some values in the text_dict{} for use later in ML infill methods
    column_dict_list = []
    
    categorylist = powercolumns.copy()
    
    for pc in powercolumns:
      
      #new parameter collected for driftreport
      tc_ratio = pc + '_ratio'
      tcratio = mdf_train[pc].sum() / mdf_train[pc].shape[0]

      powernormalization_dict = {pc : {'powerlabelsdict' : powerlabelsdict, \
                                       'labels_train' : labels_train, \
                                       'missing_cols' : missing_cols, \
                                       tc_ratio : tcratio}}
#                                        'meanlog' : meanlog, \
#                                        'maxlog' : maxlog}}
    
      column_dict = {pc : {'category' : 'pwr2', \
                           'origcategory' : category, \
                           'normalization_dict' : powernormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : powercolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}
        
      column_dict_list.append(column_dict.copy())
    
    return mdf_train, mdf_test, column_dict_list
  
  def process_por2_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in ordinal encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #infill has 0, other designations are based on the data
    
    #negative values allows, comparable to pwr2
    '''
    
    pworcolumn = column + '_por2'

    #store original column for later reversion
    mdf_train[pworcolumn] = mdf_train[column].copy()
    mdf_test[pworcolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[pworcolumn] = pd.to_numeric(mdf_train[pworcolumn], errors='coerce')
    mdf_test[pworcolumn] = pd.to_numeric(mdf_test[pworcolumn], errors='coerce')
    
    
    #copy set for negative values
    negtempcolumn = column + '_negtempcolumn'
    
    mdf_train[negtempcolumn] = mdf_train[pworcolumn].copy()
    mdf_test[negtempcolumn] = mdf_test[pworcolumn].copy()
    
    #convert all values >= 0 to Nan
    mdf_train[negtempcolumn] = \
    np.where(mdf_train[negtempcolumn] >= 0, np.nan, mdf_train[negtempcolumn].values)
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] >= 0, np.nan, mdf_test[negtempcolumn].values)
    
    #take abs value of negtempcolumn
    mdf_train[negtempcolumn] = mdf_train[negtempcolumn].abs()
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].abs()
    
    
    #convert all values <= 0 in column to Nan
    mdf_train[pworcolumn] = \
    np.where(mdf_train[pworcolumn] <= 0, np.nan, mdf_train[pworcolumn].values)
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] <= 0, np.nan, mdf_test[pworcolumn].values)


    mdf_train[pworcolumn] = \
    np.where(mdf_train[pworcolumn] != np.nan, np.floor(np.log10(mdf_train[pworcolumn])), mdf_train[pworcolumn].values)
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] != np.nan, np.floor(np.log10(mdf_test[pworcolumn])), mdf_test[pworcolumn].values)
    
    #do same for negtempcolumn
    mdf_train[negtempcolumn] = \
    np.where(mdf_train[negtempcolumn] != np.nan, np.floor(np.log10(mdf_train[negtempcolumn])), mdf_train[negtempcolumn].values)
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] != np.nan, np.floor(np.log10(mdf_test[negtempcolumn])), mdf_test[negtempcolumn].values)
  

  
    train_neg_dict = {}
    newunique_list = []
    negunique = mdf_train[negtempcolumn].unique()
    for unique in negunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_-10^' + str(int(unique))
      train_neg_dict.update({unique : newunique})
      newunique_list.append(newunique)
      
    test_neg_dict = {}
    negunique = mdf_test[negtempcolumn].unique()
    for unique in negunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_-10^' + str(int(unique))
      if newunique in newunique_list and newunique == newunique:
        test_neg_dict.update({unique : newunique})
      else:
        test_neg_dict.update({unique : np.nan})
        
    mdf_train[negtempcolumn] = mdf_train[negtempcolumn].replace(train_neg_dict)
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].replace(test_neg_dict)
    
    
    #now do same for column
    train_pos_dict = {}
    newposunique_list = []
    posunique = mdf_train[pworcolumn].unique()
    for unique in posunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_10^' + str(int(unique))
      train_pos_dict.update({unique : newunique})
      newposunique_list.append(newunique)
      
    test_pos_dict = {}
    posunique = mdf_test[pworcolumn].unique()
    for unique in posunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_10^' + str(int(unique))
      if newunique in newposunique_list and newunique == newunique:
        test_pos_dict.update({unique : newunique})
      else:
        test_pos_dict.update({unique : np.nan})
    
    
    mdf_train[pworcolumn] = mdf_train[pworcolumn].replace(train_pos_dict)
    mdf_test[pworcolumn] = mdf_test[pworcolumn].replace(test_pos_dict)
    
    
    #combine the two columns
    mdf_train[pworcolumn] = mdf_train[negtempcolumn].where(mdf_train[negtempcolumn] == mdf_train[negtempcolumn], mdf_train[pworcolumn])
    mdf_test[pworcolumn] = mdf_test[negtempcolumn].where(mdf_test[negtempcolumn] == mdf_test[negtempcolumn], mdf_test[pworcolumn])
    
    train_unique = mdf_train[pworcolumn].unique()
    test_unique = mdf_test[pworcolumn].unique()
  
    #Get missing entries in test set that are present in training set
    missing_cols = set( list(train_unique) ) - set( list(test_unique) )
    
    extra_cols = set( list(test_unique) ) - set( list(train_unique) )
    
    train_replace_dict = {}
    train_len = len(train_unique)
    for i in range(train_len):
      if train_unique[i] != train_unique[i]:
        train_replace_dict.update({train_unique[i] : 0})
      else:
        train_replace_dict.update({train_unique[i] : i+1})
      
    test_replace_dict = {}
    for testunique in test_unique:
      if testunique in train_unique:
        test_replace_dict.update({testunique : train_replace_dict[testunique]})
      else:
        test_replace_dict.update({testunique : 0})
        
        
    
#     pworcolumn = column + '_por2'
#     mdf_train[pworcolumn] = mdf_train[column].copy()
#     mdf_test[pworcolumn] = mdf_test[column].copy()
    
    
    mdf_train[pworcolumn] = mdf_train[pworcolumn].replace(train_replace_dict)
    mdf_test[pworcolumn] = mdf_test[pworcolumn].replace(test_replace_dict)
    
        
    #replace original column from training data
    del mdf_train[negtempcolumn]    
    del mdf_test[negtempcolumn]    
    
#     del mdf_train[column]    
#     del mdf_test[column]
    
#     mdf_train[column] = mdf_train[column + '_temp'].copy()
#     mdf_test[column] = mdf_test[column + '_temp'].copy()

#     del mdf_train[column + '_temp']    
#     del mdf_test[column + '_temp']
        
    #store some values in the text_dict{} for use later in ML infill methods
    column_dict_list = []
    
    powercolumns = [pworcolumn]
    
    categorylist = powercolumns.copy()
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[pworcolumn].unique():
      sumcalc = (mdf_train[pworcolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[pworcolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})
    
    for pc in powercolumns:

      powernormalization_dict = {pc : {'train_replace_dict' : train_replace_dict, \
                                       'test_replace_dict' : test_replace_dict, \
                                       'ordl_activations_dict' : ordl_activations_dict}}
    
      column_dict = {pc : {'category' : 'por2', \
                           'origcategory' : category, \
                           'normalization_dict' : powernormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : powercolumns, \
                           'categorylist' : categorylist, \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}
        
      column_dict_list.append(column_dict.copy())
    
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bins_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to post z score
    #normalization of <-2, -2-1, -10, 01, 12, >2 in one hot encoded columns
    
    #bins will be intended for a raw set that is not normalized
    #bint will be intended for a previously normalized set
    '''
    
    binscolumn = column + '_bins'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #subtract mean from column for both train and test
    mdf_train[binscolumn] = mdf_train[binscolumn] - mean
    mdf_test[binscolumn] = mdf_test[binscolumn] - mean

    #get standard deviation of training data
    std = mdf_train[binscolumn].std()
    
    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if std == 0:
      std = 1

    #divide column values by std for both training and test data
    mdf_train[binscolumn] = mdf_train[binscolumn] / std
    mdf_test[binscolumn] = mdf_test[binscolumn] / std


    #create bins based on standard deviation increments
#     binscolumn = column + '_bins'
    mdf_train[binscolumn] = \
    pd.cut( mdf_train[binscolumn], bins = [-float('inf'),-2,-1,0,1,2,float('inf')],  \
           labels = ['s<-2','s-21','s-10','s+01','s+12','s>+2'], precision=4)
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = ['s<-2','s-21','s-10','s+01','s+12','s>+2'], precision=4)



    textcolumns = \
    [binscolumn + '_s<-2', binscolumn + '_s-21', binscolumn + '_s-10', \
     binscolumn + '_s+01', binscolumn + '_s+12', binscolumn + '_s>+2']


    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                        'categorylist' : textcolumns}}}
    
    

    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)

    
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]


    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'binsstd' : std, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bins', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())



    #return mdf_train, mdf_test, mean, std, nmbrcolumns, categorylist
    return mdf_train, mdf_test, column_dict_list
  
  
  
  
  def process_bint_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to post z score
    #normalization of <-2, -2-1, -10, 01, 12, >2 in one hot encoded columns
    
    #bins will be intended for a raw set that is not normalized
    #bint will be intended for a previously z-score normalized set
    #with mean 0 and std 1
    '''
    
    binscolumn = column + '_bint'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    #mean = mdf_train[column].mean()
    mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

#     #subtract mean from column for both train and test
#     mdf_train[column] = mdf_train[column] - mean
#     mdf_test[column] = mdf_test[column] - mean

#     #get standard deviation of training data
#     std = mdf_train[column].std()

#     #divide column values by std for both training and test data
#     mdf_train[column] = mdf_train[column] / std
#     mdf_test[column] = mdf_test[column] / std


    #create bins based on standard deviation increments
#     binscolumn = column + '_bint'
    mdf_train[binscolumn] = \
    pd.cut( mdf_train[binscolumn], bins = [-float('inf'),-2,-1,0,1,2,float('inf')],  \
           labels = ['t<-2','t-21','t-10','t+01','t+12','t>+2'], precision=4)
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = ['t<-2','t-21','t-10','t+01','t+12','t>+2'], precision=4)



    textcolumns = \
    [binscolumn + '_t<-2', binscolumn + '_t-21', binscolumn + '_t-10', \
     binscolumn + '_t+01', binscolumn + '_t+12', binscolumn + '_t>+2']


    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbint_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns, \
                                                        'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbint_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbint_postprocess_dict, tempkey)
    

    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]


    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]
      
      nmbrnormalization_dict = {nc : {'bintmean' : 0, \
                                      'bintstd' : 1, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bint', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())



    #return mdf_train, mdf_test, mean, std, nmbrcolumns, categorylist
    return mdf_train, mdf_test, column_dict_list
  
  def process_bsor_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to post z score
    #normalization of <-2, -2-1, -10, 01, 12, >2 in one hot encoded columns
    
    #bins will be intended for a raw set that is not normalized
    #bint will be intended for a previously normalized set
    '''
    
    binscolumn = column + '_bsor'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #subtract mean from column for both train and test
    mdf_train[binscolumn] = mdf_train[binscolumn] - mean
    mdf_test[binscolumn] = mdf_test[binscolumn] - mean

    #get standard deviation of training data
    std = mdf_train[binscolumn].std()
    
    #special case, if standard deviation is 0 we'll set it to 1 to avoid division by 0
    if std == 0:
      std = 1

    #divide column values by std for both training and test data
    mdf_train[binscolumn] = mdf_train[binscolumn] / std
    mdf_test[binscolumn] = mdf_test[binscolumn] / std


#     binscolumn = column + '_bsor'
    mdf_train[binscolumn] = \
    pd.cut( mdf_train[binscolumn], bins = [-float('inf'),-2,-1,0,1,2,float('inf')],  \
           labels = [0,1,2,3,4,5], precision=4)
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = [0,1,2,3,4,5], precision=4)
    
    
    ordinal_dict = {'s<-2':0,'s-21':1,'s-10':2,'s+01':3,'s+12':4,'s>+2':5}
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for key in ordinal_dict:
      sumcalc = (mdf_train[binscolumn] == ordinal_dict[key]).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({key:ratio})


    #create list of columns
    nmbrcolumns = [binscolumn]


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []


    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'ordinal_dict' : ordinal_dict, \
                                      'ordl_activations_dict' : ordl_activations_dict, \
                                      'binsmean' : mean, \
                                      'binsstd' : std}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bsor', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())



    #return mdf_train, mdf_test, mean, std, nmbrcolumns, categorylist
    return mdf_train, mdf_test, column_dict_list
  

  
  def process_bnwd_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1
      
    binscolumn = column + '_bnwd'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(str(bn_width) + '_' + str(i))
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwd'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))

    foundinset = mdf_train[binscolumn].unique()
    
    textcolumns = []
    for i in foundinset:
      textcolumns.append(binscolumn + '_' + str(i))
      
    #postprocess_textsupport_class will return columns in alphabetical order
    textcolumns.sort()
    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                             'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
      
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width_bnwd' : bn_width, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bnwd', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bnwK_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000

    binscolumn = column + '_bnwK'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(str(bn_width) + '_' + str(i))
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwd'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))

    foundinset = mdf_train[binscolumn].unique()
    
    textcolumns = []
    for i in foundinset:
      textcolumns.append(binscolumn + '_' + str(i))
      
    #postprocess_textsupport_class will return columns in alphabetical order
    textcolumns.sort()
    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                             'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
      
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width_bnwK' : bn_width, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bnwK', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  
  
  def process_bnwM_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000000
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000000

    binscolumn = column + '_bnwM'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(str(bn_width) + '_' + str(i))
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwd'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))

    foundinset = mdf_train[binscolumn].unique()
    
    textcolumns = []
    for i in foundinset:
      textcolumns.append(binscolumn + '_' + str(i))
      
    #postprocess_textsupport_class will return columns in alphabetical order
    textcolumns.sort()
    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                             'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
      
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width_bnwM' : bn_width, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bnwM', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  

  def process_bnwo_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1
      
    binscolumn = column + '_bnwo'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(i)
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    #create list of columns
    nmbrcolumns = [binscolumn]
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width' : bn_width, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bnwo', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  

  def process_bnKo_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000

    binscolumn = column + '_bnKo'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(i)
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    #create list of columns
    nmbrcolumns = [binscolumn]

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width' : bn_width, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bnKo', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bnMo_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000000
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000000

    binscolumn = column + '_bnMo'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
    if bn_delta == 0:
      bn_delta = 1
    bn_count = int(np.ceil(bn_delta / bn_width))
    
    bins_id = []
    for i in range(bn_count):
      bins_id.append(i)
      
    bins_cuts = [-float('inf'), float('inf')]
    for i in range(bn_count-1):
      bins_cuts.insert(-1,(bn_min + (i+1) * bn_width))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    #create list of columns
    nmbrcolumns = [binscolumn]


    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bn_width' : bn_width, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bnMo', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bnep_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 5
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 5
      
    binscolumn = column + '_bnep'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

#     if bn_delta > 0 and bn_min == bn_min:

    if bn_delta > 0 and bn_min == bn_min:

      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operatoin
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)




      bins_id = []
      for i in range(bn_count):
        bins_id.append(str(i))

      bins_cuts = cutintervals

      #create bins based on standard deviation increments
  #     binscolumn = column + '_bnwd'
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      foundinset = mdf_train[binscolumn].unique()

      textcolumns = []
      for i in foundinset:
        if i == i:
          textcolumns.append(binscolumn + '_' + str(i))

      #postprocess_textsupport_class will return columns in alphabetical order
      textcolumns.sort()

      #we're going to use the postprocess_text_class function here since it 
      #allows us to force the columns even if no values present in the set
      #however to do so we're going to have to construct a fake postprocess_dict

      #a future extension should probnably build this capacity into a new distinct function

      #here are some data structures for reference to create the below
  #     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
  #     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      #process bins as a categorical set
      mdf_train = \
      self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)

      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)


      #delete the support column
      del mdf_train[binscolumn]
      del mdf_test[binscolumn]
      
      
    else:
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      textcolumns = [binscolumn]
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False
      
      

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount_bnep' : bincount, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bnep', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  def process_bne7_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 7
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 7
      
    binscolumn = column + '_bne7'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

    if bn_delta > 0 and bn_min == bn_min:

      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operatoin
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)




      bins_id = []
      for i in range(bn_count):
        bins_id.append(str(i))

      bins_cuts = cutintervals

      #create bins based on standard deviation increments
  #     binscolumn = column + '_bnwd'
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      foundinset = mdf_train[binscolumn].unique()

      textcolumns = []
      for i in foundinset:
        if i == i:
          textcolumns.append(binscolumn + '_' + str(i))

      #postprocess_textsupport_class will return columns in alphabetical order
      textcolumns.sort()

      #we're going to use the postprocess_text_class function here since it 
      #allows us to force the columns even if no values present in the set
      #however to do so we're going to have to construct a fake postprocess_dict

      #a future extension should probnably build this capacity into a new distinct function

      #here are some data structures for reference to create the below
  #     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
  #     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      #process bins as a categorical set
      mdf_train = \
      self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)

      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)


      #delete the support column
      del mdf_train[binscolumn]
      del mdf_test[binscolumn]
      
      
    else:
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      textcolumns = [binscolumn]
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount_bne7' : bincount, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bne7', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bne9_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 9
      
    binscolumn = column + '_bne9'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

    if bn_delta > 0 and bn_min == bn_min:

      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operatoin
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)




      bins_id = []
      for i in range(bn_count):
        bins_id.append(str(i))

      bins_cuts = cutintervals

      #create bins based on standard deviation increments
  #     binscolumn = column + '_bnwd'
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      foundinset = mdf_train[binscolumn].unique()

      textcolumns = []
      for i in foundinset:
        if i == i:
          textcolumns.append(binscolumn + '_' + str(i))

      #postprocess_textsupport_class will return columns in alphabetical order
      textcolumns.sort()

      #we're going to use the postprocess_text_class function here since it 
      #allows us to force the columns even if no values present in the set
      #however to do so we're going to have to construct a fake postprocess_dict

      #a future extension should probnably build this capacity into a new distinct function

      #here are some data structures for reference to create the below
  #     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
  #     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      #process bins as a categorical set
      mdf_train = \
      self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)

      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)


      #delete the support column
      del mdf_train[binscolumn]
      del mdf_test[binscolumn]
      
      
    else:
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      textcolumns = [binscolumn]
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount_bne9' : bincount, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bne9', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list

  
  
  def process_bneo_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 5
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 5
      
    binscolumn = column + '_bneo'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

    if bn_delta > 0 and bn_min == bn_min:


      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operation
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)


      bins_id = []
      for i in range(bn_count):
        bins_id.append(i)

      bins_cuts = cutintervals

      #create bins based on prepared increments
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='ffill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='bfill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(0)
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)



      #change column dtype
      mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False
      

    #create list of columns
    nmbrcolumns = [binscolumn]

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})

    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount' : bincount, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bneo', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  def process_bn7o_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 7
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 7
      
    binscolumn = column + '_bn7o'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

    if bn_delta > 0 and bn_min == bn_min:


      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operation
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)


      bins_id = []
      for i in range(bn_count):
        bins_id.append(i)

      bins_cuts = cutintervals

      #create bins based on prepared increments
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='ffill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='bfill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(0)
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)



      #change column dtype
      mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False

    #create list of columns
    nmbrcolumns = [binscolumn]

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})

    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount' : bincount, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bn7o', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bn9o_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 9
      
    binscolumn = column + '_bn9o'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))


    if bn_delta > 0 and bn_min == bn_min:


      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operation
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)


      bins_id = []
      for i in range(bn_count):
        bins_id.append(i)

      bins_cuts = cutintervals

      #create bins based on prepared increments
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='ffill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(method='bfill')
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_train[binscolumn] = mdf_train[binscolumn].fillna(0)
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)



      #change column dtype
      mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False

    #create list of columns
    nmbrcolumns = [binscolumn]

    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})

    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount' : bincount, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bn9o', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  

  
  
  def process_tlbn_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in one-hot encoded set
    
    #how this differs from bnep in that the activated bins are replaced with
    #min-max scaling for source column values found in that bin, and then other values as -1
    
    #note that for the bottom bin order reversed to accomodate subsequent values out of range 
    #and still use -1 register
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 9

      
    binscolumn = column + '_tlbn'

    #copy original column
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

#     #replace missing data with training set mean
#     mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #evaluate train set for transformation parameters
    bn_min = mdf_train[binscolumn].min()
    bn_max = mdf_train[binscolumn].max()
    bn_delta = bn_max - bn_min
#     if bn_delta == 0:
#       bn_delta = 1
#     bn_count = int(np.ceil(bn_delta / bn_width))

    if bn_delta > 0 and bn_min == bn_min:

      #grab the intervals using qcut based on equal population in train set
      intervalset = pd.qcut(mdf_train[binscolumn].values, bincount, duplicates='drop').unique()

      #note we're sorting here, and scrubbing any nan
      intervalset = sorted([interval for interval in intervalset if interval == interval])

      #we'll make the bottom interval open-ended at negative end
      firstinterval = pd.Interval(-np.inf, intervalset[0].right, closed='right')

      #and the last interval open-ended at positive end
      lastinterval = pd.Interval(intervalset[-1].left, np.inf, closed='right')

      #now create a list to apply in cut operatoin
      newinterval_list = []

      #now we'll assemble a list of intervals to prepare for the cut operation, 
      #replacing first and last with the open-ended
      for i in range(len(intervalset)):
        if i == 0:
          newinterval_list.append(firstinterval)
        elif i == len(intervalset)-1:
          newinterval_list.append(lastinterval)
        else:
          newinterval_list.append(intervalset[i])

      #now translate intervals to the list of boundaries for cut operatoin
      cutintervals = []

      for interval in newinterval_list:

        cutintervals.append(interval.left)

      cutintervals.append(np.inf)



      bn_count = len(newinterval_list)




      bins_id = []
      for i in range(bn_count):
        bins_id.append(str(i))

      bins_cuts = cutintervals

      #create bins based on standard deviation increments
  #     binscolumn = column + '_bnwd'
      mdf_train[binscolumn] = \
      pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

      foundinset = mdf_train[binscolumn].unique()

      textcolumns = []
      for i in foundinset:
        if i == i:
          textcolumns.append(binscolumn + '_' + str(i))

      #postprocess_textsupport_class will return columns in alphabetical order
      textcolumns.sort()

      #we're going to use the postprocess_text_class function here since it 
      #allows us to force the columns even if no values present in the set
      #however to do so we're going to have to construct a fake postprocess_dict

      #a future extension should probnably build this capacity into a new distinct function

      #here are some data structures for reference to create the below
  #     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
  #     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}
      #process bins as a categorical set
      mdf_train = \
      self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
      
      #initialize binscolumn once more
      mdf_train[binscolumn] = mdf_train[column].copy()
      mdf_test[binscolumn] = mdf_test[column].copy()
      
      mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')
      
      
      if len(textcolumns) > 1:

        #for i in range(bincount):
        for i in range(len(textcolumns)):

          tlbn_column = binscolumn + '_' + str(i)


          if i == 0:

            mdf_train[tlbn_column] = \
            np.where(mdf_train[tlbn_column] == 1, \
                    (bins_cuts[i+1] - mdf_train[binscolumn]) / (bins_cuts[i+1] - bn_min), -1)

            mdf_test[tlbn_column] = \
            np.where(mdf_test[tlbn_column] == 1, \
                    (bins_cuts[i+1] - mdf_test[binscolumn]) / (bins_cuts[i+1] - bn_min), -1)

          elif i == bincount - 1:

            mdf_train[tlbn_column] = \
            np.where(mdf_train[tlbn_column] == 1, \
                    (mdf_train[binscolumn] - bins_cuts[i]) / (bn_max - bins_cuts[i]), -1)

            mdf_test[tlbn_column] = \
            np.where(mdf_test[tlbn_column] == 1, \
                    (mdf_test[binscolumn] - bins_cuts[i]) / (bn_max - bins_cuts[i]), -1)

          else:

            mdf_train[tlbn_column] = \
            np.where(mdf_train[tlbn_column] == 1, \
                    (mdf_train[binscolumn] - bins_cuts[i]) / (bins_cuts[i+1] - bins_cuts[i]), -1)

            mdf_test[tlbn_column] = \
            np.where(mdf_test[tlbn_column] == 1, \
                    (mdf_test[binscolumn] - bins_cuts[i]) / (bins_cuts[i+1] - bins_cuts[i]), -1)


      
      

#       #change data type for memory savings
#       for textcolumn in textcolumns:
#         mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
#         mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)


      #delete the support column
      del mdf_train[binscolumn]
      del mdf_test[binscolumn]
      
      
    else:
      mdf_train[binscolumn] = 0
      mdf_test[binscolumn] = 0
      
      textcolumns = [binscolumn]
      
      bn_count = bincount
      bins_id = False
      bins_cuts = False

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'bn_min' : bn_min, \
                                      'bn_max' : bn_max, \
                                      'bn_delta' : bn_delta, \
                                      'bn_count' : bn_count, \
                                      'bins_id' : bins_id, \
                                      'bins_cuts' : bins_cuts, \
                                      'bincount_tlbn' : bincount, \
                                      'textcolumns' : textcolumns, \
                                      tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'tlbn', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  
  def process_bkt1_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets unconstrained
    #and returning in one-hot encoded set
    
    #removes buckets without activations in train set
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
      
      
    binscolumn = column + '_bkt1'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #assemble buckets  
    bins_cuts = buckets.copy()
    bins_cuts.insert(0, -np.inf)
    bins_cuts.insert(len(bins_cuts), np.inf)
    
    #create labels for bins
    bins_id = list(range(len(bins_cuts)-1))
    
    
    #create bins based on increments
#     binscolumn = column + '_bnwd'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))

    foundinset = mdf_train[binscolumn].unique()
    
    textcolumns = []
    for i in foundinset:
      textcolumns.append(binscolumn + '_' + str(i))
      
    #postprocess_textsupport_class will return columns in alphabetical order
    textcolumns.sort()
    
    #remove nan for cases where value did not fall within range
    textcolumns = [x for x in textcolumns if x[-3:] != 'nan']
    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                             'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
      
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'buckets_bkt1' : buckets, \
                                      'bins_cuts' : bins_cuts, \
                                      'bins_id' : bins_id, \
                                      'textcolumns' : textcolumns, \
                                       tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bkt1', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bkt2_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets bounded
    #and returning in one-hot encoded set
    
    #removes buckets without activations in train set
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
      
    binscolumn = column + '_bkt2'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #assemble buckets  
    bins_cuts = buckets.copy()
#     bins_cuts.insert(0, -np.inf)
#     bins_cuts.insert(len(bins_cuts), np.inf)

    #create labels for bins
    bins_id = list(range(len(bins_cuts)-1))
    
    #create bins based on increments
#     binscolumn = column + '_bnwd'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))

    foundinset = mdf_train[binscolumn].unique()
    
    textcolumns = []
    for i in foundinset:
      textcolumns.append(binscolumn + '_' + str(i))
      
    #postprocess_textsupport_class will return columns in alphabetical order
    textcolumns.sort()
    
    #remove nan for cases where value did not fall within range
    textcolumns = [x for x in textcolumns if x[-3:] != 'nan']
    
    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                             'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_train = \
    self.postprocess_textsupport_class(mdf_train, binscolumn, tempbins_postprocess_dict, tempkey)
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_train[textcolumn] = mdf_train[textcolumn].astype(np.int8)
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
      
    
    #delete the support column
    del mdf_train[binscolumn]
    del mdf_test[binscolumn]

    #create list of columns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:
      
      #new parameter collected for driftreport
      tc_ratio = nc + '_ratio'
      tcratio = mdf_train[nc].sum() / mdf_train[nc].shape[0]

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'buckets_bkt2' : buckets, \
                                      'bins_cuts' : bins_cuts, \
                                      'bins_id' : bins_id, \
                                      'textcolumns' : textcolumns, \
                                       tc_ratio : tcratio}}

      if nc in textcolumns:

        column_dict = { nc : {'category' : 'bkt2', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : textcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())
       
    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bkt3_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets unconstrained
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
      
    binscolumn = column + '_bkt3'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #assemble buckets  
    bins_cuts = buckets.copy()
    bins_cuts.insert(0, -np.inf)
    bins_cuts.insert(len(bins_cuts), np.inf)
    
    #create labels for bins
    bins_id = list(range(len(bins_cuts)-1))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    
    #change column dtype
    mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    #create list of columns
    nmbrcolumns = [binscolumn]
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'buckets' : buckets, \
                                      'bins_cuts' : bins_cuts, \
                                      'bins_id' : bins_id, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bkt3', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_bkt4_class(self, mdf_train, mdf_test, column, category, postprocess_dict, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets bounded
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
      
    binscolumn = column + '_bkt4'

    #store original column for later reversion
    mdf_train[binscolumn] = mdf_train[column].copy()
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_train[binscolumn] = pd.to_numeric(mdf_train[binscolumn], errors='coerce')
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')
    
    #set all values that fall outside of bounded buckets to nan for replacement with mean
    mdf_train.loc[mdf_train[binscolumn] <= buckets[0], (binscolumn)] = np.nan
    mdf_test.loc[mdf_test[binscolumn] <= buckets[0], (binscolumn)] = np.nan
    
    mdf_train.loc[mdf_train[binscolumn] > buckets[-1], (binscolumn)] = np.nan
    mdf_test.loc[mdf_test[binscolumn] > buckets[-1], (binscolumn)] = np.nan

    #get mean of training data
    mean = mdf_train[binscolumn].mean()
    
    if mean != mean:
      mean = 0
      
    #edge case, if mean does nto fall within buckets range, we'll apply infill to top bucket
    #this edge case specific to bkt4
    #this assumes buckets was passed with sorted values
    if mean < buckets[0] or mean > buckets[-1]:
      mean = buckets[-1]

    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(mean)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    #assemble buckets  
    bins_cuts = buckets.copy()
#     bins_cuts.insert(0, -np.inf)
#     bins_cuts.insert(len(bins_cuts), np.inf)

    #create labels for bins
    bins_id = list(range(len(bins_cuts)-1))
      
    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_train[binscolumn] = \
    pd.cut(mdf_train[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    
    #edge case
    #replace missing data with training set mean
    mdf_train[binscolumn] = mdf_train[binscolumn].fillna(0)
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)

    
    #change column dtype
    mdf_train[binscolumn] = mdf_train[binscolumn].astype(int)
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    #create list of columns
    nmbrcolumns = [binscolumn]
    
    #new driftreport metric ordl_activations_dict
    ordl_activations_dict = {}
    for unique in mdf_train[binscolumn].unique():
      sumcalc = (mdf_train[binscolumn] == unique).sum() 
      ratio = sumcalc / mdf_train[binscolumn].shape[0]
      ordl_activations_dict.update({unique:ratio})


    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      nmbrnormalization_dict = {nc : {'binsmean' : mean, \
                                      'buckets' : buckets, \
                                      'bins_cuts' : bins_cuts, \
                                      'bins_id' : bins_id, \
                                      'ordl_activations_dict' : ordl_activations_dict}}

      if nc in nmbrcolumns:

        column_dict = { nc : {'category' : 'bkt4', \
                             'origcategory' : category, \
                             'normalization_dict' : nmbrnormalization_dict, \
                             'origcolumn' : column, \
                             'inputcolumn' : column, \
                             'columnslist' : nmbrcolumns, \
                             'categorylist' : nmbrcolumns, \
                             'infillmodel' : False, \
                             'infillcomplete' : False, \
                             'deletecolumn' : False}}

        column_dict_list.append(column_dict.copy())


    return mdf_train, mdf_test, column_dict_list
  
  
  def process_null_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #here we'll delete any columns that returned a 'null' category
    #note this is a. singleprocess transform
    '''
    
    #df = df.drop([column], axis=1)
    #deletion takes place elsewhere

    column_dict_list = []

    column_dict = {column + '_null' : {'category' : 'null', \
                                      'origcategory' : category, \
                                      'normalization_dict' : {column + '_null':{}}, \
                                      'origcolumn' : column, \
                                      'inputcolumn' : column, \
                                      'columnslist' : [column], \
                                      'categorylist' : [], \
                                      'infillmodel' : False, \
                                      'infillcomplete' : False, \
                                      'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return df, column_dict_list
  
  def process_copy_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #copy function
    #accepts parameter 'suffix' for suffix appender
    #useful if want to apply same function more than once with different parameters
    '''
    
    
    if 'suffix' in params:
        
      copy_column = column + params['suffix']
    
    else:
      
      copy_column = column + '_copy'

    
    df[copy_column] = df[column].copy()

    
    column_dict_list = []

    column_dict = {copy_column : {'category' : 'copy', \
                                 'origcategory' : category, \
                                 'normalization_dict' : {copy_column:{}}, \
                                 'origcolumn' : column, \
                                 'inputcolumn' : column, \
                                 'columnslist' : [copy_column], \
                                 'categorylist' : [copy_column], \
                                 'infillmodel' : False, \
                                 'infillcomplete' : False, \
                                 'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return df, column_dict_list


  def process_excl_class(self, df, column, category, postprocess_dict, params = {}):
    """
    #here we'll address any columns that returned a 'excl' category
    #note this is a. singleprocess transform
    #we'll simply maintain the same column but with a suffix to the header
    #the excl trasnform is a very special exception, and this suffix is later
    #removed when automunge(*.)parameter excl_suffix passed as False
    
    #note that excl transform is also special in that it may only be applied
    #as a supplement primitive in a family tree (eg cousins)
    #as it replaces the source column internally (by a simple rename)
    
    #Note that the function check_transformdict(.) works 'under the hood'
    #to translate user passed excl transforms in family trees
    #from replacement primitives to corresponding supplement primitives
    """
    
    exclcolumn = column + '_excl'
    #df[exclcolumn] = df[column].copy()
    #del df[column]
    
    df.rename(columns = {column : exclcolumn}, inplace = True)
    
    
    column_dict_list = []

    column_dict = {exclcolumn : {'category' : 'excl', \
                                 'origcategory' : category, \
                                 'normalization_dict' : {exclcolumn:{}}, \
                                 'origcolumn' : column, \
                                 'inputcolumn' : column, \
                                 'columnslist' : [exclcolumn], \
                                 'categorylist' : [exclcolumn], \
                                 'infillmodel' : False, \
                                 'infillcomplete' : False, \
                                 'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return df, column_dict_list


  def process_exc2_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #here we'll address any columns that returned a 'excl' category
    #note this is a. singleprocess transform
    #we'll simply maintain the same column but with a suffix to the header
    '''
    
    
    exclcolumn = column + '_exc2'
    
    
    mdf_train[exclcolumn] = mdf_train[column].copy()
    mdf_test[exclcolumn] = mdf_test[column].copy()
    
    #del df[column]
    
    mdf_train[exclcolumn] = pd.to_numeric(mdf_train[exclcolumn], errors='coerce')
    mdf_test[exclcolumn] = pd.to_numeric(mdf_test[exclcolumn], errors='coerce')
    
    if len(mdf_train[exclcolumn].mode())<1:
      fillvalue = mdf_train[exclcolumn].mean()
    else:
      fillvalue = mdf_train[exclcolumn].mode()[0]
      
    #special case if column didn't have any numeric entries
    if fillvalue != fillvalue:
      fillvalue = 0
    
    #replace missing data with fill value
    mdf_train[exclcolumn] = mdf_train[exclcolumn].fillna(fillvalue)
    mdf_test[exclcolumn] = mdf_test[exclcolumn].fillna(fillvalue)
    
    exc2_normalization_dict = {exclcolumn : {'fillvalue' : fillvalue}}
    
    column_dict_list = []

    column_dict = {exclcolumn : {'category' : 'exc2', \
                                 'origcategory' : category, \
                                 'normalization_dict' : exc2_normalization_dict, \
                                 'origcolumn' : column, \
                                 'inputcolumn' : column, \
                                 'columnslist' : [exclcolumn], \
                                 'categorylist' : [exclcolumn], \
                                 'infillmodel' : False, \
                                 'infillcomplete' : False, \
                                 'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return mdf_train, mdf_test, column_dict_list
  
  
  def process_exc5_class(self, mdf_train, mdf_test, column, category, \
                         postprocess_dict, params = {}):
    '''
    #here we'll address any columns that returned a 'excl' category
    #note this is a. singleprocess transform
    #we'll simply maintain the same column but with a suffix to the header
    '''
    
    
    exclcolumn = column + '_exc5'
    
    
    mdf_train[exclcolumn] = mdf_train[column].copy()
    mdf_test[exclcolumn] = mdf_test[column].copy()
    
    #del df[column]
    
    mdf_train[exclcolumn] = pd.to_numeric(mdf_train[exclcolumn], errors='coerce')
    mdf_test[exclcolumn] = pd.to_numeric(mdf_test[exclcolumn], errors='coerce')
    
    #non integers are subject to infill
    mdf_train[exclcolumn] = np.where(mdf_train[exclcolumn] == mdf_train[exclcolumn].round(), mdf_train[exclcolumn], np.nan)
    mdf_test[exclcolumn] = np.where(mdf_test[exclcolumn] == mdf_test[exclcolumn].round(), mdf_test[exclcolumn], np.nan)

    
    if len(mdf_train[exclcolumn].mode())<1:
      fillvalue = mdf_train[exclcolumn].mean()
    else:
      fillvalue = mdf_train[exclcolumn].mode()[0]
      
    #special case if column didn't have any numeric entries
    if fillvalue != fillvalue:
      fillvalue = 0
    
    #replace missing data with fill value
    mdf_train[exclcolumn] = mdf_train[exclcolumn].fillna(fillvalue)
    mdf_test[exclcolumn] = mdf_test[exclcolumn].fillna(fillvalue)
    
    exc2_normalization_dict = {exclcolumn : {'fillvalue' : fillvalue}}
    
    column_dict_list = []

    column_dict = {exclcolumn : {'category' : 'exc5', \
                                 'origcategory' : category, \
                                 'normalization_dict' : exc2_normalization_dict, \
                                 'origcolumn' : column, \
                                 'inputcolumn' : column, \
                                 'columnslist' : [exclcolumn], \
                                 'categorylist' : [exclcolumn], \
                                 'infillmodel' : False, \
                                 'infillcomplete' : False, \
                                 'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return mdf_train, mdf_test, column_dict_list
  
  def process_exc6_class(self, df, column, category, postprocess_dict, params = {}):
    """
    #comparable to excl (direct pass-through, no infill) 
    #but uses a copy operation instead of inplace
    #thus can be used as entry in any family tree primitive
    #in a user passed transformdict
    """
    
    exclcolumn = column + '_exc6'
    df[exclcolumn] = df[column].copy()
    #del df[column]
    
    #df.rename(columns = {column : exclcolumn}, inplace = True)
    
    
    column_dict_list = []

    column_dict = {exclcolumn : {'category' : 'exc6', \
                                 'origcategory' : category, \
                                 'normalization_dict' : {exclcolumn:{}}, \
                                 'origcolumn' : column, \
                                 'inputcolumn' : column, \
                                 'columnslist' : [exclcolumn], \
                                 'categorylist' : [exclcolumn], \
                                 'infillmodel' : False, \
                                 'infillcomplete' : False, \
                                 'deletecolumn' : False}}
    
    #now append column_dict onto postprocess_dict
    column_dict_list.append(column_dict.copy())



    return df, column_dict_list
  

  def process_shfl_class(self, df, column, category, postprocess_dict, params = {}):
    '''
    #function to shuffle data in a column
    #non-numeric entries allowed
    #for missing values, uses adjacent cell infill as default
    '''
    
    
    #copy source column into new column
    df[column + '_shfl'] = df[column].copy()
    
    
    #we've introduced that randomseed is now accessible throughout in the postprocess_dict
    random = postprocess_dict['randomseed']
    
    #uses support function
    df = self.df_shuffle_series(df, column + '_shfl', random)
    
    
    #we'll do the adjacent cell infill after the shuffle operation
    
    #apply ffill to replace NArows with value from adjacent cell in preceding row
    df[column + '_shfl'] = df[column + '_shfl'].fillna(method='ffill')
    
    #we'll follow with a bfill just in case first row had a nan
    df[column + '_shfl'] = df[column + '_shfl'].fillna(method='bfill')
    
    
    #create list of columns
    nmbrcolumns = [column + '_shfl']


    nmbrnormalization_dict = {column + '_shfl' : {}}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []

    for nc in nmbrcolumns:

      column_dict = { nc : {'category' : 'shfl', \
                           'origcategory' : category, \
                           'normalization_dict' : nmbrnormalization_dict, \
                           'origcolumn' : column, \
                           'inputcolumn' : column, \
                           'columnslist' : nmbrcolumns, \
                           'categorylist' : [nc], \
                           'infillmodel' : False, \
                           'infillcomplete' : False, \
                           'deletecolumn' : False}}

      column_dict_list.append(column_dict.copy())
    

        
    return df, column_dict_list
  



  def evalcategory(self, df_source, column, randomseed, eval_ratio, \
                   numbercategoryheuristic, powertransform, labels = False):
    '''
    #evalcategory(df, column)
    #Function that dakes as input a dataframe and associated column id \
    #evaluates the contents of cells and classifies the column into one of four categories
    #category 1, 'bnry', is for columns with only two categorys of text or integer
    #category 2, 'nmbr', is for columns with ndumerical integer or float values
    #category 3: 'bxcx', is for nmbr category with all positive values
    #category 4, 'text', is for columns with multiple categories appropriate for one-hot
    #category 5, 'date', is for columns with Timestamp data
    #category 6, 'null', is for columns with >85% null values (arbitrary figure)
    #returns category id as a string
    '''
    
    #we'll introduce convention of special values for powertransform to change default
    #we'll allow powertransform == 'excl' to signal that nonassigned columns should
    #be left untouched (a simpler version of existing functionality of assigning excl in assigncat)
    if powertransform == 'excl':
      category = 'excl'
      
    #or powertransform == 'exc2' for unprocessed but subject to force to numeric and modeinfill
    elif powertransform == 'exc2':
      category = 'exc2'
    
    else:
      
      #_____
      #a few default categories
      
      #default categorical
      #defaultcategorical = 'text'
      defaultcategorical = '1010'
      
      defaultordinal = 'ord3'
      
      defaultnumerical = 'nmbr'
      #defaultnumerical = 'mean'
      
      defaultdatetime = 'dat6'
      
      #_____
      
      if eval_ratio > 0 and eval_ratio <= 1:
        eval_ratio = eval_ratio
      else:
        eval_ratio = eval_ratio / df_source.shape[0]
      
      #take a random sample of rows for evaluation based on eval_ratio heuristic
      df = pd.DataFrame(df_source[column]).sample(frac=eval_ratio, random_state=randomseed)

      #I couldn't find a good pandas tool for evaluating data class, \
      #So will produce an array containing data types of each cell and \
      #evaluate for most common variable using the collections library

      type1_df = df[column].apply(lambda x: type(x)).values

      c = collections.Counter(type1_df)
      mc = c.most_common(1)
      mc2 = c.most_common(2)

      #this is to address scenario where only one value so we can still call mc2[1][0]
      if len(mc2) == len(mc):
        mc2 = mc + mc

      #count number of unique values
      nunique = df[column].nunique()

      #check if nan present for cases where nunique == 3
      nanpresent = False
      if nunique == 3:
        for unique in df[column].unique():
          if unique != unique:
            nanpresent = True

      #free memory (dtypes are memory hogs)
      del type1_df


      #additional array needed to check for time series

      #df['typecolumn2'] = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce')))
      type2_df = df[column].apply(lambda x: type(pd.to_datetime(x, errors = 'coerce'))).values

      datec = collections.Counter(type2_df)
      datemc = datec.most_common(1)
      datemc2 = datec.most_common(2)

      #this is to address scenario where only one value so we can still call mc2[1][0]
      if len(datemc2) == len(datemc):
        datemc2 = datemc + datemc

      #free memory (dtypes are memory hogs)
      del type2_df

      #an extension of this approach could be for those columns that produce a text\
      #category to implement an additional text to determine the number of \
      #common groupings / or the amount of uniquity. For example if every row has\
      #a unique value then one-hot-encoding would not be appropriate. It would \
      #probably be apopropraite to either return an error message if this is found \
      #or alternatively find a furhter way to automate this processing such as \
      #look for contextual clues to groupings that can be inferred.

      #This is kind of hack to evaluate class by comparing these with output of mc
      checkint = 1
      checkfloat = 1.1
      checkstring = 'string'
      checkNAN = np.nan

      #there's probably easier way to do this, here will create a check for date
      df_checkdate = pd.DataFrame([{'checkdate' : '7/4/2018'}])
      df_checkdate['checkdate'] = pd.to_datetime(df_checkdate['checkdate'], errors = 'coerce')


      #create dummy variable to store determined class (default is text class)
      category = defaultcategorical


      #if most common in column is string and > two values, set category to text
      if isinstance(checkstring, mc[0][0]) and nunique > 2:
        category = defaultcategorical

      #if most common is date, set category to date
      if isinstance(df_checkdate['checkdate'][0], datemc[0][0]):
        category = defaultdatetime

      if df[column].dtype.name == 'category':
        if nunique <= 2:
          category = 'bnry'
        else:
          category = defaultcategorical

      #if most common in column is integer and > two values, set category to number of bxcx
      if isinstance(checkint, mc[0][0]) and nunique > 2:

        if df[column].dtype.name == 'category':
          if nunique <= 2:
            category = 'bnry'
          else:
            category = defaultcategorical

        #take account for numbercategoryheuristic
        #if df[column].nunique() / df[column].shape[0] < numbercategoryheuristic:
        #if nunique < numbercategoryheuristic:
        if nunique <= 3:
          if nunique == 3:
            category = 'text'
          else:
            category = 'bnry'
  #       if True is False:
  #         pass

        else:
          category = defaultnumerical


      #if most common in column is float, set category to number or bxcx
      if isinstance(checkfloat, mc[0][0]):

        #take account for numbercategoryheuristic
        #if df[column].nunique() / df[column].shape[0] < numbercategoryheuristic \
  #       if nunique < numbercategoryheuristic \
  #       or df[column].dtype.name == 'category':
  #       if df[column].dtype.name == 'category':
        if df[column].dtype.name == 'category':
          if nunique <= 2:
            category = 'bnry'
          else:
            category = defaultcategorical

        elif nunique <= 3:
          if nunique == 3:
            category = 'text'
          elif nunique <= 2:
            category = 'bnry'

        else:
          category = defaultnumerical


      #if most common in column is integer and <= two values, set category to binary
      if isinstance(checkint, mc[0][0]) and nunique <= 2:
        category = 'bnry'

      #if most common in column is string and <= two values, set category to binary
      if isinstance(checkstring, mc[0][0]) and nunique <= 2:
        category = 'bnry'


      #else if most common in column is NaN, re-evaluate using the second most common type
      #(I suspect the below might have a bug somewhere but is working on my current 
      #tests so will leave be for now)
      #elif df[column].isna().sum() >= df.shape[0] / 2:
      if df[column].isna().sum() >= df.shape[0] / 2:

        #if 2nd most common in column is string and two values, set category to binary
        if isinstance(checkstring, mc2[1][0]) and nunique == 2:
          category = 'bnry'

        #if 2nd most common in column is string and > two values, set category to text
        if isinstance(checkstring, mc2[1][0]) and nunique > 2:
          category = defaultcategorical

        #if 2nd most common is date, set category to date   
        if isinstance(df_checkdate['checkdate'][0], datemc2[1][0]):
          category = defaultdatetime

        #if 2nd most common in column is integer and > two values, set category to number
        if isinstance(checkint, mc2[1][0]) and nunique > 2:

          if df[column].dtype.name == 'category':
            if nunique <= 2:
              category = 'bnry'
            else:
              category = defaultcategorical

  #         #take account for numbercategoryheuristic
  #         #if df[column].nunique() / df[column].shape[0] < numbercategoryheuristic:
          if nunique <= 3:

            if nunique == 3:
              category = defaultcategorical
            else:
              category = 'bnry'

  #         if True is False:
  #           pass

          else:

            category = defaultnumerical

        #if 2nd most common in column is float, set category to number
        if isinstance(checkfloat, mc2[1][0]):

  #         #take account for numbercategoryheuristic
  #         #if df[column].nunique() / df[column].shape[0] < numbercategoryheuristic:
  #         if df[column].nunique() < numbercategoryheuristic:

  #           category = 'text'

  #         else:

          if df[column].dtype.name == 'category':
            if nunique <= 2:
              category = 'bnry'
            else:
              category = defaultcategorical

          if df[column].nunique() <= 3:

            if nunique == 3:
              category = 'text'
            else:
              category = 'bnry'

          else:

            category = defaultnumerical

        #if 2nd most common in column is integer and <= two values, set category to binary
        if isinstance(checkint, mc2[1][0]) and nunique <= 2:
          category = 'bnry'

        #if 2nd most common in column is string and <= two values, set category to binary
        if isinstance(checkstring, mc2[1][0]) and nunique <= 2:
          category = 'bnry'


      if df[column].isna().sum() == df.shape[0]:
        category = 'null'

      #if category == 'text':
      if category == defaultcategorical:
        if df[column].nunique() > numbercategoryheuristic:
          category = defaultordinal

      #new statistical tests for numerical sets from v2.25
      #I don't consider mytself an expert here, these are kind of a placeholder while I conduct more research

  #     #default to 'nmbr' category instead of 'bxcx'
  #     if category == 'bxcx' and powertransform is False:
  #       category = 'nmbr'

      if category in ['nmbr', 'bxcx', defaultnumerical] \
      and powertransform is True:
        
        if df[pd.to_numeric(df[column], errors='coerce').notnull()][column].astype(float).nunique() >= 3:

          #shapiro tests for normality, we'll use a common threshold p<0.05 to reject the normality hypothesis
          stat, p = shapiro(df[pd.to_numeric(df[column], errors='coerce').notnull()][column].astype(float))
          #a typical threshold to test for normality is >0.05, let's try a lower bar for this application
          if p > 0.025:
            category = 'nmbr'
          if p <= 0.025:
            #skewness helps recognize exponential distributions, reference wikipedia
            #reference from wikipedia
    #       A normal distribution and any other symmetric distribution with finite third moment has a skewness of 0
    #       A half-normal distribution has a skewness just below 1
    #       An exponential distribution has a skewness of 2
    #       A lognormal distribution can have a skewness of any positive value, depending on its parameters
            #skewness = skew(df[column])
            skewness = skew(df[pd.to_numeric(df[column], errors='coerce').notnull()][column].astype(float))
            if skewness < 1.5:
              category = 'mnmx'
            else:
              #if powertransform is True:
              if category in ['nmbr', 'bxcx']:

                #note we'll only allow bxcx category if all values greater than a clip value
                #>0 (currently set at 0.1) since there is an asymptote for box-cox at 0
                if (df[pd.to_numeric(df[column], errors='coerce').notnull()][column].astype(float) >= 0.1).all():
                  category = 'bxcx'

                else:
                  category = 'nmbr'

              else:
                category = 'MAD3'

      del df
      
      #special cases for evlauation of labels column
      if labels is True:
        
        if category == 'nmbr':
          category = 'lbnm'
          
        if category == '1010':
          category = 'lb10'
          
        if category == 'ord3':
          category = 'lbor'
          
        if category == 'text':
          category = 'lbte'
          
        if category == 'bnry':
          category = 'lbbn'
          
        if category == 'dat6':
          category = 'lbda'
    
    return category


  def getNArows(self, df, column, category, postprocess_dict, \
                drift_dict = {}, driftassess = False):
    '''
    #NArows(df, column), function that when fed a dataframe, \
    #column id, and category label outputs a single column dataframe composed of \
    #True and False with the same number of rows as the input and the True's \
    #coresponding to those rows of the input that had missing or NaN data. This \
    #output can later be used to identify which rows for a column to infill with ML\
    # derived plug data
    
    #also accepts a dictionary to store results of a drfit assessment available
    #by passing driftassess = True
    #if drift assessment performed returns an updated dictionary withj results
    
    #by default all NArowtypes recognize np.inf as NaN
    #(option activated external to this function)
    '''
    
    NArowtype = postprocess_dict['process_dict'][category]['NArowtype']
    
    df2 = pd.DataFrame(df[column].copy())
    
    #if category == 'text':
    if NArowtype in ['justNaN']:
      
      if driftassess is True:
        drift_dict.update({column : {'unique' : df2[column].unique(), \
                                     'nunique' : df2[column].nunique(), \
                                     'nanratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})
      
      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})

#     if category == 'bnry':

#       #returns dataframe of True and False, where True coresponds to the NaN's
#       #renames column name to column + '_NArows'
#       NArows = pd.isna(df2[column])
#       NArows = pd.DataFrame(NArows)
#       NArows = NArows.rename(columns = {column:column+'_NArows'})

    #if category == 'nmbr' or category == 'bxcx':
    #if category in ['nmbr', 'bxcx', 'nbr2']:
    if NArowtype in ['numeric']:

      #convert all values to either numeric or NaN
      df2[column] = pd.to_numeric(df2[column], errors='coerce')
      
      if driftassess is True:
        
        if df2[column].notnull().nunique() > 2:
          W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
          skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        else:
          W = np.nan
          p = np.nan
          skew_stat = np.nan
          
#         W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
#         skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        
        drift_dict.update({column : {'max' : df2[column].max(), \
                                     'quantile_99' : df2[column].quantile(0.99), \
                                     'quantile_90' : df2[column].quantile(0.90), \
                                     'quantile_66' : df2[column].quantile(0.66), \
                                     'median' : df2[column].median(), \
                                     'quantile_33' : df2[column].quantile(0.33), \
                                     'quantile_10' : df2[column].quantile(0.10), \
                                     'quantile_01' : df2[column].quantile(0.01), \
                                     'min' : df2[column].min(), \
                                     'mean' : df2[column].mean(), \
                                     'std' : df2[column].std(), \
                                     'MAD' : df2[column].mad(), \
                                     'skew' : skew_stat, \
                                     'shapiro_W' : W, \
                                     'shapiro_p' : p, \
                                     'nan_ratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})

      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
      
    if NArowtype in ['integer']:

      #convert all values to either numeric or NaN
      df2[column] = pd.to_numeric(df2[column], errors='coerce')
      
      #non integers are subject to infill
      df2[column] = np.where(df2[column] == df2[column].round(), df2[column], np.nan)
      
      if driftassess is True:
        
        if df2[column].notnull().nunique() > 2:
          W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
          skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        else:
          W = np.nan
          p = np.nan
          skew_stat = np.nan
          
#         W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
#         skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        
        drift_dict.update({column : {'max' : df2[column].max(), \
                                     'quantile_99' : df2[column].quantile(0.99), \
                                     'quantile_90' : df2[column].quantile(0.90), \
                                     'quantile_66' : df2[column].quantile(0.66), \
                                     'median' : df2[column].median(), \
                                     'quantile_33' : df2[column].quantile(0.33), \
                                     'quantile_10' : df2[column].quantile(0.10), \
                                     'quantile_01' : df2[column].quantile(0.01), \
                                     'min' : df2[column].min(), \
                                     'mean' : df2[column].mean(), \
                                     'std' : df2[column].std(), \
                                     'MAD' : df2[column].mad(), \
                                     'skew' : skew_stat, \
                                     'shapiro_W' : W, \
                                     'shapiro_p' : p, \
                                     'nan_ratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})

      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
    
    if NArowtype in ['positivenumeric']:
      
      #convert all values to either numeric or NaN
      df2[column] = pd.to_numeric(df2[column], errors='coerce')
      nonpositive_ratio = df2[df2[column] <= 0].sum()[0] / df2[column].shape[0]
      
      df2.loc[df2[column] <= 0, (column)] = np.nan
      
      if driftassess is True:
        
        if df2[column].notnull().nunique() > 2:
          W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
          skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        else:
          W = np.nan
          p = np.nan
          skew_stat = np.nan
        
        drift_dict.update({column : {'max' : df2[column].max(), \
                                     'quantile_99' : df2[column].quantile(0.99), \
                                     'quantile_90' : df2[column].quantile(0.90), \
                                     'quantile_66' : df2[column].quantile(0.66), \
                                     'median' : df2[column].median(), \
                                     'quantile_33' : df2[column].quantile(0.33), \
                                     'quantile_10' : df2[column].quantile(0.10), \
                                     'quantile_01' : df2[column].quantile(0.01), \
                                     'min' : df2[column].min(), \
                                     'mean' : df2[column].mean(), \
                                     'std' : df2[column].std(), \
                                     'MAD' : df2[column].mad(), \
                                     'skew' : skew_stat, \
                                     'shapiro_W' : W, \
                                     'shapiro_p' : p, \
                                     'nonpositive_ratio' : nonpositive_ratio, \
                                     'nan_ratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})
    
      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
    if NArowtype in ['nonnegativenumeric']:
      
      #convert all values to either numeric or NaN
      df2[column] = pd.to_numeric(df2[column], errors='coerce')
      negative_ratio = df2[df2[column] < 0].sum()[0] / df2[column].shape[0]
      
      df2.loc[df2[column] < 0, (column)] = np.nan
      
      if driftassess is True:
        
        if df2[column].notnull().nunique() > 2:
          W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
          skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        else:
          W = np.nan
          p = np.nan
          skew_stat = np.nan
        
        drift_dict.update({column : {'max' : df2[column].max(), \
                                     'quantile_99' : df2[column].quantile(0.99), \
                                     'quantile_90' : df2[column].quantile(0.90), \
                                     'quantile_66' : df2[column].quantile(0.66), \
                                     'median' : df2[column].median(), \
                                     'quantile_33' : df2[column].quantile(0.33), \
                                     'quantile_10' : df2[column].quantile(0.10), \
                                     'quantile_01' : df2[column].quantile(0.01), \
                                     'min' : df2[column].min(), \
                                     'mean' : df2[column].mean(), \
                                     'std' : df2[column].std(), \
                                     'MAD' : df2[column].mad(), \
                                     'skew' : skew_stat, \
                                     'shapiro_W' : W, \
                                     'shapiro_p' : p, \
                                     'negative_ratio' : negative_ratio, \
                                     'nan_ratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})

      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
    if NArowtype in ['nonzeronumeric']:

      #convert all values to either numeric or NaN
      df2[column] = pd.to_numeric(df2[column], errors='coerce')
      zero_ratio = df2[df2[column] == 0].sum()[0] / df2[column].shape[0]
      
      df2.loc[df2[column] == 0, (column)] = np.nan
      
      if driftassess is True:
        
        if df2[column].notnull().nunique() > 2:
          W, p = shapiro(df2[df2[column].notnull()][column].astype(float))
          skew_stat = skew(df2[df2[column].notnull()][column].astype(float))
        else:
          W = np.nan
          p = np.nan
          skew_stat = np.nan
        
        drift_dict.update({column : {'max' : df2[column].max(), \
                                     'quantile_99' : df2[column].quantile(0.99), \
                                     'quantile_90' : df2[column].quantile(0.90), \
                                     'quantile_66' : df2[column].quantile(0.66), \
                                     'median' : df2[column].median(), \
                                     'quantile_33' : df2[column].quantile(0.33), \
                                     'quantile_10' : df2[column].quantile(0.10), \
                                     'quantile_01' : df2[column].quantile(0.01), \
                                     'min' : df2[column].min(), \
                                     'mean' : df2[column].mean(), \
                                     'std' : df2[column].std(), \
                                     'MAD' : df2[column].mad(), \
                                     'skew' : skew_stat, \
                                     'shapiro_W' : W, \
                                     'shapiro_p' : p, \
                                     'zero_ratio' : zero_ratio, \
                                     'nan_ratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})

      
      #returns dataframe of True and False, where True coresponds to the NaN's
      #renames column name to column + '_NArows'
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
    if NArowtype in ['parsenumeric']:
      
      NArows = self.parsenumeric(df2, column)
      
      
    if NArowtype in ['parsenumeric_commas']:
      
      NArows = self.parsenumeric_commas(df2, column)
      
    if NArowtype in ['datetime']:
      
      df2[column] = pd.to_datetime(df2[column], errors = 'coerce')

      if driftassess is True:
        drift_dict.update({column : {'nanratio' : pd.isna(df2[column]).sum() / df2[column].shape[0]}})
      
      NArows = pd.isna(df2[column])
      NArows = pd.DataFrame(NArows)
      NArows = NArows.rename(columns = {column:column+'_NArows'})
      
#       NArows = self.parsedate(df2, column)
      
    #if category in ['excl']:
    if NArowtype in ['exclude', 'boolexclude']:
      
      if driftassess is True:
        drift_dict.update({column : {}})
      
#       NArows = pd.DataFrame(np.zeros((df2.shape[0], 1)), columns=[column+'_NArows'])
      #NArows = NArows.rename(columns = {column:column+'_NArows'})
      
      df2[column] = False
      NArows = pd.DataFrame(df2[column])
      NArows = NArows.rename(columns = {column:column+'_NArows'})
#       NArows[column+'_NArows'] = False
      
    del df2
    
    if driftassess is False:
      
      return NArows
    
    else:
      
      return NArows, drift_dict
  
  
  def parsedate(self, df, column):
    """
    #support function for NArows
    #parses datetime entries and returns a column with boolean identification
    #for entries that aren't registering as datetime objects
    #wherein activations are 0 if a datetime is present and 1 if not
    """
    
    df[column] = pd.to_datetime(df[column], errors = 'coerce')

    NArows = pd.isna(df[column])
    NArows = pd.DataFrame(NArows)
    NArows = NArows.rename(columns = {column:column+'_NArows'})
    
    return NArows
  
  def is_number(self, s):
    """
    #support function for numeric parsing
    #tests if a string s is numeric
    #partly inspired by stack overflow discussion 
    #https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-is-a-number-float
    """
    try:
      s = float(s)
    except ValueError:
      return False
    if s == s and not np.isinf(s):
      return True
    else:
      #(nan will be subject to infill)
      return False
    
    
  def is_number_comma(self, s):
    """
    #support function for numeric parsing
    #tests if a string s is numeric after stripping out commas
    #partly inspired by stack overflow discussion 
    #https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-is-a-number-float
    """
    try:
      #strips out commas
      s = float(s.replace(',',''))
    except ValueError:
      return False
    if s == s and not np.isinf(s):
      return True
    else:
      #(nan will be subject to infill)
      return False
    
    
  def parsenumeric(self, df, column):
    """
    #support function for process_nmrc and NArows
    #parses string entries and returns a column with boolean identification
    #for entries that include numeric string portions
    #wherein activations are 0 if a number is present and 1 if not
    #treats numeric entries as number as well
    """
    
    #first we find overlaps from mdf_train
    
    unique_list = list(df[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):
        
                    overlap_dict.update({unique : False})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    overlap_dict.update({unique : False})
                  
              if in_dict is False:

                overlap_dict.update({unique : True})
    
    df[column] = df[column].astype(str)
    df[column] = df[column].replace(overlap_dict)
#     df[column] = df[column].astype(np.int8)
    
    NArows = pd.DataFrame(df[column].copy())
    
    NArows.columns = [column+'_NArows']
    
    
    return NArows
  
  
  def parsenumeric_commas(self, df, column):
    """
    #support function for process_nmrc and NArows
    #parses string entries and returns a column with boolean identification
    #for entries that include numeric string portions (after stripping commas)
    #wherein activations are 0 if a number is present and 1 if not
    #treats numeric entries as number as well
    """
    
    #first we find overlaps from mdf_train
    
    unique_list = list(df[column].unique())

    unique_list = list(map(str, unique_list))
    
    maxlength = max(len(x) for x in unique_list)
    
    overlap_lengths = list(range(maxlength, 0, -1))

    overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in unique_list:
        
        if unique not in overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    overlap_dict.update({unique : False})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number_comma(extract):

                    in_dict = True

                    overlap_dict.update({unique : False})
                  
              if in_dict is False:

                overlap_dict.update({unique : True})
    
    df[column] = df[column].astype(str)
    df[column] = df[column].replace(overlap_dict)
#     df[column] = df[column].astype(np.int8)
    
    NArows = pd.DataFrame(df[column].copy())
    
    NArows.columns = [column+'_NArows']
    
    
    return NArows


  def populateMLinfilldefaults(self, randomseed):
    '''
    populates a dictionary with default values for ML infill,
    currently based on Random Forest Regressor and Random Forest Classifier 
    (Each based on ScikitLearn default values)
  
    note that n_estimators set at 100 (default for version 0.22)
    '''
  
    MLinfilldefaults = {'RandomForestClassifier':{}, 'RandomForestRegressor':{}}
    
    MLinfilldefaults['RandomForestClassifier'].update({'n_estimators':100, \
                                                       'criterion':'gini', \
                                                       'max_depth':None, \
                                                       'min_samples_split':2, \
                                                       'min_samples_leaf':1, \
                                                       'min_weight_fraction_leaf':0.0, \
                                                       'max_features':'auto', \
                                                       'max_leaf_nodes':None, \
                                                       'min_impurity_decrease':0.0, \
                                                       'min_impurity_split':None, \
                                                       'bootstrap':True, \
                                                       'oob_score':False, \
                                                       'n_jobs':None, \
                                                       'random_state':randomseed, \
                                                       'verbose':0, \
                                                       'warm_start':False, \
                                                       'class_weight':None})
  
    MLinfilldefaults['RandomForestRegressor'].update({'n_estimators':100, \
                                                      'criterion':'mse', \
                                                      'max_depth':None, \
                                                      'min_samples_split':2, \
                                                      'min_samples_leaf':1, \
                                                      'min_weight_fraction_leaf':0.0, \
                                                      'max_features':'auto', \
                                                      'max_leaf_nodes':None, \
                                                      'min_impurity_decrease':0.0, \
                                                      'min_impurity_split':None, \
                                                      'bootstrap':True, \
                                                      'oob_score':False, \
                                                      'n_jobs':None, \
                                                      'random_state':randomseed, \
                                                      'verbose':0, \
                                                      'warm_start':False})

    return MLinfilldefaults


  def initRandomForestClassifier(self, ML_cmnd, MLinfilldefaults):
    '''
    function that assigns appropriate parameters based on defaults and user inputs
    and then initializes a RandomForestClassifier model
    '''
    
    #populate ML_cmnd if stuff not already present
    if 'MLinfill_cmnd' not in ML_cmnd:
      ML_cmnd.update({'MLinfill_cmnd':{}})
    if 'RandomForestClassifier' not in ML_cmnd['MLinfill_cmnd']:
      ML_cmnd['MLinfill_cmnd'].update({'RandomForestClassifier':{}})
      

    #MLinfilldefaults['RandomForestClassifier']
    if 'n_estimators' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      n_estimators = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['n_estimators']
    else:
      n_estimators = MLinfilldefaults['RandomForestClassifier']['n_estimators']

    if 'criterion' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      criterion = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['criterion']
    else:
      criterion = MLinfilldefaults['RandomForestClassifier']['criterion']

    if 'max_depth' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      max_depth = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['max_depth']
    else:
      max_depth = MLinfilldefaults['RandomForestClassifier']['max_depth']

    if 'min_samples_split' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      min_samples_split = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['min_samples_split']
    else:
      min_samples_split = MLinfilldefaults['RandomForestClassifier']['min_samples_split']

    if 'min_samples_leaf' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      min_samples_leaf = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['min_samples_leaf']
    else:
      min_samples_leaf = MLinfilldefaults['RandomForestClassifier']['min_samples_leaf']

    if 'min_weight_fraction_leaf' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      min_weight_fraction_leaf = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['min_weight_fraction_leaf']
    else:
      min_weight_fraction_leaf = MLinfilldefaults['RandomForestClassifier']['min_weight_fraction_leaf']

    if 'max_features' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      max_features = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['max_features']
    else:
      max_features = MLinfilldefaults['RandomForestClassifier']['max_features']

    if 'max_leaf_nodes' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      max_leaf_nodes = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['max_leaf_nodes']
    else:
      max_leaf_nodes = MLinfilldefaults['RandomForestClassifier']['max_leaf_nodes']

    if 'min_impurity_decrease' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      min_impurity_decrease = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['min_impurity_decrease']
    else:
      min_impurity_decrease = MLinfilldefaults['RandomForestClassifier']['min_impurity_decrease']

    if 'min_impurity_split' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      min_impurity_split = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['min_impurity_split']
    else:
      min_impurity_split = MLinfilldefaults['RandomForestClassifier']['min_impurity_split']

    if 'bootstrap' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      bootstrap = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['bootstrap']
    else:
      bootstrap = MLinfilldefaults['RandomForestClassifier']['bootstrap']

    if 'oob_score' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      oob_score = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['oob_score']
    else:
      oob_score = MLinfilldefaults['RandomForestClassifier']['oob_score']

    if 'n_jobs' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      n_jobs = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['n_jobs']
    else:
      n_jobs = MLinfilldefaults['RandomForestClassifier']['n_jobs']

    if 'random_state' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      random_state = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['random_state']
    else:
      random_state = MLinfilldefaults['RandomForestClassifier']['random_state']

    if 'verbose' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      verbose = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['verbose']
    else:
      verbose = MLinfilldefaults['RandomForestClassifier']['verbose']

    if 'warm_start' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      warm_start = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['warm_start']
    else:
      warm_start = MLinfilldefaults['RandomForestClassifier']['warm_start']

    if 'class_weight' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
      class_weight = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['class_weight']
    else:
      class_weight = MLinfilldefaults['RandomForestClassifier']['class_weight']

    #do other stuff?

    #then initialize RandomForestClassifier model
    model = RandomForestClassifier(n_estimators = n_estimators, \
                                   criterion = criterion, \
                                   max_depth = max_depth, \
                                   min_samples_split = min_samples_split, \
                                   min_samples_leaf = min_samples_leaf, \
                                   min_weight_fraction_leaf = min_weight_fraction_leaf, \
                                   max_features = max_features, \
                                   max_leaf_nodes = max_leaf_nodes, \
                                   min_impurity_decrease = min_impurity_decrease, \
                                   min_impurity_split = min_impurity_split, \
                                   bootstrap = bootstrap, \
                                   oob_score = oob_score, \
                                   n_jobs = n_jobs, \
                                   random_state = random_state, \
                                   verbose = verbose, \
                                   warm_start = warm_start, \
                                   class_weight = class_weight)

    return model


  def initRandomForestRegressor(self, ML_cmnd, MLinfilldefaults):
    '''
    function that assigns appropriate parameters based on defaults and user inputs
    and then initializes a RandomForestRegressor model
    '''
    
    #populate ML_cmnd if stuff not already present
    if 'MLinfill_cmnd' not in ML_cmnd:
      ML_cmnd.update({'MLinfill_cmnd':{}})
    if 'RandomForestRegressor' not in ML_cmnd['MLinfill_cmnd']:
      ML_cmnd['MLinfill_cmnd'].update({'RandomForestRegressor':{}})
      

    #MLinfilldefaults['RandomForestRegressor']
    if 'n_estimators' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      n_estimators = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['n_estimators']
    else:
      n_estimators = MLinfilldefaults['RandomForestRegressor']['n_estimators']

    if 'criterion' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      criterion = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['criterion']
    else:
      criterion = MLinfilldefaults['RandomForestRegressor']['criterion']

    if 'max_depth' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      max_depth = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['max_depth']
    else:
      max_depth = MLinfilldefaults['RandomForestRegressor']['max_depth']

    if 'min_samples_split' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      min_samples_split = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['min_samples_split']
    else:
      min_samples_split = MLinfilldefaults['RandomForestRegressor']['min_samples_split']

    if 'min_samples_leaf' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      min_samples_leaf = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['min_samples_leaf']
    else:
      min_samples_leaf = MLinfilldefaults['RandomForestRegressor']['min_samples_leaf']

    if 'min_weight_fraction_leaf' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      min_weight_fraction_leaf = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['min_weight_fraction_leaf']
    else:
      min_weight_fraction_leaf = MLinfilldefaults['RandomForestRegressor']['min_weight_fraction_leaf']

    if 'max_features' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      max_features = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['max_features']
    else:
      max_features = MLinfilldefaults['RandomForestRegressor']['max_features']

    if 'max_leaf_nodes' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      max_leaf_nodes = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['max_leaf_nodes']
    else:
      max_leaf_nodes = MLinfilldefaults['RandomForestRegressor']['max_leaf_nodes']

    if 'min_impurity_decrease' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      min_impurity_decrease = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['min_impurity_decrease']
    else:
      min_impurity_decrease = MLinfilldefaults['RandomForestRegressor']['min_impurity_decrease']

    if 'min_impurity_split' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      min_impurity_split = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['min_impurity_split']
    else:
      min_impurity_split = MLinfilldefaults['RandomForestRegressor']['min_impurity_split']

    if 'bootstrap' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      bootstrap = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['bootstrap']
    else:
      bootstrap = MLinfilldefaults['RandomForestRegressor']['bootstrap']

    if 'oob_score' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      oob_score = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['oob_score']
    else:
      oob_score = MLinfilldefaults['RandomForestRegressor']['oob_score']

    if 'n_jobs' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      n_jobs = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['n_jobs']
    else:
      n_jobs = MLinfilldefaults['RandomForestClassifier']['n_jobs']

    if 'random_state' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      random_state = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['random_state']
    else:
      random_state = MLinfilldefaults['RandomForestRegressor']['random_state']

    if 'verbose' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      verbose = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['verbose']
    else:
      verbose = MLinfilldefaults['RandomForestRegressor']['verbose']

    if 'warm_start' in ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']:
      warm_start = ML_cmnd['MLinfill_cmnd']['RandomForestRegressor']['warm_start']
    else:
      warm_start = MLinfilldefaults['RandomForestRegressor']['warm_start']

    #do other stuff?

    #then initialize RandomForestRegressor model 
    model = RandomForestRegressor(n_estimators = n_estimators, \
                                  criterion = criterion, \
                                  max_depth = max_depth, \
                                  min_samples_split = min_samples_split, \
                                  min_samples_leaf = min_samples_leaf, \
                                  min_weight_fraction_leaf = min_weight_fraction_leaf, \
                                  max_features = max_features, \
                                  max_leaf_nodes = max_leaf_nodes, \
                                  min_impurity_decrease = min_impurity_decrease, \
                                  min_impurity_split = min_impurity_split, \
                                  bootstrap = bootstrap, \
                                  oob_score = oob_score, \
                                  n_jobs = n_jobs, \
                                  random_state = random_state, \
                                  verbose = verbose, \
                                  warm_start = warm_start)

    return model
  
  def inspect_ML_cmnd(self, ML_cmnd, MLinfill_type, MLinfill_alg):
    """
    #Inspects ML_cmnd to determine if any of the parameters passed
    #for regressor or classifier are passed as lists instead of distinct
    #values, in which case they will be evaluated via grid search
    #or in a future extension random search or other hyperparameter tuning methods
    
    #takes as input user-passed ML_cmnd, returns tune_marker
    #where tune_marker = True indicates sets were passed, else False
    
    #MLinfill_type refers to type of predcitive algorithm applied,
    #currently only support for scikit Random Forest via 'default'
    #intent is to build in additional options
    
    #MLinfill_alg refers to the target algorithm for passed parameters
    #currently supports 'RandomForestRegressor' & 'RandomForestClassifier'
    """
    
    #initialize tune_marker to default
    tune_marker = False
    
    if MLinfill_type == 'default':
    
      if 'MLinfill_cmnd' in ML_cmnd:

        if MLinfill_alg in ML_cmnd['MLinfill_cmnd']:

          for key in ML_cmnd['MLinfill_cmnd'][MLinfill_alg]:

            #if passed parameter is a set
            if type(ML_cmnd['MLinfill_cmnd'][MLinfill_alg][key]) \
            in [type([1]), type(range(1)), type(stats.expon(1))]:

              tune_marker = True
    
            
    return tune_marker
  
  
  def assemble_param_sets(self, ML_cmnd, MLinfill_type, MLinfill_alg):
    """
    #assembles ML_cmnd passed parameters into two sets
    #for hyoeroparameter tuning operation
    
    #those parameters that were passed as sets 
    #will be saved in tune_params dictionary
    
    #those parameters that were otherwise passed
    #will be saved in static_params dictionary
    
    #returns those two dictionaries tune_params & static_params
    
    #MLinfill_type refers to type of predcitive algorithm applied,
    #currently only support for scikit Random Forest via 'default'
    #intent is to build in additional options
    
    #MLinfill_alg refers to the target algorithm for passed parameters
    #currently supports 'RandomForestRegressor' & 'RandomForestClassifier'
    """
    
    #initialize returned dictionaries
    static_params = {}
    tune_params = {}
    
    if MLinfill_type == 'default':
      
      if 'MLinfill_cmnd' in ML_cmnd:
        
        if MLinfill_alg in ML_cmnd['MLinfill_cmnd']:
          
          for key in ML_cmnd['MLinfill_cmnd'][MLinfill_alg]:
            
            #if passed parameter is a set
            if type(ML_cmnd['MLinfill_cmnd'][MLinfill_alg][key]) \
            in [type([1]), type(range(1)), type(stats.expon(1))]:
              
              #add set to tune_params which will be targeted for grid search
              tune_params.update({key : ML_cmnd['MLinfill_cmnd'][MLinfill_alg][key]})
              
            else:
              
              #else add to static_params which will overwrite defaults
              static_params.update({key : ML_cmnd['MLinfill_cmnd'][MLinfill_alg][key]})
        
        
    return static_params, tune_params


  def predictinfill(self, category, df_train_filltrain, df_train_filllabel, \
                    df_train_fillfeatures, df_test_fillfeatures, randomseed, \
                    postprocess_dict, ML_cmnd, columnslist = []):
    '''
    #predictinfill(category, df_train_filltrain, df_train_filllabel, \
    #df_train_fillfeatures, df_test_fillfeatures, randomseed, columnslist), \
    #function that takes as input \
    #a category string, the output of createMLinfillsets(.), a seed for randomness \
    #and a list of columns produced by a text class preprocessor when applicable and 
    #returns predicted infills for the train and test feature sets as df_traininfill, \
    #df_testinfill based on derivations using scikit-learn, with the lenth of \
    #infill consistent with the number of True values from NArows, and the trained \
    #model
    #a reasonable extension of this funciton would be to allow ML inference with \
    #other ML architectures such a SVM or something SGD based for instance
    '''
    
    
    #initialize defaults dictionary
    MLinfilldefaults = \
    self.populateMLinfilldefaults(randomseed)
    
    #initialize ML_cmnd
    #ML_cmnd = postprocess_dict['ML_cmnd']
    ML_cmnd = ML_cmnd
    
    #currenlty we only have support for MLinfill_type of 'default'
    #this is what tell's us to use scikit Random Forest Regressor / Classifier
    #let's make sure it's populated in ML_cmnd if set was passed as empty
    if 'MLinfill_type' not in ML_cmnd:
      ML_cmnd.update({'MLinfill_type' : 'default'})
      
    #currenlty we only have support for hyperparam_tuner of 'grid'
    #this is what tell's us to use scikit gridsearchCV
    #let's make sure it's populated in ML_cmnd if set was passed as empty
    if 'hyperparam_tuner' not in ML_cmnd:
      ML_cmnd.update({'hyperparam_tuner' : 'gridCV'})
    
    #new hyperparameter tuning support for RandomSearchCV
    #if user passes ML_cmnd['hyperparam_tuner'] = 'randomCV'
    #a user may assign number of iterations via ML_cmnd['randomCV_n_iter']
    #otherwise defaults to randomCV_n_iter = 10
    if ML_cmnd['hyperparam_tuner'] == 'randomCV':
      if 'randomCV_n_iter' not in ML_cmnd:
        ML_cmnd.update({'randomCV_n_iter' : 10})
      randomCV_n_iter = ML_cmnd['randomCV_n_iter']

    #this is a different MLinfilltype, specific to the category's process_dict entry
    #check out the assembleprocessdict fucntion definition notes for the potential values
    MLinfilltype = postprocess_dict['process_dict'][category]['MLinfilltype']
    
    
    #convert dataframes to numpy arrays
    df_train_filltrain = df_train_filltrain.values
    df_train_filllabel = df_train_filllabel.values
    df_train_fillfeatures = df_train_fillfeatures.values
    df_test_fillfeatures = df_test_fillfeatures.values


    #
    #if a numerical set
    if MLinfilltype in ['numeric', 'concurrent_nmbr']:

      if df_train_filltrain.shape[0] == 0:
        df_traininfill = np.zeros(shape=(1,len(columnslist)))
        df_testinfill = np.zeros(shape=(1,len(columnslist)))


        model = False

      else:

        #this is to address a not weird error message suggesting I reshape the y with ravel()
        df_train_filllabel = np.ravel(df_train_filllabel)

        #currently this only supports MLinfill_type of 'default'
        if 'MLinfill_type' in ML_cmnd:
          MLinfill_type = ML_cmnd['MLinfill_type']

        if 'hyperparam_tuner' in ML_cmnd:
          MLinfill_tuner = ML_cmnd['hyperparam_tuner']

        if MLinfill_type == 'default':
          #'numeric' MLinfilltype uses regressor
          MLinfill_alg = 'RandomForestRegressor'
        else:
          #future extension for other options
          MLinfill_alg = 'RandomForestRegressor'



        #tune marker tells us if user passed some parameters as a list for hyperparameter tuning
        tune_marker = self.inspect_ML_cmnd(ML_cmnd, MLinfill_type, MLinfill_alg)

        if tune_marker is True:

          #static_params are user passed parameters that won't be tuned, 
          #tune_params are user passed params (passed as list or range) that will be tuned
          static_params, tune_params = self.assemble_param_sets(ML_cmnd, MLinfill_type, MLinfill_alg)

          #we'll create a temp ML_cmnd to initialize a tuning model
          temp_ML_cmnd = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          #then we'll initialize a tuning model
          #note that this populates the parameters to be tuned with defaults
          #my understanding is that scikit gridsearch still allows tuning for parameters
          #that were previously initialized in the model
          if MLinfill_alg == 'RandomForestRegressor':
            tuning_model = self.initRandomForestRegressor(temp_ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            print("error: MLinfill_type currently only supports 'default'.")
            tuning_model = False

          #for now we'll default to grid scoring of ‘neg_mean_squared_error’
          #I'm not positive this is the best default choice for regressor, 
          #seem to remember seeing this used somewher
          #worth some further research
          grid_scoring = 'neg_mean_squared_error'

          #now we'll initialize a grid search
          if MLinfill_tuner == 'gridCV':
            tune_search = GridSearchCV(tuning_model, cv=5, iid='deprecated', \
                                       param_grid = tune_params, scoring = grid_scoring)
          elif MLinfill_tuner == 'randomCV':
            tune_search = RandomizedSearchCV(tuning_model, cv=5, iid='deprecated', \
                                         param_distributions = tune_params, scoring = grid_scoring, \
                                         n_iter = randomCV_n_iter)
          else:
            print("error: hyperparam_tuner currently only supports 'gridCV' or 'randomCV'.")


          #now we'll run a fit on the grid search
          #for now won't pass any fit parameters
          fit_params = {}
          tune_search.fit(df_train_filltrain, df_train_filllabel, **fit_params)

          #acess the tuned parameters based on the tuning operation
          tuned_params = tune_search.best_params_

          if postprocess_dict['printstatus'] is True:

            #print("")
            print("tuned parameters:")
            print(tuned_params)
            print("")

          #now assemble final static params by incorporating the tuned params
          static_params.update(tuned_params)

          #now initialize our tuned model
          #first create another temp_ML_cmnd for the tuned set
          temp_ML_cmnd_two = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          model = self.initRandomForestRegressor(temp_ML_cmnd_two, MLinfilldefaults)


        else:

          #train linear regression model using scikit-learn for numerical prediction
          #model = LinearRegression()
          #model = PassiveAggressiveRegressor(random_state = randomseed)
          #model = Ridge(random_state = randomseed)
          #model = RidgeCV()
          #note that SVR doesn't have an argument for random_state
          #model = SVR()
          #model = RandomForestRegressor(n_estimators=100, random_state = randomseed, verbose=0)
          model = self.initRandomForestRegressor(ML_cmnd, MLinfilldefaults)


        model.fit(df_train_filltrain, df_train_filllabel)    


#           #predict infill values
#           df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any train rows needing infill
        if df_train_fillfeatures.shape[0] > 0:
          df_traininfill = model.predict(df_train_fillfeatures)
        else:
          df_traininfill = np.array([0])

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          df_testinfill = np.array([0])

      #convert infill values to dataframe
      df_traininfill = pd.DataFrame(df_traininfill, columns = ['infill'])
      df_testinfill = pd.DataFrame(df_testinfill, columns = ['infill'])


    #if category == 'bnry':
    if MLinfilltype in ['singlct', 'binary', 'concurrent_act']:

      if df_train_filltrain.shape[0] == 0:
        df_traininfill = np.zeros(shape=(1,len(columnslist)))
        df_testinfill = np.zeros(shape=(1,len(columnslist)))


        model = False

      else:

        #this is to address a not weird error message suggesting I reshape the y with ravel()
        df_train_filllabel = np.ravel(df_train_filllabel)

        #currently this only supports MLinfill_type of 'default'
        if 'MLinfill_type' in ML_cmnd:
          MLinfill_type = ML_cmnd['MLinfill_type']

        if 'hyperparam_tuner' in ML_cmnd:
          MLinfill_tuner = ML_cmnd['hyperparam_tuner']

        if MLinfill_type == 'default':
          #'numeric' MLinfilltype uses regressor
          MLinfill_alg = 'RandomForestClassifier'
        else:
          #future extension for other options
          MLinfill_alg = 'RandomForestClassifier'

        #tune marker tells us if user passed some parameters as a list for hyperparameter tuning
        tune_marker = self.inspect_ML_cmnd(ML_cmnd, MLinfill_type, MLinfill_alg)

        if tune_marker is True:

          #static_params are user passed parameters that won't be tuned, 
          #tune_params are user passed params (passed as list or range) that will be tuned
          static_params, tune_params = self.assemble_param_sets(ML_cmnd, MLinfill_type, MLinfill_alg)

          #we'll create a temp ML_cmnd to initialize a tuning model
          temp_ML_cmnd = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          #then we'll initialize a tuning model
          #note that this populates the parameters to be tuned with defaults
          #my understanding is that scikit gridsearch still allows tuning for parameters
          #that were previously initialized in the model
          if MLinfill_alg == 'RandomForestClassifier':
            tuning_model = self.initRandomForestClassifier(temp_ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            tuning_model = False

          #for now we'll default to grid scoring of ‘roc_auc’
          #I'm not positive this is the best default choice for classifier, 
          #seem to remember seeing this used somewher
          #worth some further research
          #grid_scoring = 'roc_auc'
          #on second thought I think accuracy more generalizable for edge cases
          grid_scoring = 'accuracy'

          #now we'll initialize a grid search
          if MLinfill_tuner == 'gridCV':
            tune_search = GridSearchCV(tuning_model, cv=5, iid='deprecated', \
                                       param_grid = tune_params, scoring = grid_scoring)
          elif MLinfill_tuner == 'randomCV':
            tune_search = RandomizedSearchCV(tuning_model, cv=5, iid='deprecated', \
                                         param_distributions = tune_params, scoring = grid_scoring, \
                                         n_iter = randomCV_n_iter)
          else:
            print("error: hyperparam_tuner currently only supports 'gridCV' or 'randomCV'.")

          #now we'll run a fit on the grid search
          #for now won't pass any fit parameters
          fit_params = {}
          tune_search.fit(df_train_filltrain, df_train_filllabel, **fit_params)

          #acess the tuned parameters based on the tuning operation
          tuned_params = tune_search.best_params_

          if postprocess_dict['printstatus'] is True:

            #print("")
            print("tuned parameters:")
            print(tuned_params)
            print("")

          #now assemble final static params by incorporating the tuned params
          static_params.update(tuned_params)

          #now initialize our tuned model
          #first create another temp_ML_cmnd for the tuned set
          temp_ML_cmnd_two = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(temp_ML_cmnd_two, MLinfilldefaults)
          else:
            #future extension
            model = False

        else:

          #train logistic regression model using scikit-learn for binary classifier
          #model = LogisticRegression()
          #model = LogisticRegression(random_state = randomseed)
          #model = SGDClassifier(random_state = randomseed)
          #model = SVC(random_state = randomseed)
          #model = RandomForestClassifier(n_estimators=100, random_state = randomseed, verbose=0)

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            model = False


        model.fit(df_train_filltrain, df_train_filllabel)

#           #predict infill values
#           df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any train rows needing infill
        if df_train_fillfeatures.shape[0] > 0:
          df_traininfill = model.predict(df_train_fillfeatures)
        else:
          df_traininfill = np.array([0])

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          df_testinfill = np.array([0])

      #convert infill values to dataframe
      df_traininfill = pd.DataFrame(df_traininfill, columns = ['infill'])
      df_testinfill = pd.DataFrame(df_testinfill, columns = ['infill'])

#       print('category is bnry, df_traininfill is')
#       print(df_traininfill)


    #if category in ['text', 'bins', 'bint']:
    if MLinfilltype in ['multirt', 'multisp']:

      if df_train_filltrain.shape[0] == 0:
        df_traininfill = np.zeros(shape=(1,len(columnslist)))
        df_testinfill = np.zeros(shape=(1,len(columnslist)))

        model = False

      else:

#         #FUTURE EXTENSION - Label Smoothing for ML infill
#         if ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing'] >0.0 and
#         ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing'] < 1.0 and
#         str(ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']) != 'False':

#           epsilon = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']

#           #apply_LabelSmoothing_numpy will be comparable to apply_LabelSmoothing
#           #but intended for numpy arrays, making use of fact that #columns = len(categorylist) for MLinfill
#           df_traininfill = self.apply_LabelSmoothing_numpy(df_traininfill, epsilon)


        #muiltirt sets as edge case may sometimes be returned with one column
        if df_train_filllabel.shape[1] == 1:
          df_train_filllabel = np.ravel(df_train_filllabel)

        #currently this only supports MLinfill_type of 'default'
        if 'MLinfill_type' in ML_cmnd:
          MLinfill_type = ML_cmnd['MLinfill_type']

        if 'hyperparam_tuner' in ML_cmnd:
          MLinfill_tuner = ML_cmnd['hyperparam_tuner']

        if MLinfill_type == 'default':
          #'numeric' MLinfilltype uses regressor
          MLinfill_alg = 'RandomForestClassifier'
        else:
          #future extension for other options
          MLinfill_alg = 'RandomForestClassifier'


        #tune marker tells us if user passed some parameters as a list for hyperparameter tuning
        tune_marker = self.inspect_ML_cmnd(ML_cmnd, MLinfill_type, MLinfill_alg)

        if tune_marker is True:

          #static_params are user passed parameters that won't be tuned, 
          #tune_params are user passed params (passed as list or range) that will be tuned
          static_params, tune_params = self.assemble_param_sets(ML_cmnd, MLinfill_type, MLinfill_alg)

          #we'll create a temp ML_cmnd to initialize a tuning model
          temp_ML_cmnd = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          #then we'll initialize a tuning model
          #note that this populates the parameters to be tuned with defaults
          #my understanding is that scikit gridsearch still allows tuning for parameters
          #that were previously initialized in the model
          if MLinfill_alg == 'RandomForestClassifier':
            tuning_model = self.initRandomForestClassifier(temp_ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            tuning_model = False


          #for now we'll default to grid scoring of ‘roc_auc’
          #I'm not positive this is the best default choice for classifier, 
          #seem to remember seeing this used somewher
          #worth some further research
          #grid_scoring = 'roc_auc'
          #on second thought I think accuracy more generalizable for edge cases
          grid_scoring = 'accuracy'


          #now we'll initialize a grid search
          if MLinfill_tuner == 'gridCV':
            tune_search = GridSearchCV(tuning_model, cv=5, iid='deprecated', \
                                       param_grid = tune_params, scoring = grid_scoring)
          elif MLinfill_tuner == 'randomCV':
            tune_search = RandomizedSearchCV(tuning_model, cv=5, iid='deprecated', \
                                         param_distributions = tune_params, scoring = grid_scoring, \
                                         n_iter = randomCV_n_iter)
          else:
            print("error: hyperparam_tuner currently only supports 'gridCV' or 'randomCV'.")


          #now we'll run a fit on the grid search
          #for now won't pass any fit parameters
          fit_params = {}
          tune_search.fit(df_train_filltrain, df_train_filllabel, **fit_params)

          #acess the tuned parameters based on the tuning operation
          tuned_params = tune_search.best_params_

          if postprocess_dict['printstatus'] is True:

            #print("")
            print("tuned parameters:")
            print(tuned_params)
            print("")

          #now assemble final static params by incorporating the tuned params
          static_params.update(tuned_params)

          #now initialize our tuned model
          #first create another temp_ML_cmnd for the tuned set
          temp_ML_cmnd_two = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(temp_ML_cmnd_two, MLinfilldefaults)
          else:
            #future extension
            model = False

        else:

          #train logistic regression model using scikit-learn for binary classifier
          #model = LogisticRegression()
          #model = LogisticRegression(random_state = randomseed)
          #model = SGDClassifier(random_state = randomseed)
          #model = SVC(random_state = randomseed)
          #model = RandomForestClassifier(n_estimators=100, random_state = randomseed, verbose=0)

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            model = False


        model.fit(df_train_filltrain, df_train_filllabel)

#           #predict infill values
#           df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any train rows needing infill
        if df_train_fillfeatures.shape[0] > 0:
          df_traininfill = model.predict(df_train_fillfeatures)
        else:
          #this needs to have same number of columns as text category
          df_traininfill = np.zeros(shape=(1,len(columnslist)))

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          #this needs to have same number of columns as text category
          df_testinfill = np.zeros(shape=(1,len(columnslist)))


      #convert infill values to dataframe
#         if len(columnslist) > 1:
      df_traininfill = pd.DataFrame(df_traininfill, columns = columnslist)
      df_testinfill = pd.DataFrame(df_testinfill, columns = columnslist)



    if MLinfilltype in ['1010']:

      if df_train_filltrain.shape[0] == 0:

        df_traininfill = np.zeros(shape=(1,len(columnslist)))
        df_testinfill = np.zeros(shape=(1,len(columnslist)))

        model = False

      else:

        #convert from binary to one-hot encoding
        df_train_filllabel = \
        self.convert_1010_to_onehot(df_train_filllabel)

#         #FUTURE EXTENSION - Label Smoothing for ML infill
#         if ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing'] >0.0 and
#         ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing'] < 1.0 and
#         str(ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']) != 'False':

#           epsilon = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']

#           #apply_LabelSmoothing_numpy will be comparable to apply_LabelSmoothing
#           #but intended for numpy arrays, making use of fact that #columns = len(categorylist) for MLinfill
#           df_traininfill = self.apply_LabelSmoothing_numpy(df_traininfill, epsilon)


        #muiltirt sets as edge case may sometimes be returned with one column
        if df_train_filllabel.shape[1] == 1:
          df_train_filllabel = np.ravel(df_train_filllabel)

        #currently this only supports MLinfill_type of 'default'
        if 'MLinfill_type' in ML_cmnd:
          MLinfill_type = ML_cmnd['MLinfill_type']

        if 'hyperparam_tuner' in ML_cmnd:
          MLinfill_tuner = ML_cmnd['hyperparam_tuner']

        if MLinfill_type == 'default':
          #'numeric' MLinfilltype uses regressor
          MLinfill_alg = 'RandomForestClassifier'
        else:
          #future extension for other options
          MLinfill_alg = 'RandomForestClassifier'


        #tune marker tells us if user passed some parameters as a list for hyperparameter tuning
        tune_marker = self.inspect_ML_cmnd(ML_cmnd, MLinfill_type, MLinfill_alg)

        if tune_marker is True:

          #static_params are user passed parameters that won't be tuned, 
          #tune_params are user passed params (passed as list or range) that will be tuned
          static_params, tune_params = self.assemble_param_sets(ML_cmnd, MLinfill_type, MLinfill_alg)

          #we'll create a temp ML_cmnd to initialize a tuning model
          temp_ML_cmnd = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          #then we'll initialize a tuning model
          #note that this populates the parameters to be tuned with defaults
          #my understanding is that scikit gridsearch still allows tuning for parameters
          #that were previously initialized in the model
          if MLinfill_alg == 'RandomForestClassifier':
            tuning_model = self.initRandomForestClassifier(temp_ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            tuning_model = False


          #for now we'll default to grid scoring of ‘roc_auc’
          #I'm not positive this is the best default choice for classifier, 
          #seem to remember seeing this used somewher
          #worth some further research
          #grid_scoring = 'roc_auc'
          #on second thought I think accuracy more generalizable for edge cases
          grid_scoring = 'accuracy'


          #now we'll initialize a grid search
          if MLinfill_tuner == 'gridCV':
            tune_search = GridSearchCV(tuning_model, cv=5, iid='deprecated', \
                                       param_grid = tune_params, scoring = grid_scoring)
          elif MLinfill_tuner == 'randomCV':
            tune_search = RandomizedSearchCV(tuning_model, cv=5, iid='deprecated', \
                                         param_distributions = tune_params, scoring = grid_scoring, \
                                         n_iter = randomCV_n_iter)
          else:
            print("error: hyperparam_tuner currently only supports 'gridCV' or 'randomCV'.")

          #now we'll run a fit on the grid search
          #for now won't pass any fit parameters
          fit_params = {}
          tune_search.fit(df_train_filltrain, df_train_filllabel, **fit_params)

          #acess the tuned parameters based on the tuning operation
          tuned_params = tune_search.best_params_

          if postprocess_dict['printstatus'] is True:

            #print("")
            print("tuned parameters:")
            print(tuned_params)
            print("")

          #now assemble final static params by incorporating the tuned params
          static_params.update(tuned_params)

          #now initialize our tuned model
          #first create another temp_ML_cmnd for the tuned set
          temp_ML_cmnd_two = {'MLinfill_cmnd':{MLinfill_alg : static_params}}

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(temp_ML_cmnd_two, MLinfilldefaults)
          else:
            #future extension
            model = False

        else:

          #train logistic regression model using scikit-learn for binary classifier
          #model = LogisticRegression()
          #model = LogisticRegression(random_state = randomseed)
          #model = SGDClassifier(random_state = randomseed)
          #model = SVC(random_state = randomseed)
          #model = RandomForestClassifier(n_estimators=100, random_state = randomseed, verbose=0)

          if MLinfill_alg == 'RandomForestClassifier':
            model = self.initRandomForestClassifier(ML_cmnd, MLinfilldefaults)
          else:
            #future extension
            model = False


        model.fit(df_train_filltrain, df_train_filllabel)

#           #predict infill values
#           df_traininfill = model.predict(df_train_fillfeatures)

#           #convert from one-hot to binary encoding
#           df_traininfill = \
#           self.convert_onehot_to_1010(df_traininfill)

        #only run following if we have any train rows needing infill
        if df_train_fillfeatures.shape[0] > 0:
          df_traininfill = model.predict(df_train_fillfeatures)

          df_traininfill = \
          self.convert_onehot_to_1010(df_traininfill)

        else:
          #this needs to have same number of columns as text category
          df_traininfill = np.zeros(shape=(1,len(columnslist)))


        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)

          df_testinfill = \
          self.convert_onehot_to_1010(df_testinfill)

        else:
          #this needs to have same number of columns as text category
          df_testinfill = np.zeros(shape=(1,len(columnslist)))


      #convert infill values to dataframe
      df_traininfill = pd.DataFrame(df_traininfill, columns = columnslist)
      df_testinfill = pd.DataFrame(df_testinfill, columns = columnslist)



    #if category in ['date', 'NArw', 'null']:
    if MLinfilltype in ['exclude', 'boolexclude']:

      #create empty sets for now
      #an extension of this method would be to implement a comparable infill \
      #method for the time category, based on the columns output from the \
      #preprocessing
      df_traininfill = pd.DataFrame({'infill' : [0]}) 
      df_testinfill = pd.DataFrame({'infill' : [0]}) 

      model = False
    
    
    
    return df_traininfill, df_testinfill, model


  def createMLinfillsets(self, df_train, df_test, column, trainNArows, testNArows, \
                         category, randomseed, postprocess_dict, columnslist = [], \
                         categorylist = []):
    '''
    #update createMLinfillsets as follows:
    #instead of diferientiation by category, do a test for whether categorylist = []
    #if so do a single column transform excluding those other columns from columnslist
    #in the sets comparable to , otherwise do a transform comparable to text category
    #createMLinfillsets(df_train, df_test, column, trainNArows, testNArows, \
    #category, columnslist = []) function that when fed dataframes of train and\
    #test sets, column id, df of True/False corresponding to rows from original \
    #sets with missing values, a string category of 'text', 'date', 'nmbr', or \
    #'bnry', and a list of column id's for the text category if applicable. The \
    #function returns a seris of dataframes which can be applied to training a \
    #machine learning model to predict apppropriate infill values for those points \
    #that had missing values from the original sets, indlucing returns of \
    #df_train_filltrain, df_train_filllabel, df_train_fillfeatures, \
    #and df_test_fillfeatures
    '''
    
    MLinfilltype = postprocess_dict['process_dict'][category]['MLinfilltype']
    
    #create 3 new dataframes for each train column - the train and labels \
    #for rows not needing infill, and the features for rows needing infill \
    #also create a test features column 
    
    #categories are nmbr, bnry, text, date, bxcx, bins, bint, NArw, null
    #if category in ['nmbr', 'bxcx', 'bnry', 'text', 'bins', 'bint']:
    
    #if category in ['nmbr', 'nbr2', 'bxcx', 'bnry', 'text', 'bins', 'bint']:
    if MLinfilltype in ['numeric', 'singlct', 'binary', \
                        'multirt', 'multisp', '1010', \
                        'concurrent_act', 'concurrent_nmbr']:

      #if this is a single column set or concurrent_act
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][category]['MLinfilltype'] in ['concurrent_act', 'concurrent_nmbr']:

        #first concatinate the NArows True/False designations to df_train & df_test
        df_train = pd.concat([df_train, trainNArows], axis=1)
        df_test = pd.concat([df_test, testNArows], axis=1)

        #create copy of df_train to serve as training set for fill
        df_train_filltrain = df_train.copy()
        #now delete rows coresponding to True
        df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns[0]] == False]

        #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df
        df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)
        df_train_filltrain = df_train_filltrain.drop([trainNArows.columns[0]], axis=1)



        #create a copy of df_train[column] for fill train labels
        df_train_filllabel = pd.DataFrame(df_train[column].copy())
        #concatinate with the NArows
        df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)
        #drop rows corresponding to True
        df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns[0]] == False]

        #delete the NArows column
        df_train_filllabel = df_train_filllabel.drop([trainNArows.columns[0]], axis=1)

        #create features df_train for rows needing infill
        #create copy of df_train (note it already has NArows included)
        df_train_fillfeatures = df_train.copy()
        #delete rows coresponding to False
        df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns[0]])]
        #delete columnslist and column+'_NArows'
        df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)
        df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns[0]], axis=1)


        #create features df_test for rows needing infill
        #create copy of df_test (note it already has NArows included)
        df_test_fillfeatures = df_test.copy()
        #delete rows coresponding to False
        df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns[0]])]
        #delete column and column+'_NArows'
        df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)
        df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns[0]], axis=1)

        #delete NArows from df_train, df_test
        df_train = df_train.drop([trainNArows.columns[0]], axis=1)
        df_test = df_test.drop([testNArows.columns[0]], axis=1)





      #else if categorylist wasn't single entry
      else:

        #create a list of columns representing columnslist exlucding elements from
        #categorylist
        noncategorylist = columnslist[:]
        #this removes categorylist elements from noncategorylist
        noncategorylist = list(set(noncategorylist).difference(set(categorylist)))


        #first concatinate the NArows True/False designations to df_train & df_test
        df_train = pd.concat([df_train, trainNArows], axis=1)
        df_test = pd.concat([df_test, testNArows], axis=1)

        #create copy of df_train to serve as training set for fill
        df_train_filltrain = df_train.copy()
        #now delete rows coresponding to True
        df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns[0]] == False]

        #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df
        df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)
        df_train_filltrain = df_train_filltrain.drop([trainNArows.columns[0]], axis=1)


        #create a copy of df_train[categorylist] for fill train labels
        df_train_filllabel = df_train[categorylist].copy()
        #concatinate with the NArows
        df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)
        #drop rows corresponding to True
        df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns[0]] == False]



        #delete the NArows column
        df_train_filllabel = df_train_filllabel.drop([trainNArows.columns[0]], axis=1)


        #create features df_train for rows needing infill
        #create copy of df_train (note it already has NArows included)
        df_train_fillfeatures = df_train.copy()
        #delete rows coresponding to False
        df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns[0]])]
        #delete columnslist and column+'_NArows'
        df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)
        df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns[0]], axis=1)


        #create features df_test for rows needing infill
        #create copy of df_test (note it already has NArows included)
        df_test_fillfeatures = df_test.copy()
        #delete rows coresponding to False
        df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns[0]])]
        #delete column and column+'_NArows'
        df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)
        df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns[0]], axis=1)

        #delete NArows from df_train, df_test
        df_train = df_train.drop([trainNArows.columns[0]], axis=1)
        df_test = df_test.drop([testNArows.columns[0]], axis=1)


    #if MLinfilltype in ['exclude']:
    else:

      #create empty sets for now
      #an extension of this method would be to implement a comparable method \
      #for the time category, based on the columns output from the preprocessing
      df_train_filltrain = pd.DataFrame({'foo' : []}) 
      df_train_filllabel = pd.DataFrame({'foo' : []})
      df_train_fillfeatures = pd.DataFrame({'foo' : []})
      df_test_fillfeatures = pd.DataFrame({'foo' : []})
    
    
    return df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures




  def insertinfill(self, df, column, infill, category, NArows, postprocess_dict, \
                   columnslist = [], categorylist = [], singlecolumncase = False):
    '''
    #insertinfill(df, column, infill, category, NArows, columnslist = [])
    #function that takes as input a dataframe, column id, category string of either\
    #'nmbr'/'text'/'bnry'/'date', a df column of True/False identifiying row id of\
    #rows that will recieve infill, and and a list of columns produced by a text \
    #class preprocessor when applicable. Replaces the column cells in rows \
    #coresponding to the NArows True values with the values from infill, returns\
    #the associated transformed dataframe.
    #singlecolumn case is for special case (used in adjinfill) when we want to 
    #override the categorylist >1 methods
    '''
    
    
    MLinfilltype = postprocess_dict['process_dict'][category]['MLinfilltype']
    
    #NArows column name uses original column name + _NArows as key
    #by convention, current column has original column name + '_ctgy' at end
    #so we'll drop final 5 characters from column string
    #origcolumnname = column[:-5]
    NArowcolumn = NArows.columns[0]

    #if category in ['nmbr', 'nbr2', 'bxcx', 'bnry', 'text']:
    if MLinfilltype in ['numeric', 'singlct', 'binary', \
                        'multisp', 'multirt', '1010', \
                        'concurrent_act', 'concurrent_nmbr']:

      #if this is a single column set (not categorical)
      if len(categorylist) == 1 or singlecolumncase is True \
      or MLinfilltype in ['concurrent_act', 'concurrent_nmbr']:

        #create new dataframe for infills wherein the infill values are placed in \
        #rows coresponding to NArows True values and rows coresponding to NArows \
        #False values are filled with a 0    

        #assign index values to a column
        df['tempindex1'] = df.index

        #concatinate our df with NArows
        df = pd.concat([df, NArows], axis=1)

        #create list of index numbers coresponding to the NArows True values
        infillindex = df.loc[df[NArowcolumn]]['tempindex1']

        #create a dictionary for use to insert infill using df's index as the key
        infill_dict = dict(zip(infillindex, infill.values))
        #infill_dict = dict(zip(infillindex, infill['infill']))

        #replace 'tempindex1' column with infill in rows where NArows is True
        #df['tempindex1'] = np.where(df[NArowcolumn], df['tempindex1'].replace(infill_dict), 'fill')
        df['tempindex1'] = np.where(df[NArowcolumn], df['tempindex1'].replace(infill_dict), 0)

        #now carry that infill over to the target column for rows where NArows is True
        df[column] = np.where(df[NArowcolumn], df['tempindex1'], df[column])

        #remove the temporary columns from df
        df = df.drop(['tempindex1'], axis=1)
        df = df.drop([NArowcolumn], axis=1)




      #else if categorylist wasn't single value
      else:

        #create new dataframe for infills wherein the infill values are placed in \
        #rows coresponding to NArows True values and rows coresponding to NArows \
        #False values are filled with a 0

        #text infill contains multiple columns for each predicted calssification
        #which were derived from one-hot encoding the original column in preprocessing
        for textcolumnname in categorylist:
          

          #create newcolumn which will serve as the NArows specific to textcolumnname
          df['textNArows'] = NArows

          df['textNArows'] = df['textNArows'].replace(0, False)
          df['textNArows'] = df['textNArows'].replace(1, True)

          #assign index values to a column
          df['tempindex1'] = df.index

          #create list of index numbers coresponding to the NArows True values
          textinfillindex = pd.DataFrame(df.loc[df['textNArows']]['tempindex1'])
          
          
          #reset the index
          textinfillindex = textinfillindex.reset_index()
          

          #now before we create our infill dicitonaries, we're going to need to
          #create a seperate textinfillindex for each category

          infill['tempindex1'] = textinfillindex['tempindex1']
          
          
          #if we didn't have infill we created a plug infill set with column name 'infill'
          if 'infill' not in list(infill):
            
            #first let's create a copy of this textcolumn's infill column replacing 
            #0/1 with True False (this works because we are one hot encoding)
            infill[textcolumnname + '_bool'] = infill[textcolumnname].astype('bool')

            #we'll use the mask feature to create infillindex which only contains \
            #rows coresponding to the True value in the column we just created

            mask = (infill[textcolumnname + '_bool']==True)
            infillindex = infill[mask]['tempindex1']



            #we're only going to insert the infill to column textcolumnname if we \
            #have infill to insert

            if len(infillindex.values) > 0:

              df.loc[infillindex.values[0], textcolumnname] = 1


          #now we'll delete temporary support columns associated with textcolumnname
          
          #for some reason these infill drops are returning performance warnings
          #but since this function doesn't even return infill I'm just going to leave out
#           infill = infill.drop([textcolumnname + '_bool'], axis=1)
#           infill = infill.drop(['tempindex1'], axis=1)
          
          df = df.drop(['textNArows'], axis=1)
          df = df.drop(['tempindex1'], axis=1)


    #if category == 'date':
    if MLinfilltype in ['exclude', 'boolexclude']:
      #this spot reserved for future update to incorporate address of datetime\
      #category data
      df = df


    return df



  def MLinfillfunction (self, df_train, df_test, column, postprocess_dict, \
                        masterNArows_train, masterNArows_test, randomseed, \
                        ML_cmnd):
    '''
    #new function ML infill, generalizes the MLinfill application between categories
    #def MLinfill (df_train, df_test, column, postprocess_dict, \
    #masterNArows_train, masterNArows_test, randomseed)
    #function that applies series of functions of createMLinfillsets, 
    #predictinfill, and insertinfill to a categorical encoded set.
    #for the record I'm sure that the conversion of the single column
    #series to a dataframe is counter to the intent of pandas
    #it's probably less memory efficient but it's the current basis of
    #the functions so we're going to maintain that approach for now
    #the revision of these functions to accept pandas series is a
    #possible future extension
    '''
    
    
    if postprocess_dict['column_dict'][column]['infillcomplete'] is False:

      columnslist = postprocess_dict['column_dict'][column]['columnslist']
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      category = postprocess_dict['column_dict'][column]['category']
      
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        #copy the datatype to ensure returned set is consistent
        df_temp_dtype = pd.DataFrame(df_train[column][:0]).copy()

      elif len(categorylist) > 1:
        #copy the datatype to ensure returned set is consistent
        df_temp_dtype = pd.DataFrame(df_train[categorylist][:0]).copy()
      

      #createMLinfillsets
      df_train_filltrain, df_train_filllabel, df_train_fillfeatures, df_test_fillfeatures = \
      self.createMLinfillsets(df_train, \
                         df_test, column, \
                         pd.DataFrame(masterNArows_train[origcolumn+'_NArows']), \
                         pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \
                         category, randomseed, postprocess_dict, \
                         columnslist = columnslist, \
                         categorylist = categorylist)

      #predict infill values using defined function predictinfill(.)
      df_traininfill, df_testinfill, model = \
      self.predictinfill(category, df_train_filltrain, df_train_filllabel, \
                    df_train_fillfeatures, df_test_fillfeatures, \
                    randomseed, postprocess_dict, ML_cmnd, columnslist = categorylist)

      #now we'll add our trained model to the postprocess_dict
      postprocess_dict['column_dict'][column]['infillmodel'] \
      = model

      #troubleshooting note: it occurs to me that we're only saving our
      #trained model in the postprocess_dict for one of the text columns
      #not all, however since this will be the first column to be 
      #addressed here and also in the postmunge function (they're in 
      #same order) my expectation is that this will not be an issue and \
      #accidental bonus since we're only saving once results in reduced
      #file size
      
      #only insert infill if we have a valid model
      if model is not False:

        #apply the function insertinfill(.) to insert missing value predicitons \
        #to df's associated column
        df_train = self.insertinfill(df_train, column, df_traininfill, category, \
                              pd.DataFrame(masterNArows_train[origcolumn+'_NArows']), \
                              postprocess_dict, columnslist = columnslist, \
                              categorylist = categorylist)

        #if we don't train the train set model on any features, that we won't be able 
        #to apply the model to predict the test set infill. 

        if any(x == True for x in masterNArows_train[origcolumn+'_NArows']):

          df_test = self.insertinfill(df_test, column, df_testinfill, category, \
                             pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \
                             postprocess_dict, columnslist = columnslist, \
                             categorylist = categorylist)

      #now change the infillcomplete marker in the text_dict for each \
      #associated text column unless in concurrent_activations MLinfilltype
      if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        
        postprocess_dict['column_dict'][column]['infillcomplete'] = True

        #now we'll add our trained text model to the postprocess_dict
        postprocess_dict['column_dict'][column]['infillmodel'] \
        = model
        
      else:
        
        for columnname in categorylist:
          postprocess_dict['column_dict'][columnname]['infillcomplete'] = True

          #now we'll add our trained text model to the postprocess_dict
          postprocess_dict['column_dict'][columnname]['infillmodel'] \
          = model

#         #now change the infillcomplete marker in the dict for each associated column
#         for columnname in categorylist:
#           postprocess_dict['column_dict'][columnname]['infillcomplete'] = True

      #reset data type to ensure returned data is consistent with what was passed
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        df_train[column] = \
        df_train[column].astype({column:df_temp_dtype[column].dtypes})
        
        df_test[column] = \
        df_test[column].astype({column:df_temp_dtype[column].dtypes})

      elif len(categorylist) > 1:
        for dtype_column in categorylist:
          df_train[dtype_column] = \
          df_train[dtype_column].astype({dtype_column:df_temp_dtype[dtype_column].dtypes})
          
          df_test[dtype_column] = \
          df_test[dtype_column].astype({dtype_column:df_temp_dtype[dtype_column].dtypes})

    return df_train, df_test, postprocess_dict


  def convert_1010_to_onehot(self, np_1010):  
    """
    takes as input numpy array encoded in 1010 format
    and translates to a one-hot encoding equivalent
    with number of columns based on 2^n where n is number of 1010 columns
    and potentially with columns with all 0
    """

    #create a dataframe because my numpy sauce is weak
    df_array = pd.DataFrame(np_1010)

    #initialize a column to store encodings
    df_array['onehot'] = ''

    #populate column to store encodings 
    for column in list(df_array):
      if column != 'onehot':
        df_array['onehot'] = \
        df_array['onehot'] + df_array[column].astype(int).astype(str)

    #discard other columns
    df_array = pd.DataFrame(df_array['onehot'])

    #create list of columns for the encoding with binary encodings
    #this will be full list of range of values based on number of 1010 columns
    textcolumns = list(range(2**np_1010.shape[1]))
    textcolumns = ['onehot_' + str(format(item, f"0{np_1010.shape[1]}b")) for item in textcolumns]

    #we'll make use of the postprocess_textsupportclass function
    #which requires some qadmittedly kind of hacky prepopulation of a temp ppd
    temp_ppd = {'column_dict' : {'columnkey' : {'categorylist' : textcolumns}}}

  #   df_onehot = \
  #   postprocess_textsupport_class(df_array, 'onehot', temp_ppd, 'columnkey')
    df_onehot = \
    self.postprocess_textsupport_class(df_array, 'onehot', temp_ppd, 'columnkey')

    del df_onehot['onehot']

    np_onehot = df_onehot.values

    return np_onehot
  
  
  
  def convert_onehot_to_1010(self, np_onehot):
    """
    takes as input numpy array encoded in one-hot format
    and translates to a 1010 encoding equivalent
    based on assumption that order of columns consistent per 
    convention of convert_1010_to_onehot(.)
    """
    
#     #if not all zeros (all zeros is an edge case)
#     if np_onehot.any():

    #create list of binary encodings corresponding to the onehot array
    #assumes consistent order of columns from convert_1010_to_onehot basis
    columnslist = list(range(np_onehot.shape[1]))
    columnslist = \
    [str(format(item, f"0{int(np.log2(np_onehot.shape[1]))}b")) for item in columnslist]

    #convert to dataframe with columnslist as column headers
    df_array = pd.DataFrame(np_onehot, columns = columnslist)

    #create new column to store encodings
    df_array['1010'] = 0

    #copy columns headers to activated cells, others are 0
    for column in df_array:

      if column != '1010':

        df_array[column].replace(1, column, inplace=True)

        df_array['1010'] = \
        np.where(df_array[column] != 0, df_array[column], df_array['1010'])

        del df_array[column]

#       uniquevalues = df_array['1010'].unique()
#       uniquevalues.sort()
#       uniquevalues = list(uniquevalues)

#       #get number of 1010 columns
#       nbrcolumns = len(str(uniquevalues[0]))

    nbrcolumns = int(np.ceil(np.log2(np_onehot.shape[1])))
  
    #replace zeros with infill partition (a string of zeros of lenth nmbrcolumns)
    #note this corresponds to the default infill encoding for '1010'
    infill_plug = '0' * nbrcolumns
    df_array['1010'] = np.where(df_array.eq(0).all(1), infill_plug, df_array['1010'])
    

    _1010_columns = []
    for i in range(nbrcolumns):
      _1010_columns.append('1010_'+str(i))


    df_array['1010'] = df_array['1010'].astype(str)

    #now let's store the encoding
    i=0
    for _1010_column in _1010_columns:

      df_array[_1010_column] = df_array['1010'].str.slice(i,i+1).astype(np.int8)

      i+=1


    del df_array['1010']


    np_1010 = df_array.values
      
    
#     #else if np_onehot was all zeros (edge case)
#     else:
      
#       nbrcolumns = int(np.ceil(np.log2(np_onehot.shape[1])))
      
#       np_1010 = \
#       np.zeros((np_onehot.shape[0], nbrcolumns))

    return np_1010
  

  def LabelSetGenerator(self, df, column, label):
    '''
    #LabelSetGenerator
    #takes as input dataframe for test set, label column name, and label
    #returns a dataframe set of all rows which included that label in the column
    '''
    
    df = df[df[column] == label]

    return df



  def LabelFrequencyLevelizer(self, train_df, labels_df, labelsencoding_dict, \
                              postprocess_dict, process_dict, LabelSmoothing):
    '''
    #LabelFrequencyLevelizer(.)
    #takes as input dataframes for train set, labels, and label category
    #combines them to single df, then creates sets for each label category
    #such as to add on multiples of each set to achieve near levelized
    #frequency of label occurence in training set (increases the size
    #of the training set by redundant inclusion of rows with lower frequency
    #labels.) Returns train_df, labels_df, trainID_df.
    #for now have convention that MLinfilltypes of 1010 or concurrent_act
    #not yet supported (future extension)
    '''
    
    columns_labels = list(labels_df)
    
    #labelscategory = next(iter(labelsencoding_dict))
    #labelscategory = 
    
    
    #find origcateogry of am_labels from FSpostprocess_dict
    labelcolumnkey = list(labels_df)[0]
    origcolumn = postprocess_dict['column_dict'][labelcolumnkey]['origcolumn']
    origcategory = postprocess_dict['column_dict'][labelcolumnkey]['origcategory']

    #find labelctgy from process_dict based on this origcategory
    labelscategory = process_dict[origcategory]['labelctgy']
    
    
    
    
    MLinfilltype = postprocess_dict['process_dict'][labelscategory]['MLinfilltype']
    
    #labels = list(labelsencoding_dict[labelscategory].keys())
    labels = list(labels_df)
    labels.sort()
    
    if labels != []:

      setnameslist = []
      setlengthlist = []
      multiplierlist = []

      #if labelscategory == 'bnry':
      if MLinfilltype in ['singlct', 'binary']:
        
        singlctcolumn = False
        
        if len(labels) == 1:
          singlctcolumn = labels[0]
        else:
          for labelcolumn in labels:
            if postprocess_dict['column_dict'][labelcolumn]['category'] == labelscategory:
              singlctcolumn = labelcolumn
          if singlctcolumn is False:
            singlctcolumn = labels[0]
        
        uniquevalues = list(labels_df[singlctcolumn].unique())

        #for label in labels:
        #for label in [0,1]:
        for label in uniquevalues:
          
          #value = 
          
          #derive set of labels dataframe for counting length
          df = self.LabelSetGenerator(labels_df, singlctcolumn, label)


          #append length onto list
          setlength = df.shape[0]
          #setlengthlist = setlengthlist.append(setlength)
          setlengthlist.append(setlength)


        #length of biggest label set
        maxlength = max(setlengthlist)
        #set counter to 0
        i = 0
        #for label in labels:
        #for label in [0,1]:
        for label in uniquevalues:
          #derive multiplier to levelize label frequency
          setlength = setlengthlist[i]
          if setlength > 0:
            
            labelmultiplier = int(round(maxlength / setlength)) - 1
          else:
            labelmultiplier = 0
          #append multiplier onto list
          #multiplierlist = multiplierlist.append(labelmultiplier)
          multiplierlist.append(labelmultiplier)
          #increment counter
          i+=1

        #concatinate labels onto train set
        train_df = pd.concat([train_df, labels_df], axis=1)

        #reset counter
        i=0
        #for loop through labels
        #for label in labels:
        #for label in [0,1]:
        for label in uniquevalues:

          #create train subset corresponding to label
          df = self.LabelSetGenerator(train_df, singlctcolumn, label)

          #set j counter to 0
          j = 0
          #concatinate an additional copy of the label set multiplier times
          while j < multiplierlist[i]:
            train_df = pd.concat([train_df, df], axis=0)
            #train_df = train_df.reset_index()
            j+=1
            
          i+=1

        #now seperate the labels df from the train df
        labels_df = pd.DataFrame(train_df[singlctcolumn].copy())
        #now delete the labels column from train set
        del train_df[singlctcolumn]


      #if labelscategory in ['nmbr', 'bxcx']:
      if MLinfilltype in ['numeric', 'multisp']:

        columns_labels = []
        for label in list(labels_df):
          for label in list(labels_df):
            #here we're checking if the column is a numneric set aggregated bins
            #for support of additional types of numerical set bins just add the
            #category to this list
            if postprocess_dict['column_dict'][label]['category'] \
            in ['pwrs', 'pwr2', 'bins', 'bint', 'bnwd', 'bnwK', 'bnwM', \
                'bnep', 'bne7', 'bne9', 'bkt1', 'bkt2']:
            
              columns_labels.append(label)
            
            
      #if labelscategory in ['text', 'nmbr', 'bxcx']:
      if MLinfilltype in ['numeric', 'multisp', 'multirt']:
        if columns_labels != []:
          
          #note for. label smoothing activation values won't be 1
          level_activation = LabelSmoothing
          if level_activation <= 0.0 \
          or level_activation >= 1.0 \
          or str(level_activation) == 'False':
            level_activation = 1
          
          
          i=0
          #for label in labels:
          for label in columns_labels:
                
            column = columns_labels[i]
            #derive set of labels dataframe for counting length
            df = self.LabelSetGenerator(labels_df, column, level_activation)
        
            #append length onto list
            setlength = df.shape[0]
            #setlengthlist = setlengthlist.append(setlength)
            setlengthlist.append(setlength)

            i+=1

          #length of biggest label set
          maxlength = max(setlengthlist)

          #set counter to 0
          i = 0
          #for label in labels:
          for label in columns_labels:

            #derive multiplier to levelize label frequency
            setlength = setlengthlist[i]
            if setlength > 0:
              labelmultiplier = int(round(maxlength / setlength)) - 1
            else:
              labelmultiplier = 0
            #append multiplier onto list
            #multiplierlist = multiplierlist.append(labelmultiplier)
            multiplierlist.append(labelmultiplier)
            #increment counter
            i+=1

          #concatinate labels onto train set
          train_df = pd.concat([train_df, labels_df], axis=1)

          #reset counter
          i=0
          #for loop through labels
          
          
          #for label in labels:
          for label in columns_labels:

            #create train subset corresponding to label
            column = columns_labels[i]
            df = self.LabelSetGenerator(train_df, column, level_activation)

            #set j counter to 0
            j = 0
            #concatinate an additional copy of the label set multiplier times
            while j < multiplierlist[i]:
              train_df = pd.concat([train_df, df], axis=0)
              #train_df = train_df.reset_index()
              j+=1

            i+=1

          columns_labels = list(labels_df)
            
          #now seperate the labels df from the train df
          labels_df = train_df[columns_labels]
          #now delete the labels column from train set
          train_df = train_df.drop(columns_labels, axis=1)
        


    return train_df, labels_df
  

  
  def dictupdatetrim(self, column, postprocess_dict):
    '''
    dictupdatetrim addresses the maintenance of postprocess_dict for cases where
    a feature is struck due to the feature selection mechanism. Speciifcally, the
    function is intended to:
    - remove column entry from every case where it is included in a columnslist
    i.e. column in postprocess_dict['comlumn_dict'][key1]['columnslist'] for all key1
    - remove column entry from every case where it is included in a categorylist
    i.e. column in postprocess_dict['comlumn_dict'][key1]['categorylist'] for all key1
    - trim column's postprocess_dict['column_dict'][column]
    As a reminder, a columnslist is a list of every column that originated from the
    same source, such that we will need to edit the columnslist for every dervied
    column that originated from the same source.
    As a reminder, a categorylist is a list of every column derived as part of the 
    same single or multi-column transformation, such that we will need to edit the
    categorylist for every derived column that originated from the same transformation
    as the column we are trimming
    Trimming the postprocess_dict['column_dict'][column] is fairly strainghtforward
    Note that since we cant' edit a dictionary as we are cycling through it, we
    will use some helper objects to store details of the edits.
    For some reason creating this function was harder than it should have been.
    Sometimes it helps to just sketch it out again from scratch.
    '''

    #initialize helper objects
    helper1_dict = {}
    helper2_dict = {}
    helper3_list = []

    #this if probably isn't neccesary but just going to throw it in
    if column in postprocess_dict['column_dict']:

      #for every column_dict
      for key in postprocess_dict['column_dict']:

        #if key originates from the same source column as our column argument
        if postprocess_dict['column_dict'][key]['origcolumn'] == \
        postprocess_dict['column_dict'][key]['origcolumn']:

          #then we'll be editting the columnslist for key, first we'll store in helper1_dict
          helper1_dict.update({key : column})

          #now we'll check if key shares the same categorylist as column
          if column in postprocess_dict['column_dict'][key]['categorylist']:

            #if so we'll be removing column from key's categorylist entry, first
            #we'll store in helper2_dict
            helper2_dict.update({key : column})

      #now we'll strike the column's column_dict, firtst we'll store in helper3_list
      helper3_list = helper3_list + [column]

    #ok here we'll do the trimming associated with helper1_dict for columnslists
    for key1 in helper1_dict:
      if helper1_dict[key1] in postprocess_dict['column_dict'][key1]['columnslist']:
        postprocess_dict['column_dict'][key1]['columnslist'].remove(helper1_dict[key1])

    #ok here we'll do the trimming associated with helper2_dict for categorylists
    for key2 in helper2_dict:
      if helper2_dict[key2] in postprocess_dict['column_dict'][key2]['categorylist']:
        postprocess_dict['column_dict'][key2]['categorylist'].remove(helper2_dict[key2])

    #and finally we'll trim the column_dict for the column
    for column3 in helper3_list:

      del postprocess_dict['column_dict'][column3]
    
#     #here we'll address the postprocess_dict['origcolumn'] entry for columnkey
#     #basically if we trim the column associated with the columnkey, we'll need
#     #to assign a new columnkey for use in postmunge which has not been previously trimmed
    
#     origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
#     newcolumnkey = ''
#     if column = postprocess_dict['origcolumn'][origcolumn]['columnkey']:
#       for potentialcolumnkey in postprocess_dict['origcolumn'][origcolumn]['columnkey']:
#         if potentialcolumnkey in list(postprocess_dict['column_dict'][column]['columnlist']):
#             newcolumnkey = potentialcolumnkey
#             postprocess_dict['origcolumn'][origcolumn]['columnkey'] = newcolumnkey
#             break

    return postprocess_dict
  


  def trainFSmodel(self, am_subset, am_labels, randomseed, labelsencoding_dict, \
                   process_dict, postprocess_dict, labelctgy, ML_cmnd):
    
    if len(list(am_labels)) > 0:

      df_train_fillfeatures_plug = am_subset[:][:1].copy()
      df_test_fillfeatures_plug = am_subset[:][:1].copy()
      categorylist = postprocess_dict['column_dict'][list(am_labels)[0]]['categorylist']

      _infilla, _infillb, FSmodel = \
      self.predictinfill(labelctgy, am_subset, am_labels, \
                         df_train_fillfeatures_plug, df_test_fillfeatures_plug, \
                         randomseed, postprocess_dict, ML_cmnd, \
                         columnslist = categorylist)

      del _infilla, _infillb
      
    else:
      
      FSmodel = False
    
    return FSmodel
      
  
  def createFSsets(self, am_subset, column, columnslist, randomseed):
    '''
    very simply shuffles rows of columns from columnslist with randomseed
    then returns the resulting dataframe
    
    hat tip for permutation method from "Beware Default Random Forest Importances"
    by Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard
    '''
    
    shuffleset = am_subset.copy()
    
    for clcolumn in columnslist:
      
      #uses support function
      shuffleset = self.df_shuffle_series(shuffleset, clcolumn, randomseed)
      
    return shuffleset

  def createFSsets2(self, am_subset, column, columnslist, randomseed):
    '''
    similar to createFSsets except performed such as to only leave one column from
    the columnslist untouched and shuffle the rest 
    '''
    shuffleset2 = am_subset.copy()
    
    for clcolumn in columnslist:
        
      if clcolumn != column:
            
        #uses support function
        shuffleset2 = self.df_shuffle_series(shuffleset2, clcolumn, randomseed)
    
    return shuffleset2
    
  
  def shuffleaccuracy(self, np_shuffleset, np_labels, FSmodel, randomseed, \
                      labelsencoding_dict, process_dict, labelctgy, postprocess_dict):
    '''
    measures accuracy of predictions of shuffleset (which had permutation method)
    against the model trained on the unshuffled set
    '''

    labelscategory = labelctgy
    
    
    MLinfilltype = process_dict[labelscategory]['MLinfilltype']
    
    
    #if labelscategory in ['nmbr']:
    if MLinfilltype in ['numeric', 'concurrent_nmbr']:
          
      #convert dataframes to numpy arrays
      np_shuffleset = np_shuffleset.values
      np_labels = np_labels.values
      
      #this is to address a weird error message suggesting I reshape the y with ravel()
      np_labels = np.ravel(np_labels)
      
      #generate predictions
      np_predictions = FSmodel.predict(np_shuffleset)
      
      #just in case this returned any negative predictions
      np_predictions = np.absolute(np_predictions)
      #and we're trying to generalize here so will go ahead and apply to labels
      np_labels = np.absolute(np_labels)
      
      #evaluate accuracy metric
      #columnaccuracy = accuracy_score(np_labels, np_predictions)
      #columnaccuracy = mean_squared_log_error(np_labels, np_predictions)
      columnaccuracy = 1 - mean_squared_log_error(np_labels, np_predictions)
      
    #if labelscategory in ['bnry']:
    if MLinfilltype in ['singlct', 'binary', 'concurrent_act']:
      
      #convert dataframes to numpy arrays
      np_shuffleset = np_shuffleset.values
      np_labels = np_labels.values
      
      #this is to address a weird error message suggesting I reshape the y with ravel()
      np_labels = np.ravel(np_labels)
      
      #generate predictions
      np_predictions = FSmodel.predict(np_shuffleset)
      
      #evaluate accuracy metric
      columnaccuracy = accuracy_score(np_labels, np_predictions)
      
    #if labelscategory in ['text']:
    if MLinfilltype in ['multirt', 'multisp']:
      
      #convert dataframes to numpy arrays
      np_shuffleset = np_shuffleset.values
      np_labels = np_labels.values
      
      #muiltirt sets as edge case may sometimes be returned with one column
      if np_labels.shape[1] == 1:
        np_labels = np.ravel(np_labels)
      
      #generate predictions
      np_predictions = FSmodel.predict(np_shuffleset)
      
      #evaluate accuracy metric
      #columnaccuracy = accuracy_score(np_labels, np_predictions)
      columnaccuracy = accuracy_score(np_labels, np_predictions)

    if MLinfilltype in ['1010']:

      #convert dataframes to numpy arrays
      np_shuffleset = np_shuffleset.values
      np_labels = np_labels.values
      
      np_labels = \
      self.convert_1010_to_onehot(np_labels)
      
      #generate predictions
      np_predictions = FSmodel.predict(np_shuffleset)
      
      
      #evaluate accuracy metric
      #columnaccuracy = accuracy_score(np_labels, np_predictions)
      columnaccuracy = accuracy_score(np_labels, np_predictions)
        
    #I think this will clear some memory
    del np_labels, np_shuffleset
    
    return columnaccuracy
  
  
  def assemblemadethecut(self, FScolumn_dict, featurepct, featuremetric, featuremethod, \
                         am_subset_columns):
    '''
    takes as input the FScolumn_dict and the passed automunge argument featurepct
    and a list of the columns from automunge application in featureselect
    and uses to assemble a list of columns that made it through the feature
    selection process
    
    returns list madethecut
    '''
    
    #create empty dataframe for sorting purposes
    FSsupport_df = pd.DataFrame(columns=['FS_column', 'metric', 'category'])
    
    #Future extension:
    #FSsupport_df = pd.DataFrame(columns=['FS_column', 'metric', 'metric2', 'category'])
    
    #add rows to the dataframe for each column
    for key in FScolumn_dict:
      
      column_df = pd.DataFrame([[key, FScolumn_dict[key]['metric'], FScolumn_dict[key]['category']]], \
                               columns=['FS_column', 'metric', 'category'])
  
      FSsupport_df = pd.concat([FSsupport_df, column_df], axis=0)
    
    #sort the rows by metric (from large to small, not that higher metric implies
    #more predictive power associated with that column's feature)
    #(note that NaN rows will have NaN values at bottom of list)
    FSsupport_df = FSsupport_df.sort_values(['metric'], ascending=False)
    
    #create list of candidate entries for madethecut
    candidates = list(FSsupport_df['FS_column'])
    
    
    #count the total number of rows
    totalrowcount =  FSsupport_df.shape[0]
    #count ranked rows
    metriccount = totalrowcount
    
    #create list of NArws
    #candidateNArws = candidates[-NaNcount:]
#     candidateNArws = list(FSsupport_df[FSsupport_df['category']=='NArw']['FS_column'])
    candidateNArws = list()
    
    #create list of feature rows
    #candidatefeaturerows = candidates[:-NaNcount]
#     candidatefeaturerows = list(FSsupport_df[FSsupport_df['category']!='NArw']['FS_column'])
    candidatefeaturerows = list(FSsupport_df['FS_column'])
    
#     #calculate the number of features we'll keep using the ratio passed from automunge
#     numbermakingcut = int(metriccount * featurepct)
    
    if featuremethod not in ['default', 'pct', 'metric', 'report']:
      print("error featuremethod object must be one of ['default', 'pct', 'metric', 'report']")
      
    if featuremethod == 'default':

      #calculate the number of features we'll keep using the ratio passed from automunge
      numbermakingcut = len(FSsupport_df)
    
    if featuremethod == 'pct':

      #calculate the number of features we'll keep using the ratio passed from automunge
      numbermakingcut = int(metriccount * featurepct)
      
    if featuremethod == 'metric':
      
      #calculate the number of features we'll keep using the ratio passed from automunge
      numbermakingcut = len(FSsupport_df[FSsupport_df['metric'] >= featuremetric])
      
    if featuremethod == 'report':
      #just a plug vlaue
      numbermakingcut = 1
      
    #generate list of rows making the cut
    madethecut = candidatefeaturerows[:numbermakingcut]
    
    
    return madethecut


  def featureselect(self, df_train, labels_column, trainID_column, \
                    powertransform, binstransform, randomseed, \
                    numbercategoryheuristic, assigncat, transformdict, \
                    processdict, featurepct, featuremetric, featuremethod, \
                    ML_cmnd, process_dict, valpercent1, valpercent2, printstatus, \
                    NArw_marker, assignparam):
    """
    featureselect is a function called within automunge() that applies methods
    to evaluate predictive power of derived features towards a downstream model
    such as to trim the branches of the transform tree.
    
    The function returns a list of column names that "made the cut" so that
    automunge() can then remove extraneous branches.
    """
    
    #now we'll use automunge() to prepare the subset for feature evaluation
    #note the passed arguments, these are all intentional (no MLinfill applied,
    #primary goal here is to produce a processed dataframe for df_subset
    #with corresponding labels)
    
        
    #printout display progress
    if printstatus is True:
      print("_______________")
      print("Begin Feature Importance evaluation")
      print("")
    
    if labels_column is False:
      
      FSmodel = False
      
      #printout display progress
      if printstatus is True:
        print("_______________")
        print("No labels_column passed, Feature Importance halted")
        print("")
    
    elif labels_column is not False:

      #but first real quick we'll just deal with PCA default functionality for FS
      FSML_cmnd = deepcopy(ML_cmnd)
      FSML_cmnd['PCA_type'] = 'off'

      FS_LabelSmoothing = False

  #     #FUTURE EXTENSION
  #     #user has option to turn off LabelSmoothing for feature importance evaluation such as by passing False
  #     FS_LabelSmoothing = 0.9
  #     if 'LabelSmoothing' in ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']:
  #       FS_LabelSmoothing = ML_cmnd['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']

      FS_assignparam = deepcopy(assignparam)

      totalvalidation = valpercent1 + valpercent2

      if totalvalidation == 0:
        totalvalidation = 0.2

      am_train, _1, am_labels, \
      am_validation1, _3, am_validationlabels1, \
      _5, _6, _7, \
      _8, _9, _10, \
      labelsencoding_dict, finalcolumns_train, _10,  \
      _11, FSpostprocess_dict = \
      self.automunge(df_train, df_test = False, labels_column = labels_column, trainID_column = trainID_column, \
                    testID_column = False, valpercent1 = totalvalidation, valpercent2 = 0.0, \
                    shuffletrain = True, TrainLabelFreqLevel = False, powertransform = powertransform, \
                    binstransform = binstransform, MLinfill = False, infilliterate=1, randomseed = randomseed, \
                    LabelSmoothing_train = FS_LabelSmoothing, excl_suffix = True, \
                    numbercategoryheuristic = numbercategoryheuristic, pandasoutput = True, NArw_marker = NArw_marker, \
                    featureselection = False, featurepct = 1.00, featuremetric = featuremetric, \
                    featuremethod = 'pct', ML_cmnd = FSML_cmnd, assigncat = assigncat, \
                    assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \
                                   'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}, \
                    assignparam = FS_assignparam, \
                    transformdict = transformdict, processdict = processdict, printstatus=printstatus)


      #this is the returned process_dict
      #(remember "processdict" is what we pass to automunge() call, "process_dict" is what is 
      #assembled inside automunge, there is a difference)
      FSprocess_dict = FSpostprocess_dict['process_dict']

      
      if am_labels.empty is True:
        FSmodel = False
        
        #printout display progress
        if printstatus is True:
          print("_______________")
          print("No labels returned from automunge(.), Feature Importance halted")
          print("")
    
      #if am_labels is not an empty set
      if am_labels.empty is False:

        #find origcateogry of am_labels from FSpostprocess_dict
        labelcolumnkey = list(am_labels)[0]
        origcolumn = FSpostprocess_dict['column_dict'][labelcolumnkey]['origcolumn']
        origcategory = FSpostprocess_dict['column_dict'][labelcolumnkey]['origcategory']

        #find labelctgy from process_dict based on this origcategory
        labelctgy = process_dict[origcategory]['labelctgy']

        am_categorylist = []


        for am_label_column in list(am_labels):

          if FSpostprocess_dict['column_dict'][am_label_column]['category'] == labelctgy:

            am_categorylist = FSpostprocess_dict['column_dict'][am_label_column]['categorylist']
            
            #we'll follow convention that if target label category MLinfilltype is concurrent
            #we'll arbitrarily take the first column and use that as target
            if FSpostprocess_dict['process_dict'][labelctgy]['MLinfilltype'] \
            in ['concurrent_act', 'concurrent_nmbr']:
              
              am_categorylist = [am_categorylist[0]]
              
            break

        if len(am_categorylist) == 1:
          am_labels = pd.DataFrame(am_labels[am_categorylist[0]])
          am_validationlabels1 = pd.DataFrame(am_validationlabels1[am_categorylist[0]])

        else:
          am_labels = am_labels[am_categorylist]
          am_validationlabels1 = am_validationlabels1[am_categorylist]

        #if there's a bug occuring after this point it might mean the labelctgy wasn't
        #properly populated in the process_dict for the root category assigned to the labels
        #again the labelctgy entry to process_dict represents for labels returned in 
        #multiple configurations the trasnofrmation category whose returned set will be
        #used to train the feature selection model


        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Training feature importance evaluation model")
          print("")

        #apply function trainFSmodel
        #FSmodel, baseaccuracy = \
        FSmodel = \
        self.trainFSmodel(am_train, am_labels, randomseed, labelsencoding_dict, \
                          FSprocess_dict, FSpostprocess_dict, labelctgy, ML_cmnd)
        
        if FSmodel is False:
          
          FScolumn_dict = {}
          
          FS_origcolumns = list(FSpostprocess_dict['origcolumn'])
          
          #printout display progress
          if printstatus is True:
            print("_______________")
            print("No model returned from training, Feature Importance halted")
            print("")
          
        
        elif FSmodel is not False:

          #update v2.11 baseaccuracy should be based on validation set
          baseaccuracy = self.shuffleaccuracy(am_validation1, am_validationlabels1, \
                                              FSmodel, randomseed, labelsencoding_dict, \
                                              FSprocess_dict, labelctgy, FSpostprocess_dict)

          #get list of columns
          am_train_columns = list(am_train)

          #initialize dictionary FScolumn_dict = {}
          FScolumn_dict = {}
          
          FS_origcolumns = list(FSpostprocess_dict['origcolumn'])
          
          #assemble FScolumn_dict to support the feature evaluation
          for column in am_train_columns:

            #pull categorylist, category, columnslist
            categorylist = FSpostprocess_dict['column_dict'][column]['categorylist']
            category = FSpostprocess_dict['column_dict'][column]['category']
            columnslist = FSpostprocess_dict['column_dict'][column]['columnslist']
            origcolumn = FSpostprocess_dict['column_dict'][column]['origcolumn']

            #create entry to FScolumn_dict
            FScolumn_dict.update({column : {'categorylist' : categorylist, \
                                            'category' : category, \
                                            'columnslist' : columnslist, \
                                            'origcolumn' : origcolumn, \
                                            'FScomplete' : False, \
                                            'shuffleaccuracy' : None, \
                                            'shuffleaccuracy2' : None, \
                                            'baseaccuracy' : baseaccuracy, \
                                            'metric' : None, \
                                            'metric2' : None}})

          #printout display progress
          if printstatus is True:
            print("_______________")
            print("Evaluating feature importances")
            print("")


          #perform feature evaluation on each column
          for column in am_train_columns:

            if FScolumn_dict[column]['FScomplete'] is False:

              #categorylist = FScolumn_dict[column]['categorylist']
              #update version 1.80, let's perform FS on columnslist instead of categorylist
              columnslist = FScolumn_dict[column]['columnslist']

              #create set with columns shuffle from columnslist
              #shuffleset = self.createFSsets(am_train, column, categorylist, randomseed)
              #shuffleset = self.createFSsets(am_train, column, columnslist, randomseed)
              shuffleset = self.createFSsets(am_validation1, column, columnslist, randomseed)

              #determine resulting accuracy after shuffle
              columnaccuracy = self.shuffleaccuracy(shuffleset, am_validationlabels1, \
                                                    FSmodel, randomseed, labelsencoding_dict, \
                                                    FSprocess_dict, labelctgy, FSpostprocess_dict)


              #I think this will clear some memory
              del shuffleset

              #category accuracy penalty metric
              metric = baseaccuracy - columnaccuracy
              #metric2 = baseaccuracy - columnaccuracy2



              #save accuracy to FScolumn_dict and set FScomplete to True
              #(for each column in the categorylist)
              #for categorycolumn in FSpostprocess_dict['column_dict'][column]['categorylist']:
              for categorycolumn in FSpostprocess_dict['column_dict'][column]['columnslist']:

                FScolumn_dict[categorycolumn]['FScomplete'] = True
                FScolumn_dict[categorycolumn]['shuffleaccuracy'] = columnaccuracy
                FScolumn_dict[categorycolumn]['metric'] = metric
                #FScolumn_dict[categorycolumn]['shuffleaccuracy2'] = columnaccuracy2
                #FScolumn_dict[categorycolumn]['metric2'] = metric2



            columnslist = FScolumn_dict[column]['columnslist']

            #create second set with all but one columns shuffled from columnslist
            #this will allow us to compare the relative importance between columns
            #derived from the same parent
            #shuffleset2 = self.createFSsets2(am_train, column, columnslist, randomseed)
            shuffleset2 = self.createFSsets2(am_validation1, column, columnslist, randomseed)

            #determine resulting accuracy after shuffle
            columnaccuracy2 = self.shuffleaccuracy(shuffleset2, am_validationlabels1, \
                                                  FSmodel, randomseed, labelsencoding_dict, \
                                                  FSprocess_dict, labelctgy, FSpostprocess_dict)

            metric2 = baseaccuracy - columnaccuracy2

            FScolumn_dict[column]['shuffleaccuracy2'] = columnaccuracy2
            FScolumn_dict[column]['metric2'] = metric2


          
          
          madethecut = self.assemblemadethecut(FScolumn_dict, featurepct, featuremetric, \
                                           featuremethod, am_train_columns)
    
    
    #if the only column left in madethecut from origin column is a NArw, delete from the set
    #(this is going to lean on the column ID string naming conventions)
    #couldn't get this to work, this functionality a future extension
#     trimfrommtc = []
#     for traincolumn in list(df_train):
#       if (traincolumn + '_') not in [checkmtc[:(len(traincolumn)+1)] for checkmtc in madethecut]:
#         for mtc in madethecut:
#           #if mtc originated from traincolumn
#           if mtc[:(len(traincolumn)+1)] == traincolumn + '_':
#             #count the number of same instance in madethecut set
#             madethecut_trim = [mdc_trim[:(len(traincolumn)+1)] for mdc_trim in madethecut]
#             if madethecut_trim.count(mtc[:(len(traincolumn)+1)]) == 1 \
#             and mtc[-5:] == '_NArw':
#               trimfrommtc = trimfrommtc + [mtc]
#     madethecut = list(set(madethecut).difference(set(trimfrommtc)))
          
       
    #apply function madethecut(FScolumn_dict, featurepct)
    #return madethecut
    #where featurepct is the percent of features that we intend to keep
    #(might want to make this a passed argument from automunge)
    
        #I think this will clear some memory
        del am_train, _1, am_labels, am_validation1, _3, \
        am_validationlabels1, _5, _6, _7, \
        _8, _9, labelsencoding_dict, finalcolumns_train, _10,  \
        FSpostprocess_dict
        

        if printstatus is True:
          print("_______________")
          print("Feature Importance results:")
          print("")

        #to inspect values returned in featureimportance object one could run
        if printstatus is True:
          for keys,values in FScolumn_dict.items():
            print(keys)
            print('metric = ', values['metric'])
            print('metric2 = ', values['metric2'])
            print("")
    


    FS_sorted = {'metric_key':{}, 'column_key':{}, 'metric2_key':{}, 'metric2_column_key':{}}
    
    #first we'll handle first metric based on source column
    for FS_origcolumn in FS_origcolumns:
      for key in FScolumn_dict:
        if FScolumn_dict[key]['origcolumn'] == FS_origcolumn:
          if FScolumn_dict[key]['metric'] in FS_sorted['metric_key']:
            if isinstance(FS_sorted['metric_key'][FScolumn_dict[key]['metric']], list):
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']].append(FS_origcolumn)
            else:
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']] = \
              [FS_sorted['metric_key'][FScolumn_dict[key]['metric']]]
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']].append(FS_origcolumn)
          else:
            FS_sorted['metric_key'].update({FScolumn_dict[key]['metric'] : [FS_origcolumn]})
          break

          
    FS_sorted['metric_key'] = dict(sorted(FS_sorted['metric_key'].items(), reverse=True))
    
    for key in FS_sorted['metric_key']:
      for entry in FS_sorted['metric_key'][key]:
        entry_index = FS_sorted['metric_key'][key].index(entry)
        FS_sorted['column_key'].update({FS_sorted['metric_key'][key][entry_index] : key})
      
    
    #now for metric2 based on derived columns relative importance, note sorted in other order
    for FS_origcolumn in FS_origcolumns:
      FS_sorted['metric2_key'].update({FS_origcolumn : {}})
      for key in FScolumn_dict:
        if FScolumn_dict[key]['origcolumn'] == FS_origcolumn:
          if FScolumn_dict[key]['metric2'] in FS_sorted['metric2_key'][FS_origcolumn]:
            if isinstance(FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']], list):
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']].append(key)
            else:
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']] = \
              [FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']]]
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']].append(key)
          else:
            FS_sorted['metric2_key'][FS_origcolumn].update({FScolumn_dict[key]['metric2'] : [key]})
    
    for key in FS_sorted['metric2_key']:
      FS_sorted['metric2_key'][key] = dict(sorted(FS_sorted['metric2_key'][key].items(), reverse=False))
    
    for key1 in FS_sorted['metric2_key']:
      FS_sorted['metric2_column_key'].update({key1 : {}})
      for key2 in FS_sorted['metric2_key'][key1]:
        for entry in FS_sorted['metric2_key'][key1][key2]:
          entry_index = FS_sorted['metric2_key'][key1][key2].index(entry)
          FS_sorted['metric2_column_key'][key1].update({FS_sorted['metric2_key'][key1][key2][entry_index] : key2})
        
    
    if printstatus is True:
      print()
      print("______________________")
      print("sorted metric results:")
      print()
      for keys,values in FS_sorted['metric_key'].items():
        for entry in values:
          print(entry)
          print(keys)
          print()
      print("______________________")
      print("sorted metric2 results:")
      print()
      for key in FS_sorted['metric2_key']:
        print("for source column: ", key)
        for keys,values in FS_sorted['metric2_key'][key].items():
          for entry in values:
            print(entry)
            print(keys)
            print()
        print()
    
    if FSmodel is False:
      
      madethecut = []
      FScolumn_dict = {}
    
    
    #printout display progress
    if printstatus is True:
      print("_______________")
      print("Feature Importance evaluation complete")
      print("")
    
    return madethecut, FSmodel, FScolumn_dict, FS_sorted
  
  
  def assemblepostprocess_assigninfill(self, assigninfill, infillcolumns_list, 
                                       columns_train, postprocess_dict, MLinfill):
    """
    #this function converts user passed assigninfill
    #into a collection of post-transform column assignments
    #to various infill methods for application in apply infill functions
    
    #assigninfill is as passed by user
    #(note assigninfill previously had validations performed in check_assigninfill)
    #infillcolumns_list is all dervied columns in train set
    #columns_train is all source columns in train set
    #postprocess_dict is how data shared between functions
    #MLinfill is boolean marker for default MLinfill applciation
    
    #The convention is that unspecified columns are cast to
    #stndrdinfill or MLinfill based on MLinfill parameter
    #The other convention is that user may assign column headers
    #both with and without suffix appenders
    #the source column headers are converted to set of dervied column headers
    #and then any user specified derived columns w/ suffix take precendence over the converted ones
    
    #so workflow is as follows
    #- received spec'd assigninfill (which may include both pre-suf and w/-suf)
    #- aggregate just spec'd w/-suf into mirror spec assigninfill_withsuffix
    #- aggregate spec'd pre-suf into mirror spec assigninfill_sourcecolumn
    #- identify unspecified source columns missing from assigninfill_sourcecolumn,
    #add as new category assigninfill_sourcecolumn['unspecified']
    #- convert source columns to dervied columns from assigninfill_sourcecolumn
    #to assigninfill_sourcecolumn_converted
    #- if assigninfill_sourcecolumn_converted doesn't yet have entries for stdrdinfill or MLinfill, create
    #- based on MLinfill, copy entries from assigninfill_sourcecolumn_converted['unspecified']
    #into either 'stdrdinfill' or 'MLinfill'
    #(we'll keep 'unspecified' entry in case might be of use down the road)
    #- for duplicates between entries to assigninfill_withsuffix and assigninfill_sourcecolumn_converted
    #assigninfill_withsuffix takes precedence
    #so cycle through and if duplicates found remove from assigninfill_sourcecolumn_converted
    #- combine assigninfill_withsuffix and assigninfill_sourcecolumn_converted into
    #the returned set postprocess_assigninfill_dict
    #- insert any missing keys needed for apply_am_infill
    #- return postprocess_assigninfill_dict
    """
    
    #- received spec'd assigninfill (which may include both pre-suf and w/-suf)
    
    #- aggregate just spec'd w/-suf into mirror spec assigninfill_withsuffix
    assigninfill_withsuffix = {}
    for key in assigninfill:
      assigninfill_withsuffix.update({key:[]})
      for entry in assigninfill[key]:
        if entry in infillcolumns_list:
          assigninfill_withsuffix[key].append(entry)
          
    #- aggregate spec'd pre-suf into mirror spec assigninfill_sourcecolumn
    assigninfill_sourcecolumn = {}
    for key in assigninfill:
      assigninfill_sourcecolumn.update({key:[]})
      for entry in assigninfill[key]:
        if entry in columns_train:
          assigninfill_sourcecolumn[key].append(entry)
          
    #- identify unspecified source columns missing from assigninfill_sourcecolumn,
    #add as new category assigninfill_sourcecolumn['unspecified']
    specd_sourcecolumns = []
    for key in assigninfill_sourcecolumn:
      specd_sourcecolumns += assigninfill_sourcecolumn[key]
    unspecd_sourcecolumns = list(set(columns_train) - set(specd_sourcecolumns))
    assigninfill_sourcecolumn.update({'unspecified':unspecd_sourcecolumns})
    
    #- convert source columns to dervied columns from assigninfill_sourcecolumn
    #to assigninfill_sourcecolumn_converted
    assigninfill_sourcecolumn_converted = {}
    for key in assigninfill_sourcecolumn:
      assigninfill_sourcecolumn_converted.update({key:[]})
      for entry in assigninfill_sourcecolumn[key]:
        #accessing dervied columns from source column, 
        #adding as entries to assigninfill_sourcecolumn_converted[key]
        assigninfill_sourcecolumn_converted[key] += postprocess_dict['origcolumn'][entry]['columnkeylist']
        
    #- if assigninfill_sourcecolumn_converted doesn't yet have entries for stdrdinfill or MLinfill, create
    if 'stdrdinfill' not in assigninfill_sourcecolumn_converted:
      assigninfill_sourcecolumn_converted.update({'stdrdinfill':[]})
    if 'MLinfill' not in assigninfill_sourcecolumn_converted:
      assigninfill_sourcecolumn_converted.update({'MLinfill':[]})
    
    #- based on MLinfill, copy entries from assigninfill_sourcecolumn_converted['unspecified']
    #into either 'stdrdinfill' or 'MLinfill'
    #(we'll keep 'unspecified' entry in case might be of use down the road)
    if MLinfill is True:
      assigninfill_sourcecolumn_converted['MLinfill'] += assigninfill_sourcecolumn_converted['unspecified']
    else:
      assigninfill_sourcecolumn_converted['stdrdinfill'] += assigninfill_sourcecolumn_converted['unspecified']
      
    #- for duplicates between entries to assigninfill_withsuffix and assigninfill_sourcecolumn_converted
    #assigninfill_withsuffix takes precedence
    #so cycle through and if duplicates found remove from assigninfill_sourcecolumn_converted
    all_specd_withsuffix = []
    for key in assigninfill_withsuffix:
      all_specd_withsuffix += assigninfill_withsuffix[key]
    for key in assigninfill_sourcecolumn_converted:
      for entry in assigninfill_sourcecolumn_converted[key]:
        if entry in all_specd_withsuffix:
          assigninfill_sourcecolumn_converted[key].remove(entry)
          
    #- combine assigninfill_withsuffix and assigninfill_sourcecolumn_converted into
    #the returned set postprocess_assigninfill_dict
    
    #first let's make sure they have equivalent keys
    for key1 in assigninfill_withsuffix:
      if key1 not in assigninfill_sourcecolumn_converted:
        assigninfill_sourcecolumn_converted.update({key1:[]})
    for key2 in assigninfill_sourcecolumn_converted:
      if key2 not in assigninfill_withsuffix:
        assigninfill_withsuffix.update({key2:[]})
    
    #ok now populate 
    postprocess_assigninfill_dict = {}
    
    for key in assigninfill_sourcecolumn_converted:
      postprocess_assigninfill_dict.update({key: assigninfill_withsuffix[key] + assigninfill_sourcecolumn_converted[key]})
    
    #- insert any missing keys needed for apply_am_infill
    if 'stdrdinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['stdrdinfill'] = []
    
    if 'zeroinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['zeroinfill'] = []

    if 'oneinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['oneinfill'] = []

    if 'adjinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['adjinfill'] = []

    if 'medianinfill' not in postprocess_assigninfill_dict: 
      postprocess_assigninfill_dict['medianinfill'] = []

    if 'meaninfill' not in postprocess_assigninfill_dict: 
      postprocess_assigninfill_dict['meaninfill'] = []

    if 'modeinfill' not in postprocess_assigninfill_dict: 
      postprocess_assigninfill_dict['modeinfill'] = []
      
    if 'lcinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['lcinfill'] = []
      
    if 'MLinfill' not in postprocess_assigninfill_dict:
      postprocess_assigninfill_dict['MLinfill'] = []
    
    #- return postprocess_assigninfill_dict
    return postprocess_assigninfill_dict
    
  
  def apply_am_infill(self, df_train, df_test, postprocess_assigninfill_dict, \
                      postprocess_dict, infilliterate, printstatus, infillcolumns_list, \
                      masterNArows_train, masterNArows_test, process_dict, randomseed, ML_cmnd):
    """
    #Modularizes the application of infill to train and test sets
    """
    
    #infilliterate allows ML infill sets to run multiple times
    #as may be beneficial if set had a high proportion of infill for instance
    iteration = 0
    if infilliterate == 0:
      infilliterate = 1
      
    #if we're uysing this method we'll have some extra printouts
    if infilliterate > 1:
      print_infilliterate = True
    else:
      print_infilliterate = False
      
    while iteration < infilliterate:
      
      #resent MLinfill infillcomplete markers to False
      if iteration > 0:
        for key in postprocess_assigninfill_dict['MLinfill']:
          postprocess_dict['column_dict'][key]['infillcomplete'] = False
      
      if printstatus is True:
        if print_infilliterate is True:
          print("______")
          print("ML infill infilliterate iteration: ", iteration + 1)
          print(" ")
          
      for column in infillcolumns_list:
          
        if column in postprocess_dict['column_dict']:
          
          if process_dict[postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
          != 'boolexclude':

            if iteration == 0:
              
              #stndrdinfill (just prinouts, this was done in processing funcitons)
              if column in postprocess_assigninfill_dict['stdrdinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: stdrdinfill")
                  print("")
                  
              #zeroinfill
              if column in postprocess_assigninfill_dict['zeroinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: zeroinfill")
                  print("")

                categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])

                df_train = \
                self.zeroinfillfunction(df_train, column, postprocess_dict, \
                                        masterNArows_train)

                df_test = \
                self.zeroinfillfunction(df_test, column, postprocess_dict, \
                                        masterNArows_test)
                

              #oneinfill
              if column in postprocess_assigninfill_dict['oneinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: oneinfill")
                  print("")

                categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])


                df_train = \
                self.oneinfillfunction(df_train, column, postprocess_dict, \
                                       masterNArows_train)

                df_test = \
                self.oneinfillfunction(df_test, column, postprocess_dict, \
                                       masterNArows_test)
                

              #adjinfill
              if column in postprocess_assigninfill_dict['adjinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: adjinfill")
                  print("")


                df_train = \
                self.adjinfillfunction(df_train, column, postprocess_dict, \
                                       masterNArows_train)

                df_test = \
                self.adjinfillfunction(df_test, column, postprocess_dict, \
                                       masterNArows_test)
                

              #medianinfill
              if column in postprocess_assigninfill_dict['medianinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: medianinfill")
                  print("")


                #check if column is boolean
                boolcolumn = False
                #exclude boolean and ordinal from this infill method
                if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
                in ['multirt', 'multisp', 'singlct', 'binary', '1010', 'boolexclude', 'concurrent_act']:
                  boolcolumn = True

                categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])

                if (categorylistlength == 1) \
                and boolcolumn is False:
                  #noting that currently we're only going to infill 0 for single column categorylists
                  #some comparable address for multi-column categories is a future extension

                  df_train, infillvalue = \
                  self.train_medianinfillfunction(df_train, column, postprocess_dict, \
                                                  masterNArows_train)

                  postprocess_dict['column_dict'][column]['normalization_dict'][column].update({'infillvalue':infillvalue})

                  df_test = \
                  self.test_medianinfillfunction(df_test, column, postprocess_dict, \
                                                 masterNArows_test, infillvalue)
                  
              
              #meaninfill
              if column in postprocess_assigninfill_dict['meaninfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: meaninfill")
                  print("")


                #check if column is boolean
                boolcolumn = False
                #exclude boolean and ordinal from this infill method
                if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
                in ['multirt', 'multisp', 'singlct', 'binary', '1010', 'boolexclude', 'concurrent_act']:
                  boolcolumn = True

                categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])

                #if (column not in excludetransformscolumns) \
                if (categorylistlength == 1) \
                and boolcolumn is False:
                  #noting that currently we're only going to infill 0 for single column categorylists
                  #some comparable address for multi-column categories is a future extension

                  df_train, infillvalue = \
                  self.train_meaninfillfunction(df_train, column, postprocess_dict, \
                                                masterNArows_train)

                  postprocess_dict['column_dict'][column]['normalization_dict'][column].update({'infillvalue':infillvalue})

                  df_test = \
                  self.test_meaninfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)
                  
              
              #modeinfill
              if column in postprocess_assigninfill_dict['modeinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: modeinfill")
                  print("")

                #check if column is excluded (variable poorly named, interpret boolcolumn here as excluded)
                boolcolumn = False
                
                #seems reasonable to exclude concurrent_nmbr from mode
                if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
                in ['boolexclude', 'concurrent_nmbr']:
                  boolcolumn = True


                if boolcolumn is False:

                  df_train, infillvalue = \
                  self.train_modeinfillfunction(df_train, column, postprocess_dict, \
                                                masterNArows_train)

                  postprocess_dict['column_dict'][column]['normalization_dict'][column].update({'infillvalue':infillvalue})

                  df_test = \
                  self.test_modeinfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)
                  
              
              #lcinfill:
              if column in postprocess_assigninfill_dict['lcinfill']:

                #printout display progress
                if printstatus is True:
                  print("infill to column: ", column)
                  print("     infill type: lcinfill")
                  print("")

                #check if column is excluded (variable poorly named, interpret boolcolumn here as excluded)
                boolcolumn = False

                #seems reasonable to exclude concurrent_nmbr from mode
                if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
                in ['boolexclude', 'concurrent_nmbr']:
                  boolcolumn = True


                if boolcolumn is False:

                  df_train, infillvalue = \
                  self.train_lcinfillfunction(df_train, column, postprocess_dict, \
                                                masterNArows_train)

                  postprocess_dict['column_dict'][column]['normalization_dict'][column].update({'infillvalue':infillvalue})

                  #repurpose modeinfillfunction for test, only difference is the passed infillvalue
                  df_test = \
                  self.test_modeinfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)
                  
                  

            #MLinfill:
            if column in postprocess_assigninfill_dict['MLinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: MLinfill")
                print("")


              df_train, df_test, postprocess_dict = \
              self.MLinfillfunction(df_train, df_test, column, postprocess_dict, \
                                    masterNArows_train, masterNArows_test, randomseed, ML_cmnd)

      
      for columnname in list(df_train):
        postprocess_dict['column_dict'][columnname]['infillcomplete'] = False
      
      iteration += 1
                  
    
    return df_train, df_test, postprocess_dict
  
  
  def apply_pm_infill(self, df_test, postprocess_assigninfill_dict, \
                      postprocess_dict, printstatus, infillcolumns_list, \
                      masterNArows_test, process_dict):
    """
    #Modularizes the application of infill to test sets
    """
    
    
    #infilliterate allows ML infill sets to run multiple times
    #as may be bneficial if set had a high number of infill for instance
    iteration = 0
    
    #if we're uysing this method we'll have some extra printouts
    if postprocess_dict['infilliterate'] > 1:
      print_infilliterate = True
    else:
      print_infilliterate = False
      
    
    #just the convention
    if postprocess_dict['infilliterate'] == 0:
      postprocess_dict['infilliterate'] = 1
    
    while iteration < postprocess_dict['infilliterate']:
      
      #resent MLinfill infillcomplete markers to False
#       if iteration > 0:
      for key in postprocess_assigninfill_dict['MLinfill']:
        postprocess_dict['column_dict'][key]['infillcomplete'] = False
      
      if printstatus is True:
        if print_infilliterate is True:
          print("______")
          print("ML infill infilliterate iteration: ", iteration + 1)
          print(" ")
    
      for column in infillcolumns_list:
        
        if process_dict[postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
        != 'boolexclude':

          if iteration == 0:
            
            #stndrdinfill (just prinouts, this was done in processing funcitons)
            if column in postprocess_assigninfill_dict['stdrdinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: stdrdinfill")
                print("")
                

            #zeroinfill:
            if column in postprocess_assigninfill_dict['zeroinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: zeroinfill")
                print("")


              df_test = \
              self.zeroinfillfunction(df_test, column, postprocess_dict, \
                                      masterNArows_test)
              
            #oneinfill:
            if column in postprocess_assigninfill_dict['oneinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: oneinfill")
                print("")

              df_test = \
              self.oneinfillfunction(df_test, column, postprocess_dict, \
                                     masterNArows_test)
              
              
            #adjinfill:
            if column in postprocess_assigninfill_dict['adjinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: adjinfill")
                print("")

              df_test = \
              self.adjinfillfunction(df_test, column, postprocess_dict, \
                                     masterNArows_test)
              
            #medianinfill
            if column in postprocess_assigninfill_dict['medianinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: medianinfill")
                print("")


              #check if column is boolean
              boolcolumn = False
              #exclude boolean and ordinal from this infill method
              if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
              in ['multirt', 'multisp', 'singlct', 'binary', '1010', 'boolexclude', 'concurrent_act']:
                boolcolumn = True

              categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])

              #if (column not in excludetransformscolumns) \
              if (categorylistlength == 1) \
              and boolcolumn is False:
                #noting that currently we're only going to infill 0 for single column categorylists
                #some comparable address for multi-column categories is a future extension

                infillvalue = postprocess_dict['column_dict'][column]['normalization_dict'][column]['infillvalue']


                df_test = \
                self.test_medianinfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)
                

            #meaninfill:
            if column in postprocess_assigninfill_dict['meaninfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: meaninfill")
                print("")


              #check if column is boolean
              boolcolumn = False
              #exclude boolean and ordinal from this infill method
              if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
              in ['multirt', 'multisp', 'singlct', 'binary', '1010', 'boolexclude', 'concurrent_act']:
                boolcolumn = True

              categorylistlength = len(postprocess_dict['column_dict'][column]['categorylist'])

              #if (column not in excludetransformscolumns) \
              if (categorylistlength == 1) \
              and boolcolumn is False:
                #noting that currently we're only going to infill 0 for single column categorylists
                #some comparable address for multi-column categories is a future extension

                infillvalue = postprocess_dict['column_dict'][column]['normalization_dict'][column]['infillvalue']

                df_test = \
                self.test_meaninfillfunction(df_test, column, postprocess_dict, \
                                             masterNArows_test, infillvalue)
                

            #modeinfill:
            if column in postprocess_assigninfill_dict['modeinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: modeinfill")
                print("")


              #check if column is excluded (variable poorly named, interpret boolcolumn here as excluded)
              boolcolumn = False

              if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
              in ['boolexclude', 'concurrent_nmbr']:
                boolcolumn = True


              if boolcolumn is False:

                infillvalue = postprocess_dict['column_dict'][column]['normalization_dict'][column]['infillvalue']


                df_test = \
                self.test_modeinfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)
                
                
            #lcinfill:
            if column in postprocess_assigninfill_dict['lcinfill']:

              #printout display progress
              if printstatus is True:
                print("infill to column: ", column)
                print("     infill type: lcinfill")
                print("")



              #check if column is excluded (variable poorly named, interpret boolcolumn here as excluded)
              boolcolumn = False

              if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
              in ['boolexclude', 'concurrent_nmbr']:
                boolcolumn = True


              if boolcolumn is False:

                #noting that currently we're only going to infill 0 for single column categorylists
                #some comparable address for multi-column categories is a future extension

                infillvalue = postprocess_dict['column_dict'][column]['normalization_dict'][column]['infillvalue']

                #repurpose modeinfillfunction for test, only difference is the passed infillvalue
                df_test = \
                self.test_modeinfillfunction(df_test, column, postprocess_dict, \
                                               masterNArows_test, infillvalue)


          #MLinfill:
          if column in postprocess_assigninfill_dict['MLinfill']:

            #printout display progress
            if printstatus is True:
              print("infill to column: ", column)
              print("     infill type: MLinfill")
              print("")


            df_test, postprocess_dict = \
            self.postMLinfillfunction (df_test, column, postprocess_dict, \
                                       masterNArows_test)


      for columnname in list(df_test):
        postprocess_dict['column_dict'][columnname]['infillcomplete'] = False
      
      iteration += 1
      
      
    return df_test


  def zeroinfillfunction(self, df, column, postprocess_dict, \
                        masterNArows):
    
    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])

    infill = pd.DataFrame(np.zeros((NAcount, 1)), columns=[column])
    
    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase=True)

    
    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df

  def oneinfillfunction(self, df, column, postprocess_dict, \
                        masterNArows):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()
    

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])

    infill = pd.DataFrame(np.ones((NAcount, 1)), columns=[column])
    
    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase=True)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df


  def adjinfillfunction(self, df, column, postprocess_dict, \
                        masterNArows):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()
    
    #create infill dataframe of all nan with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    

    infill = pd.DataFrame(np.zeros((NAcount, 1)), columns=[column])
    infill = infill.replace(0, np.nan)
    
    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase=True)
    
    
    #this is hack
    df[column] = df[column].replace('nan', np.nan)
    
    #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
    df[column] = df[column].fillna(method='ffill')
    
    #we'll follow with a bfill just in case first row had a nan
    df[column] = df[column].fillna(method='bfill')
    
    #and final edge case if all cells were subject to infill we'll just insert 0
    df[column] = df[column].fillna(value=0)
    
    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})

    return df


  def train_medianinfillfunction(self, df, column, postprocess_dict, \
                                 masterNArows):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    #create df without rows that were subject to infill to dervie median
    tempdf = pd.concat([df[column], masterNArows[NArw_columnname]], axis=1)
    #remove rows that were subject to infill
    tempdf = tempdf[tempdf[NArw_columnname] != 1]
    #calculate median of remaining rows
    median = tempdf[column].median()
    
    #edge case
    if median != median:
      median = 0
    
    del tempdf

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, median)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df, median

  def test_medianinfillfunction(self, df, column, postprocess_dict, \
                                 masterNArows, median):


    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()
    
    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    median = median

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, median)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df

  def train_meaninfillfunction(self, df, column, postprocess_dict, \
                                 masterNArows):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()
    
    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    #create df without rows that were subject to infill to dervie median
    tempdf = pd.concat([df[column], masterNArows[NArw_columnname]], axis=1)
    #remove rows that were subject to infill
    tempdf = tempdf[tempdf[NArw_columnname] != 1]
    #calculate median of remaining rows
    mean = tempdf[column].mean()
    
    #edge case
    if mean != mean:
      mean = 0
    
    del tempdf

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, mean)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df, mean


  def test_meaninfillfunction(self, df, column, postprocess_dict, \
                                 masterNArows, mean):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    mean = mean

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, mean)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})
    
    return df




  def train_modeinfillfunction(self, df, column, postprocess_dict, \
                               masterNArows):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    #deriving a mode for one-hot encoded sets requires a different approach
    if len(postprocess_dict['column_dict'][column]['categorylist']) > 1 \
    and postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
    in ['multirt', 'multisp']:
      
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      
      tempdf = pd.concat([df[categorylist], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]
      
      del tempdf[NArw_columnname]
      
      tempdf_mode_dict = {}
      
      #since this is one hot encoded we can count activations in a column with sum
      for tempcolumn in list(tempdf):
        tempdf_mode_dict.update({tempdf[tempcolumn].sum() : tempcolumn})
      
      #create a list of those sums then sort to grab the mode column
      tempdf_mode_dict_keys = list(tempdf_mode_dict)
      tempdf_mode_dict_keys = sorted(tempdf_mode_dict_keys)
      mode_column = tempdf_mode_dict[tempdf_mode_dict_keys[-1]]
      
      if mode_column == column:
        mode = 1
      else:
        mode = 0
        
      del tempdf
      del tempdf_mode_dict
      del tempdf_mode_dict_keys
      del mode_column
      
    #deriving a mode for multi-column binary encoded sets requires a different approach
    elif len(postprocess_dict['column_dict'][column]['categorylist']) > 1 \
    and postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
    in ['1010']:
      
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      
      tempdf = pd.concat([df[categorylist], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]
      
      del tempdf[NArw_columnname]
      
      #initialize a column to store encodings
      tempdf['onehot'] = ''

      #populate column to store aggregated encodings 
      for tempdf_column in list(tempdf):
        if tempdf_column != 'onehot':
          tempdf['onehot'] = \
          tempdf['onehot'] + tempdf[tempdf_column].astype(int).astype(str)

      #find mode of the aggregation
      binary_mode = tempdf['onehot'].mode()
      
      if len(binary_mode) > 0:
        binary_mode = binary_mode[0]
      else:
        binary_mode = tempdf['onehot'].values[0]
      
#       if len(binary_mode) < 1:
#         binary_mode = 0
      
      
      #remove rows other than mode
      tempdf = tempdf[tempdf['onehot'] == binary_mode]
      
      #mode is the current columns value associated with that mode
      mode = tempdf[column].values[0]
      
      del tempdf
      del binary_mode
      
    
    #else if columns were not multi-column
    #note this scenario also includes 'concurrent_act' MLinfilltype
    else:
    
      #create df without rows that were subject to infill to dervie mode
      tempdf = pd.concat([df[column], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]

      
      #calculate mode of remaining rows
      mode = tempdf[column].mode()
      
      if len(mode) > 0:
        mode = mode[0]
      else:
        mode = 0

      del tempdf

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, mode)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']


    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase = True)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})

    return df, mode


  def test_modeinfillfunction(self, df, column, postprocess_dict, \
                              masterNArows, mode):

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    mode = mode

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, mode)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']

#     #insert infill
#     df = self.insertinfill(df, column, infill, category, \
#                            pd.DataFrame(masterNArows[NArw_columnname]), \
#                            postprocess_dict, columnslist = columnslist, \
#                            categorylist = categorylist)

    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase = True)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})

    return df

  
  def train_lcinfillfunction(self, df, column, postprocess_dict, \
                             masterNArows):
    """
    #comparable to modeinfill function but uses least common value instead of most common value
    """

    #copy the datatype to ensure returned set is consistent
    df_temp_dtype = pd.DataFrame(df[column][:1]).copy()

    #create infill dataframe of all zeros with number of rows corepsonding to the
    #number of 1's found in masterNArows
    NArw_columnname = \
    postprocess_dict['column_dict'][column]['origcolumn'] + '_NArows'

    NAcount = len(masterNArows[masterNArows[NArw_columnname] == 1])
    
    #deriving a mode for one-hot encoded sets requires a different approach
    if len(postprocess_dict['column_dict'][column]['categorylist']) > 1 \
    and postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
    in ['multirt', 'multisp']:
      
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      
      tempdf = pd.concat([df[categorylist], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]
      
      del tempdf[NArw_columnname]
      
      tempdf_mode_dict = {}
      
      #since this is one hot encoded we can count activations in a column with sum
      for tempcolumn in list(tempdf):
        tempdf_mode_dict.update({tempdf[tempcolumn].sum() : tempcolumn})
      
      #create a list of those sums then sort to grab the mode column
      tempdf_mode_dict_keys = list(tempdf_mode_dict)
      tempdf_mode_dict_keys = sorted(tempdf_mode_dict_keys)
      #mode_column = tempdf_mode_dict[tempdf_mode_dict_keys[-1]]
      mode_column = tempdf_mode_dict[tempdf_mode_dict_keys[0]]
      
      if mode_column == column:
        mode = 1
      else:
        mode = 0
        
      del tempdf
      del tempdf_mode_dict
      del tempdf_mode_dict_keys
      del mode_column
      
    #deriving a mode for multi-column binary encoded sets requires a different approach
    elif len(postprocess_dict['column_dict'][column]['categorylist']) > 1 \
    and postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
    in ['1010']:
      
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      
      tempdf = pd.concat([df[categorylist], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]
      
      del tempdf[NArw_columnname]
      
      #initialize a column to store encodings
      #note this arbitrary column won't overlap with any
      #because categorylist all have suffixes with '_' character
      tempdf['onehot'] = ''

      #populate column to store aggregated encodings 
      for tempdf_column in list(tempdf):
        if tempdf_column != 'onehot':
          tempdf['onehot'] = \
          tempdf['onehot'] + tempdf[tempdf_column].astype(int).astype(str)

      #find mode of the aggregation
      #binary_mode = tempdf['onehot'].mode()
      mode_valuecounts_list = pd.DataFrame(tempdf['onehot'].value_counts())
      mode_valuecounts_list = mode_valuecounts_list.rename_axis('zzzinfill').sort_values(by = ['onehot', 'zzzinfill'], ascending = [False, True])
      mode_valuecounts_list = list(mode_valuecounts_list.index)
      if len(mode_valuecounts_list) > 0:
        binary_mode = mode_valuecounts_list[-1]
      else:
        binary_mode = 0
      
#       if len(binary_mode) > 0:
#         binary_mode = binary_mode[0]
#       else:
#         binary_mode = tempdf['onehot'].values[0]

      if binary_mode != binary_mode:
        binary_mode = 0
      
#       if len(binary_mode) < 1:
#         binary_mode = 0
      
      
      #remove rows other than mode
      tempdf = tempdf[tempdf['onehot'] == binary_mode]
      
      #mode is the current columns value associated with that mode
      mode = tempdf[column].values[0]
      
      del tempdf
      del binary_mode
      
    
    #else if columns were not multi-column
    #note this scenario also includes 'concurrent_act' MLinfilltype
    else:
    
      #create df without rows that were subject to infill to dervie mode
      tempdf = pd.concat([df[column], masterNArows[NArw_columnname]], axis=1)
      #remove rows that were subject to infill
      tempdf = tempdf[tempdf[NArw_columnname] != 1]

      
      #calculate mode of remaining rows
      mode_valuecounts_list = pd.DataFrame(tempdf[column].value_counts())
      mode_valuecounts_list = mode_valuecounts_list.rename_axis('zzzinfill').sort_values(by = [column, 'zzzinfill'], ascending = [False, True])
      mode_valuecounts_list = list(mode_valuecounts_list.index)
      if len(mode_valuecounts_list) > 0:
        mode = mode_valuecounts_list[-1]
      else:
        mode = 0
      
      if mode != mode:
        mode = 0
      
#       if len(mode) > 0:
#         mode = mode[0]
#       else:
#         mode = 0

      del tempdf

    infill = pd.DataFrame(np.zeros((NAcount, 1)))
    infill = infill.replace(0, mode)

    category = postprocess_dict['column_dict'][column]['category']
    columnslist = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']


    #insert infill
    df = self.insertinfill(df, column, infill, category, \
                           pd.DataFrame(masterNArows[NArw_columnname]), \
                           postprocess_dict, columnslist = columnslist, \
                           categorylist = categorylist, singlecolumncase = True)

    #reset data type to ensure returned data is consistent with what was passed
    df[column] = \
    df[column].astype({column:df_temp_dtype[column].dtypes})

    return df, mode
  

  def populatePCAdefaults(self, randomseed):
    '''
    populates sa dictionary with default values for PCA methods PCA, 
    SparsePCA, and KernelPCA. (Each based on ScikitLearn default values)
    #note that for SparsePCA the 'normalize_components' is not passed 
    #since will be depreciated
    '''

    PCAdefaults = {'PCA':{}, 'SparsePCA':{}, 'KernelPCA':{}}

    PCAdefaults['PCA'].update({'copy':True, \
                               'whiten':False, \
                               'svd_solver':'auto', \
                               'tol':0.0, \
                               'iterated_power':'auto', \
                               'random_state':randomseed})

    PCAdefaults['SparsePCA'].update({'alpha':1, \
                                     'ridge_alpha':0.01, \
                                     'max_iter':1000, \
                                     'tol':1e-08, \
                                     'method':'lars', \
                                     'n_jobs':None, \
                                     'U_init':None, \
                                     'V_init':None, \
                                     'verbose':False, \
                                     'random_state':randomseed})
#                                       , \
#                                      'normalize_components':True})

    PCAdefaults['KernelPCA'].update({'kernel':'linear', \
                                     'gamma':None, \
                                     'degree':3, \
                                     'coef0':1, \
                                     'kernel_params':None, \
                                     'alpha':1.0, \
                                     'fit_inverse_transform':False, \
                                     'eigen_solver':'auto', \
                                     'tol':0, \
                                     'max_iter':None, \
                                     'remove_zero_eig':False, \
                                     'random_state':randomseed, \
                                     'copy_X':True, \
                                     'n_jobs':None})

    return PCAdefaults



  def evalPCA(self, df_train, PCAn_components, ML_cmnd):
    '''
    function serves to evaluate properties of dataframe to determine 
    if an automated application of PCA is appropriate, and if so 
    what kind of PCA to apply
    returns PCActgy as
    'noPCA' -> self explanatory, this is the default when number of features 
                is less than 15% of number of rows
                Please note this is a somewhat arbitrary ratio and some more
                research is needed to validate methods for this rule
                A future iteration may perform addition kinds of evaluations
                such as distribuytions and correlations of data for this method.
    'KernelPCA' -> dataset suitable for automated KernelPCA application
                    (preffered method when data is all non-negative)
    'SparsePCA' -> dataset suitable for automated SparsePCA application
                    (prefered method when data is not all non-negative)
    'PCA' -> not currently used as a default
    also returns a n_components value which is based on the user passed 
    value to PCAn_components or if user passes None (the default) then
    one is assigned based on properties of the data set
    also returns a value for n_components based on that same 15% rule
    where PCA application will default to user passed n_components but if
    none passed will apply this returned value
    '''

    number_rows = df_train.shape[0]
    number_columns = df_train.shape[1]
    
    #ok this is to allow user to set the default columns/rows ratio for automated PCA
    if 'col_row_ratio' in ML_cmnd['PCA_cmnd']:
      col_row_ratio = ML_cmnd['PCA_cmnd']['col_row_ratio']
    else:
      col_row_ratio = 0.50

    if ML_cmnd['PCA_type'] == 'default':

      #if number_columns / number_rows < 0.15:
      if number_columns / number_rows < col_row_ratio:

        if PCAn_components == None:

          PCActgy = 'noPCA'

          n_components = PCAn_components

        if PCAn_components != None:

          #if df_train[df_train < 0.0].count() == 0:
          if any(df_train < 0.0):

            PCActgy = 'SparsePCA'

          #else if there were negative values in the dataframe
          else:

            PCActgy = 'KernelPCA'

          n_components = PCAn_components

      #else if number_columns / number_rows > 0.15
      #else:
      if number_columns / number_rows > col_row_ratio:

        #if df_train[df_train < 0.0].count() == 0:
        #if df_train[df_train < 0.0].sum() == 0:
        if any(df_train < 0.0):

          PCActgy = 'SparsePCA'

        #else if there were negative values in the dataframe
        else:

          PCActgy = 'KernelPCA'

        #if user did not pass a PCAn_component then we'll create one
        if PCAn_components == None:

          #this is a somewhat arbitrary figure, some
          #additional research needs to be performed
          #a future expansion may base this on properties
          #of the data
          #n_components = int(round(0.15 * number_rows))
          n_components = int(round(col_row_ratio * number_rows))

        else:

          n_components = PCAn_components
    
    if isinstance(PCAn_components, (int, float)):
    
      if PCAn_components > 0.0 and PCAn_components < 1.0:
        
        PCActgy = 'PCA'
    
        n_components = PCAn_components

    if ML_cmnd['PCA_type'] != 'default':

      PCActgy = ML_cmnd['PCA_type']

      n_components = PCAn_components
    
    return PCActgy, n_components


  def initSparsePCA(self, ML_cmnd, PCAdefaults, PCAn_components):
    '''
    function that assigns appropriate parameters based on defaults and user inputs
    and then initializes a SparsePCA model
    '''

    #if user passed values use those, otherwise pass scikit defaults
    if 'alpha' in ML_cmnd['PCA_cmnd']:
      alpha = ML_cmnd['PCA_cmnd']['alpha']
    else:
      alpha = PCAdefaults['SparsePCA']['alpha']

    if 'ridge_alpha' in ML_cmnd['PCA_cmnd']:
      ridge_alpha = ML_cmnd['PCA_cmnd']['ridge_alpha']
    else:
      ridge_alpha = PCAdefaults['SparsePCA']['ridge_alpha']

    if 'max_iter' in ML_cmnd['PCA_cmnd']:
      max_iter = ML_cmnd['PCA_cmnd']['max_iter']
    else:
      max_iter = PCAdefaults['SparsePCA']['max_iter']

    if 'tol' in ML_cmnd['PCA_cmnd']:
      tol = ML_cmnd['PCA_cmnd']['tol']
    else:
      tol = PCAdefaults['SparsePCA']['tol']

    if 'method' in ML_cmnd['PCA_cmnd']:
      method = ML_cmnd['PCA_cmnd']['method']
    else:
      method = PCAdefaults['SparsePCA']['method']

    if 'n_jobs' in ML_cmnd['PCA_cmnd']:
      n_jobs = ML_cmnd['PCA_cmnd']['n_jobs']
    else:
      n_jobs = PCAdefaults['SparsePCA']['n_jobs']

    if 'U_init' in ML_cmnd['PCA_cmnd']:
      U_init = ML_cmnd['PCA_cmnd']['U_init']
    else:
      U_init = PCAdefaults['SparsePCA']['U_init']

    if 'V_init' in ML_cmnd['PCA_cmnd']:
      V_init = ML_cmnd['PCA_cmnd']['V_init']
    else:
      V_init = PCAdefaults['SparsePCA']['V_init']

    if 'verbose' in ML_cmnd['PCA_cmnd']:
      verbose = ML_cmnd['PCA_cmnd']['verbose']
    else:
      verbose = PCAdefaults['SparsePCA']['verbose']

    if 'random_state' in ML_cmnd['PCA_cmnd']:
      random_state = ML_cmnd['PCA_cmnd']['random_state']
    else:
      random_state = PCAdefaults['SparsePCA']['random_state']

#     if 'normalize_components' in ML_cmnd['PCA_cmnd']:
#       normalize_components = ML_cmnd['PCA_cmnd']['normalize_components']
#     else:
#       normalize_components = PCAdefaults['SparsePCA']['normalize_components']

    #do other stuff?

    #then train PCA model 
    PCAmodel = SparsePCA(n_components = PCAn_components, \
                         alpha = alpha, \
                         ridge_alpha = ridge_alpha, \
                         max_iter = max_iter, \
                         tol = tol, \
                         method = method, \
                         n_jobs = n_jobs, \
                         U_init = U_init, \
                         V_init = V_init, \
                         verbose = verbose, \
                         random_state = random_state)
#                          , \
#                          normalize_components = normalize_components)

    return PCAmodel




  def initKernelPCA(self, ML_cmnd, PCAdefaults, PCAn_components):
    '''
    function that assigns approrpiate parameters based on defaults and user inputs
    and then initializes a KernelPCA model
    '''

    #if user passed values use those, otherwise pass scikit defaults
    if 'kernel' in ML_cmnd['PCA_cmnd']:
      kernel = ML_cmnd['PCA_cmnd']['kernel']
    else:
      kernel = PCAdefaults['KernelPCA']['kernel']

    if 'gamma' in ML_cmnd['PCA_cmnd']:
      gamma = ML_cmnd['PCA_cmnd']['gamma']
    else:
      gamma = PCAdefaults['KernelPCA']['gamma']

    if 'degree' in ML_cmnd['PCA_cmnd']:
      degree = ML_cmnd['PCA_cmnd']['degree']
    else:
      degree = PCAdefaults['KernelPCA']['degree']

    if 'coef0' in ML_cmnd['PCA_cmnd']:
      coef0 = ML_cmnd['PCA_cmnd']['coef0']
    else:
      coef0 = PCAdefaults['KernelPCA']['coef0']

    if 'kernel_params' in ML_cmnd['PCA_cmnd']:
      kernel_params = ML_cmnd['PCA_cmnd']['kernel_params']
    else:
      kernel_params = PCAdefaults['KernelPCA']['kernel_params']

    if 'alpha' in ML_cmnd['PCA_cmnd']:
      alpha = ML_cmnd['PCA_cmnd']['alpha']
    else:
      alpha = PCAdefaults['KernelPCA']['alpha']

    if 'fit_inverse_transform' in ML_cmnd['PCA_cmnd']:
      fit_inverse_transform = ML_cmnd['PCA_cmnd']['fit_inverse_transform']
    else:
      fit_inverse_transform = PCAdefaults['KernelPCA']['fit_inverse_transform']

    if 'eigen_solver' in ML_cmnd['PCA_cmnd']:
      eigen_solver = ML_cmnd['PCA_cmnd']['eigen_solver']
    else:
      eigen_solver = PCAdefaults['KernelPCA']['eigen_solver']

    if 'tol' in ML_cmnd['PCA_cmnd']:
      tol = ML_cmnd['PCA_cmnd']['tol']
    else:
      tol = PCAdefaults['KernelPCA']['tol']

    if 'max_iter' in ML_cmnd['PCA_cmnd']:
      max_iter = ML_cmnd['PCA_cmnd']['max_iter']
    else:
      max_iter = PCAdefaults['KernelPCA']['max_iter']

    if 'remove_zero_eig' in ML_cmnd['PCA_cmnd']:
      remove_zero_eig = ML_cmnd['PCA_cmnd']['remove_zero_eig']
    else:
      remove_zero_eig = PCAdefaults['KernelPCA']['remove_zero_eig']

    if 'random_state' in ML_cmnd['PCA_cmnd']:
      random_state = ML_cmnd['PCA_cmnd']['random_state']
    else:
      random_state = PCAdefaults['KernelPCA']['random_state']

    if 'copy_X' in ML_cmnd['PCA_cmnd']:
      copy_X = ML_cmnd['PCA_cmnd']['copy_X']
    else:
      copy_X = PCAdefaults['KernelPCA']['copy_X']

    if 'n_jobs' in ML_cmnd['PCA_cmnd']:
      n_jobs = ML_cmnd['PCA_cmnd']['n_jobs']
    else:
      n_jobs = PCAdefaults['KernelPCA']['n_jobs']


    #do other stuff?

    #then train PCA model 
    PCAmodel = KernelPCA(n_components = PCAn_components, \
                         kernel = kernel, \
                         gamma = gamma, \
                         degree = degree, \
                         coef0 = coef0, \
                         kernel_params = kernel_params, \
                         alpha = alpha, \
                         fit_inverse_transform = fit_inverse_transform, \
                         eigen_solver = eigen_solver, \
                         tol = tol, \
                         max_iter = max_iter, \
                         remove_zero_eig = remove_zero_eig, \
                         random_state = random_state, \
                         copy_X = copy_X, \
                         n_jobs = n_jobs)

    return PCAmodel



  def initPCA(self, ML_cmnd, PCAdefaults, PCAn_components):
    '''
    function that assigns approrpiate parameters based on defaults and user inputs
    and then initializes a basic PCA model
    '''

    #run PCA version

    #if user passed values use those, otherwise pass scikit defaults
    if 'copy' in ML_cmnd['PCA_cmnd']:
      copy = ML_cmnd['PCA_cmnd']['copy']
    else:
      copy = PCAdefaults['PCA']['copy']

    if 'whiten' in ML_cmnd['PCA_cmnd']:
      whiten = ML_cmnd['PCA_cmnd']['whiten']
    else:
      whiten = PCAdefaults['PCA']['whiten']

    if 'svd_solver' in ML_cmnd['PCA_cmnd']:
      svd_solver = ML_cmnd['PCA_cmnd']['svd_solver']
    else:
      svd_solver = PCAdefaults['PCA']['svd_solver']

    if 'tol' in ML_cmnd['PCA_cmnd']:
      tol = ML_cmnd['PCA_cmnd']['tol']
    else:
      tol = PCAdefaults['PCA']['tol']

    if 'iterated_power' in ML_cmnd['PCA_cmnd']:
      iterated_power = ML_cmnd['PCA_cmnd']['iterated_power']
    else:
      iterated_power = PCAdefaults['PCA']['iterated_power']

    if 'random_state' in ML_cmnd['PCA_cmnd']:
      random_state = ML_cmnd['PCA_cmnd']['random_state']
    else:
      random_state = PCAdefaults['PCA']['random_state']

    #do other stuff?

    #then train PCA model 
    PCAmodel = PCA(n_components = PCAn_components, \
                   copy = copy, \
                   whiten = whiten, \
                   svd_solver = svd_solver, \
                   tol = tol, \
                   iterated_power = iterated_power, \
                   random_state = random_state)

    return PCAmodel


#   def boolexcl(self, ML_cmnd, df, PCAexcl):
#     """
#     If user passed bool_PCA_excl as True in ML_cmnd['PCA_cmnd']
#     {'PCA_cmnd':{'bool_PCA_excl': True}}
#     Then add boolean columns to the PCAexcl list of columns
#     to be carved out from PCA application
#     Note that PCAexcl may alreadyn be populated with user-passed
#     columns to 4exclude from PCA. The returned bool_PCAexcl list
#     seperately tracks just those columns that were added as part 
#     of this function, in case may be of later use
#     """
#     bool_PCAexcl = []
#     if 'bool_PCA_excl' in ML_cmnd['PCA_cmnd']:
        
#       #if user passed the bool_PCA_excl as True in ML_cmnd['PCA_cmnd'] 
#       if ML_cmnd['PCA_cmnd']['bool_PCA_excl'] is True:
#         for checkcolumn in df:
#           #if column is boolean then add to lists
#           if set(df[checkcolumn].unique()) == {0,1} \
#           or set(df[checkcolumn].unique()) == {0} \
#           or set(df[checkcolumn].unique()) == {1}:
#             PCAexcl.append(checkcolumn)
#             bool_PCAexcl.append(checkcolumn)
            
#     return PCAexcl, bool_PCAexcl

  def boolexcl(self, ML_cmnd, df, PCAexcl, postprocess_dict):
    """
    If user passed bool_PCA_excl as True in ML_cmnd['PCA_cmnd']
    {'PCA_cmnd':{'bool_PCA_excl': True}}
    Then add boolean columns to the PCAexcl list of columns
    to be carved out from PCA application
    If user passed bool_ordl_PCAexcl as True in ML_cmnd['PCA_cmnd']
    Then add ordinal columns (recognized becayuse they are catehgorical)
    to the PCAexcl list of columns
    to be carved out from PCA application
    
    Note that PCAexcl may alreadyn be populated with user-passed
    columns to 4exclude from PCA. The returned bool_PCAexcl list
    seperately tracks just those columns that were added as part 
    of this function, in case may be of later use
    """
    
    bool_PCAexcl = []
    if 'bool_PCA_excl' in ML_cmnd['PCA_cmnd']:
        
      #if user passed the bool_PCA_excl as True in ML_cmnd['PCA_cmnd'] 
      if ML_cmnd['PCA_cmnd']['bool_PCA_excl'] is True:
        for checkcolumn in df:
          #if column is boolean then add to lists
        #   if set(df[checkcolumn].unique()) == {0,1} \
        #   or set(df[checkcolumn].unique()) == {0} \
        #   or set(df[checkcolumn].unique()) == {1}:
          if postprocess_dict['process_dict'][postprocess_dict['column_dict'][checkcolumn]['category']]['MLinfilltype'] \
          in ['multirt', 'multisp', 'binary', '1010', 'boolexclude', 'concurrent_act']:
            if checkcolumn not in PCAexcl:
              PCAexcl.append(checkcolumn)
            bool_PCAexcl.append(checkcolumn)
    
    if 'bool_ordl_PCAexcl' in ML_cmnd['PCA_cmnd']:
      #if user passed the bool_ordl_PCAexcl as True in ML_cmnd['PCA_cmnd'] 
      if ML_cmnd['PCA_cmnd']['bool_ordl_PCAexcl'] is True:
        for checkcolumn in df:
          #if column is boolean then add to lists
        #   if set(df[checkcolumn].unique()) == {0,1} \
        #   or set(df[checkcolumn].unique()) == {0} \
        #   or set(df[checkcolumn].unique()) == {1} \
        #   or checkcolumn[-5:] == '_ordl':
          if postprocess_dict['process_dict'][postprocess_dict['column_dict'][checkcolumn]['category']]['MLinfilltype'] \
          in ['singlct', 'binary', 'multirt', 'multisp', '1010', 'boolexclude', 'concurrent_act']:
            #or isinstance(df[checkcolumn].dtype, pd.api.types.CategoricalDtype):
            if checkcolumn not in PCAexcl:
              PCAexcl.append(checkcolumn)
            bool_PCAexcl.append(checkcolumn)
            
    return PCAexcl, bool_PCAexcl


  def createPCAsets(self, df_train, df_test, PCAexcl, postprocess_dict):
    '''
    Function that takes as input the dataframes df_train and df_test 
    Removes those columns associated with the PCAexcl (which are the original 
    columns passed to automunge which are to be exlcuded from PCA), and returns 
    those sets as PCAset_trian, PCAset_test, and the list of columns extracted as
    PCAexcl_posttransform.
    '''

    #initiate list PCAexcl_postransform
    PCAexcl_posttransform = []

    #derive the excluded columns post-transform using postprocess_dict
    for exclcolumn in PCAexcl:
      
      #if this is one of the original columns (pre-transform)
      if exclcolumn in postprocess_dict['origcolumn']:
      
        #get a column key for this column (used to access stuff in postprofcess_dict)
        exclcolumnkey = postprocess_dict['origcolumn'][exclcolumn]['columnkey']

        #get the columnslist from this columnkey
        exclcolumnslist = postprocess_dict['column_dict'][exclcolumnkey]['columnslist']

        #add these items to PCAexcl_posttransform
        PCAexcl_posttransform.extend(exclcolumnslist)
        
      #if this is a post-transformation column
      elif exclcolumn in postprocess_dict['column_dict']:
        
        #if we hadn't already done another column from the same source
        if exclcolumn not in PCAexcl_posttransform:
          
          #add these items to PCAexcl_posttransform
          PCAexcl_posttransform.extend([exclcolumn])
          
    #assemble the sets by dropping the columns excluded
    PCAset_train = df_train.drop(PCAexcl_posttransform, axis=1)
    PCAset_test = df_test.drop(PCAexcl_posttransform, axis=1)

    return PCAset_train, PCAset_test, PCAexcl_posttransform


  def PCAfunction(self, PCAset_train, PCAset_test, PCAn_components, postprocess_dict, \
                  randomseed, ML_cmnd):
    '''
    Function that takes as input the train and test sets intended for PCA
    dimensionality reduction. Returns a trained PCA model saved in postprocess_dict
    and trasnformed sets.
    '''
    
    
    #initialize ML_cmnd
    #ML_cmnd = postprocess_dict['ML_cmnd']
    ML_cmnd = ML_cmnd
    
    #Find PCA type
    PCActgy, n_components = \
    self.evalPCA(PCAset_train, PCAn_components, ML_cmnd)
    
    #Save the PCActgy to the postprocess_dict
    postprocess_dict.update({'PCActgy' : PCActgy})
    
    #initialize PCA defaults dictionary
    PCAdefaults = \
    self.populatePCAdefaults(randomseed)
    
    #convert PCAsets to numpy arrays
    PCAset_train = PCAset_train.values
    PCAset_test = PCAset_test.values
    
    #initialize a PCA model
    #PCAmodel = PCA(n_components = PCAn_components, random_state = randomseed)
    if PCActgy == 'default' or PCActgy == 'SparsePCA':
  
      #PCAmodel = self.initSparsePCA(ML_cmnd, PCAdefaults, PCAn_components)
      PCAmodel = self.initSparsePCA(ML_cmnd, PCAdefaults, n_components)

    if PCActgy == 'KernelPCA':
  
      #PCAmodel = self.initKernelPCA(ML_cmnd, PCAdefaults, PCAn_components)
      PCAmodel = self.initKernelPCA(ML_cmnd, PCAdefaults, n_components)
    
    if PCActgy == 'PCA':
  
      #PCAmodel = self.initPCA(ML_cmnd, PCAdefaults, PCAn_components)
      PCAmodel = self.initPCA(ML_cmnd, PCAdefaults, n_components)
    
    
    #derive the PCA model (note htis is unsupervised training, no labels)
    PCAmodel.fit(PCAset_train)

    #Save the trained PCA model to the postprocess_dict
    postprocess_dict.update({'PCAmodel' : PCAmodel})

    #apply the transform
    PCAset_train = PCAmodel.transform(PCAset_train)
    PCAset_test = PCAmodel.transform(PCAset_test)

    #get new number of columns
    newcolumncount = np.size(PCAset_train,1)

    #generate a list of column names for the conversion to pandas
    columnnames = ['PCAcol'+str(y) for y in range(newcolumncount)]

    #convert output to pandas
    PCAset_train = pd.DataFrame(PCAset_train, columns = columnnames)
    PCAset_test = pd.DataFrame(PCAset_test, columns = columnnames)

    return PCAset_train, PCAset_test, postprocess_dict, PCActgy
  
  
  def check_am_miscparameters(self, valpercent1, valpercent2, floatprecision, shuffletrain, \
                             TrainLabelFreqLevel, powertransform, binstransform, MLinfill, \
                             infilliterate, randomseed, eval_ratio, LabelSmoothing_train, LabelSmoothing_test, \
                             LabelSmoothing_val, LSfit, numbercategoryheuristic, pandasoutput, \
                             NArw_marker, featureselection, featurepct, featuremetric, \
                             featuremethod, Binary, PCAn_components, PCAexcl, printstatus, excl_suffix):
    """
    #Performs validation to confirm valid entries of passed automunge(.) parameters
    #Note that this function is intended specifically for non-dictionary parameters
    #eg assigncat, assigninfill, assignparam, transformdict, processdict validated elsewhere
    #also note that labels_column, trainID_column, testID_column are checked inside automunge function
    #also note that df_train, df_test, evalcat parameters validation methods still pending
    #returns a dictionary of results
    #False is good
    """
    
    miscparameters_results = {'columnoverlap_valresults':{}}
    
    #check valpercent1
    valpercent1_valresult = False
    if isinstance(valpercent1, (int, float)) and not isinstance(valpercent1, bool):
      if valpercent1 < 0 or valpercent1 >= 1:
        valpercent1_valresult = True
        print("Error: invalid entry passed for valpercent1")
        print("Acceptable values are numbers in range 0 <= valpercent1 < 1.")
        print()
    else:
      valpercent1_valresult = True
      print("Error: invalid entry passed for valpercent1")
      print("Acceptable values are numbers in range 0 <= valpercent1 < 1.")
      print()
      
    miscparameters_results.update({'valpercent1_valresult' : valpercent1_valresult})
    
    #check valpercent2
    valpercent2_valresult = False
    if isinstance(valpercent2, (int, float)) and not isinstance(valpercent2, bool):
      if valpercent2 < 0 or valpercent2 >= 1:
        valpercent2_valresult = True
        print("Error: invalid entry passed for valpercent2")
        print("Acceptable values are numbers in range 0 <= valpercent2 < 1")
        print()
    else:
      valpercent2_valresult = True
      print("Error: invalid entry passed for valpercent2")
      print("Acceptable values are numbers in range 0 <= valpercent2 < 1")
      print()
      
    miscparameters_results.update({'valpercent2_valresult' : valpercent2_valresult})
    
    #check valpercent_sum
    valpercent_sum_valresult = False
    if (isinstance(valpercent1, (int, float)) and not isinstance(valpercent1, bool)) \
    and isinstance(valpercent2, (int, float)) and not isinstance(valpercent2, bool):
      valpercent_sum = valpercent1 + valpercent2
      if valpercent_sum >= 1.0:
        print("Error: invalid entries passed for valpercent1 &/or valpercent2.")
        print("Valid entries for these two paraemters are numbers subject to constraint")
        print("valpercent1 + valpercent2 < 1.0")
        print()
        valpercent_sum_valresult = True
        
    miscparameters_results.update({'valpercent_sum_valresult' : valpercent_sum_valresult})
    
    #check floatprecision
    floatprecision_valresult = False
    if floatprecision not in [16, 32, 64]:
      floatprecision_valresult = True
      print("Error: invalid entry passed for floatprecision parameter.")
      print("Acceptable values are one of {16, 32, 64}")
      print()
      
    miscparameters_results.update({'floatprecision_valresult' : floatprecision_valresult})
    
    #check shuffletrain
    shuffletrain_valresult = False
    if shuffletrain not in [True, False, 'traintest']:
      shuffletrain_valresult = True
      print("Error: invalid entry passed for shuffletrain parameter.")
      print("Acceptable values are one of {True, False, 'traintest'}")
      print()
    elif shuffletrain not in ['traintest'] \
    and not isinstance(shuffletrain, bool):
      shuffletrain_valresult = True
      print("Error: invalid entry passed for shuffletrain parameter.")
      print("Acceptable values are one of {True, False, 'traintest'}")
      print()
      
    miscparameters_results.update({'shuffletrain_valresult' : shuffletrain_valresult})
    
    #check TrainLabelFreqLevel
    TrainLabelFreqLevel_valresult = False
    if TrainLabelFreqLevel not in [True, False, 'test', 'traintest']:
      TrainLabelFreqLevel_valresult = True
      print("Error: invalid entry passed for TrainLabelFreqLevel parameter.")
      print("Acceptable values are one of {True, False, 'test', 'traintest'}")
      print()
    elif TrainLabelFreqLevel not in ['test', 'traintest'] and not isinstance(TrainLabelFreqLevel, bool):
      TrainLabelFreqLevel_valresult = True
      print("Error: invalid entry passed for TrainLabelFreqLevel parameter.")
      print("Acceptable values are one of {True, False, 'test', 'traintest'}")
      print()
      
    miscparameters_results.update({'TrainLabelFreqLevel_valresult' : TrainLabelFreqLevel_valresult})
    
    #check powertransform
    powertransform_valresult = False
    if powertransform not in [True, False, 'excl', 'exc2']:
      powertransform_valresult = True
      print("Error: invalid entry passed for powertransform parameter.")
      print("Acceptable values are one of {True, False, 'excl', 'exc2'}")
      print()
    elif powertransform not in ['excl', 'exc2'] \
    and not isinstance(powertransform, bool):
      powertransform_valresult = True
      print("Error: invalid entry passed for powertransform parameter.")
      print("Acceptable values are one of {True, False, 'excl', 'exc2'}")
      print()
      
    miscparameters_results.update({'powertransform_valresult' : powertransform_valresult})
    
    #check binstransform
    binstransform_valresult = False
    if binstransform not in [True, False] or not isinstance(binstransform, bool):
      binstransform_valresult = True
      print("Error: invalid entry passed for binstransform parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'binstransform_valresult' : binstransform_valresult})
    
    #check MLinfill
    MLinfill_valresult = False
    if MLinfill not in [True, False] or not isinstance(MLinfill, bool):
      MLinfill_valresult = True
      print("Error: invalid entry passed for MLinfill parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'MLinfill_valresult' : MLinfill_valresult})
    
    #check infilliterate
    infilliterate_valresult = False
    if not isinstance(infilliterate, (int)) \
    or isinstance(infilliterate, bool):
      infilliterate_valresult = True
      print("Error: invalid entry passed for infilliterate parameter.")
      print("Acceptable values are integers >= 0")
      print()
    elif infilliterate < 0:
      infilliterate_valresult = True
      print("Error: invalid entry passed for infilliterate parameter.")
      print("Acceptable values are integers >= 0")
      print()
      
    miscparameters_results.update({'infilliterate_valresult' : infilliterate_valresult})
    
    #check randomseed
    randomseed_valresult = False
    if not isinstance(randomseed, (int)) \
    or isinstance(randomseed, bool):
      randomseed_valresult = True
      print("Error: invalid entry passed for randomseed parameter.")
      print("Acceptable values are integers >= 0")
      print()
    elif randomseed < 0:
      randomseed_valresult = True
      print("Error: invalid entry passed for randomseed parameter.")
      print("Acceptable values are integers >= 0")
      print()
      
    miscparameters_results.update({'randomseed_valresult' : randomseed_valresult})
    
    #check eval_ratio
    eval_ratio_valresult = False
    if not (isinstance(eval_ratio, (int)) \
    or isinstance(eval_ratio, (float))):
      eval_ratio_valresult = True
      print("Error: invalid entry passed for eval_ratio parameter.")
      print("Acceptable values are floats 0-1 or integers >1")
      print()
    elif eval_ratio < 0:
      eval_ratio_valresult = True
      print("Error: invalid entry passed for eval_ratio parameter.")
      print("Acceptable values are floats 0-1 or integers >1")
      print()
      
    miscparameters_results.update({'eval_ratio_valresult' : eval_ratio_valresult})
    
    #check LabelSmoothing_train
    LabelSmoothing_train_valresult = False
    if not isinstance(LabelSmoothing_train, (float, bool)):
      
      LabelSmoothing_train_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_train parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_train < 1.0")
      print("Or boolean value of False")
      print()
      
    elif isinstance(LabelSmoothing_train, float) \
    and (LabelSmoothing_train <= 0.0 or LabelSmoothing_train >= 1.0):
      
      LabelSmoothing_train_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_train parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_train < 1.0")
      print("Or boolean value of False")
      print()
      
    elif isinstance(LabelSmoothing_train, bool) \
    and LabelSmoothing_train is not False:
      
      LabelSmoothing_train_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_train parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_train < 1.0")
      print("Or boolean value of False")
      print()
      
    miscparameters_results.update({'LabelSmoothing_train_valresult' : LabelSmoothing_train_valresult})
    
    #check LabelSmoothing_test
    LabelSmoothing_test_valresult = False
    if not isinstance(LabelSmoothing_test, (float, bool)):
      
      LabelSmoothing_test_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_test parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_test < 1.0")
      print("Or boolean value.")
      print()
      
    elif isinstance(LabelSmoothing_test, float) \
    and (LabelSmoothing_test <= 0.0 or LabelSmoothing_test >= 1.0):
      
      LabelSmoothing_test_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_test parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_test < 1.0")
      print("Or boolean value.")
      print()
      
    miscparameters_results.update({'LabelSmoothing_test_valresult' : LabelSmoothing_test_valresult})
    
    #check LabelSmoothing_val
    LabelSmoothing_val_valresult = False
    if not isinstance(LabelSmoothing_val, (float, bool)):
      
      LabelSmoothing_val_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_val parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_val < 1.0")
      print("Or boolean value.")
      print()
      
    elif isinstance(LabelSmoothing_val, float) \
    and (LabelSmoothing_val <= 0.0 or LabelSmoothing_val >= 1.0):
      
      LabelSmoothing_val_valresult = True
      print("Error: invalid entry passed for LabelSmoothing_val parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing_val < 1.0")
      print("Or boolean value.")
      print()
      
    miscparameters_results.update({'LabelSmoothing_val_valresult' : LabelSmoothing_val_valresult})
    
    #check LSfit
    LSfit_valresult = False
    if LSfit not in [True, False] or not isinstance(LSfit, bool):
      LSfit_valresult = True
      print("Error: invalid entry passed for LSfit parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'LSfit_valresult' : LSfit_valresult})
    
    #check numbercategoryheuristic
    numbercategoryheuristic_valresult = False
    if not isinstance(numbercategoryheuristic, int):
      numbercategoryheuristic_valresult = True
      print("Error: invalid entry passed for numbercategoryheuristic parameter.")
      print("Acceptable values are integers >= 1")
      print()
    elif numbercategoryheuristic < 1:
      numbercategoryheuristic_valresult = True
      print("Error: invalid entry passed for numbercategoryheuristic parameter.")
      print("Acceptable values are integers >= 1")
      print()
      
    #check pandasoutput
    pandasoutput_valresult = False
    if pandasoutput not in [True, False] or not isinstance(pandasoutput, bool):
      pandasoutput_valresult = True
      print("Error: invalid entry passed for pandasoutput parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'pandasoutput_valresult' : pandasoutput_valresult})
    
    #check NArw_marker
    NArw_marker_valresult = False
    if NArw_marker not in [True, False] or not isinstance(NArw_marker, bool):
      NArw_marker_valresult = True
      print("Error: invalid entry passed for NArw_marker parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'NArw_marker_valresult' : NArw_marker_valresult})
    

    #check featureselection
    featureselection_valresult = False
    if featureselection not in [True, False] or not isinstance(featureselection, bool):
      featureselection_valresult = True
      print("Error: invalid entry passed for featureselection parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'featureselection_valresult' : featureselection_valresult})
    
    #check featurepct
    featurepct_valresult = False
    if not isinstance(featurepct, float):
      featurepct_valresult = True
      print("Error: invalid entry passed for featurepct parameter.")
      print("Acceptable values are floats within range 0.0 < featurepct < 1.0")
      print()
    elif (featurepct <= 0.0 or featurepct > 1.0):
      featurepct_valresult = True
      print("Error: invalid entry passed for featurepct parameter.")
      print("Acceptable values are floats within range 0.0 < featurepct <= 1.0")
      print()
      
    miscparameters_results.update({'featurepct_valresult' : featurepct_valresult})
    
    #check featuremetric
    featuremetric_valresult = False
    if not isinstance(featuremetric, float):
      featuremetric_valresult = True
      print("Error: invalid entry passed for featuremetric parameter.")
      print("Acceptable values are floats within range 0.0 <= featuremetric < 100.0")
      print()
    elif (featuremetric < 0.0 or featuremetric >= 100.0):
      featuremetric_valresult = True
      print("Error: invalid entry passed for featuremetric parameter.")
      print("Acceptable values are floats within range 0.0 <= featuremetric < 100.0")
      print()
      
    miscparameters_results.update({'featuremetric_valresult' : featuremetric_valresult})
    
    #check featuremethod
    featuremethod_valresult = False
    if featuremethod not in ['pct', 'metric', 'default', 'report']:
      featuremethod_valresult = True
      print("Error: invalid entry passed for featuremethod parameter.")
      print("Acceptable values are one of {'pct', 'metric', 'default', 'report'}")
      print()
      
    miscparameters_results.update({'featuremethod_valresult' : featuremethod_valresult})
  

    #check Binary
    Binary_valresult = False
    if Binary not in [True, False, 'retain']:
      Binary_valresult = True
      print("Error: invalid entry passed for Binary parameter.")
      print("Acceptable values are one of {True, False, 'retain'}")
      print()
    elif Binary not in ['retain'] \
    and not isinstance(Binary, bool):
      Binary_valresult = True
      print("Error: invalid entry passed for Binary parameter.")
      print("Acceptable values are one of {True, False, 'retain'}")
      
    miscparameters_results.update({'Binary_valresult' : Binary_valresult})
      
    
    #check PCAn_components
    #accepts integers >1 or floats between 0-1 or None
    PCAn_components_valresult = False
    if (isinstance(PCAn_components, (int, float)) \
        and not isinstance(PCAn_components, bool)) \
    or PCAn_components == None:
      
      if isinstance(PCAn_components, int):
        if PCAn_components < 1:
          PCAn_components_valresult = True
          print("Error: invalid entry passed for PCAn_components")
          print("Acceptable values are integers > 1, floats between 0-1, or None.")
          print()
        
      if isinstance(PCAn_components, float):
        if (PCAn_components > 1.0 or PCAn_components < 0.0):
          PCAn_components_valresult = True
          print("Error: invalid entry passed for PCAn_components")
          print("Acceptable values are integers > 1, floats between 0-1, or None.")
          print()

    else:
      PCAn_components_valresult = True
      print("Error: invalid entry passed for PCAn_components")
      print("Acceptable values are integers > 1, floats between 0-1, or None.")
      print()
      
    miscparameters_results.update({'PCAn_components_valresult' : PCAn_components_valresult})
      
    
    #check PCAexcl
    #defer on this one for now, this is just a list of column headers to exclude from PCA
    #to validate owuld need to pass list of column headers to this funciton
    
    #check printstatus
    printstatus_valresult = False
    if printstatus not in [True, False] or not isinstance(printstatus, bool):
      printstatus_valresult = True
      print("Error: invalid entry passed for printstatus parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'printstatus_valresult' : printstatus_valresult})
    
    
    #check excl_suffix
    excl_suffix_valresult = False
    if excl_suffix not in [True, False] or not isinstance(excl_suffix, bool):
      excl_suffix_valresult = True
      print("Error: invalid entry passed for excl_suffix parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    miscparameters_results.update({'excl_suffix_valresult' : excl_suffix_valresult})
    
    
    return miscparameters_results
    
  
  
  def check_pm_miscparameters(self, pandasoutput, printstatus, TrainLabelFreqLevel, \
                              featureeval, driftreport, LabelSmoothing, LSfit, \
                              returnedsets, shuffletrain, inversion):
    """
    #Performs validation to confirm valid entries of passed postmunge(.) parameters
    #note one parameter not directly passed is df_test, just pass a list of the columns
    #returns a dictionary of results
    #False is good
    """
    
    pm_miscparameters_results = {}
    
    #check pandasoutput
    pandasoutput_valresult = False
    if pandasoutput not in [True, False] or not isinstance(pandasoutput, bool):
      pandasoutput_valresult = True
      print("Error: invalid entry passed for pandasoutput parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'pandasoutput_valresult' : pandasoutput_valresult})
    
    
    #check printstatus
    printstatus_valresult = False
    if printstatus not in [True, False] or not isinstance(printstatus, bool):
      printstatus_valresult = True
      print("Error: invalid entry passed for printstatus parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'printstatus_valresult' : printstatus_valresult})
    
    
    #check inversion
    inversion_valresult = False
    if inversion not in [False, 'test', 'labels']:
      inversion_valresult = True
      print("Error: invalid entry passed for inversion parameter.")
      print("Acceptable values are one of {False, 'test', 'labels'}")
      print()
    elif inversion not in ['test', 'labels'] \
    and not isinstance(inversion, bool):
      inversion_valresult = True
      print("Error: invalid entry passed for inversion parameter.")
      print("Acceptable values are one of {False, 'test', 'labels'}")
      print()
      
    pm_miscparameters_results.update({'inversion_valresult' : inversion_valresult})
    
    
    #check TrainLabelFreqLevel
    TrainLabelFreqLevel_valresult = False
    if TrainLabelFreqLevel not in [True, False] or not isinstance(TrainLabelFreqLevel, bool):
      TrainLabelFreqLevel_valresult = True
      print("Error: invalid entry passed for TrainLabelFreqLevel parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'TrainLabelFreqLevel_valresult' : TrainLabelFreqLevel_valresult})
    
    
    #check featureeval
    featureeval_valresult = False
    if featureeval not in [True, False] or not isinstance(featureeval, bool):
      featureeval_valresult = True
      print("Error: invalid entry passed for featureeval parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'featureeval_valresult' : featureeval_valresult})
    
    
    #check driftreport
    driftreport_valresult = False
    if driftreport not in [True, False, 'efficient', 'report_effic', 'report_full']:
      driftreport_valresult = True
      print("Error: invalid entry passed for driftreport parameter.")
      print("Acceptable values are one of {True, False, 'efficient', 'report_effic', 'report_full'}")
      print()
    elif driftreport not in ['efficient', 'report_effic', 'report_full'] \
    and not isinstance(driftreport, bool):
      driftreport_valresult = True
      print("Error: invalid entry passed for driftreport parameter.")
      print("Acceptable values are one of {True, False, 'efficient', 'report_effic', 'report_full'}")
      print()
      
    pm_miscparameters_results.update({'driftreport_valresult' : driftreport_valresult})
    
    
    #check LabelSmoothing
    LabelSmoothing_valresult = False
    if not isinstance(LabelSmoothing, (float, bool)):
      
      LabelSmoothing_valresult = True
      print("Error: invalid entry passed for LabelSmoothing parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing < 1.0")
      print("Or boolean value.")
      print()
      
    elif isinstance(LabelSmoothing, float) \
    and (LabelSmoothing <= 0.0 or LabelSmoothing >= 1.0):
      
      LabelSmoothing_valresult = True
      print("Error: invalid entry passed for LabelSmoothing parameter.")
      print("Acceptable values are floats within range 0.0 < LabelSmoothing < 1.0")
      print("Or boolean value.")
      print()
      
    pm_miscparameters_results.update({'LabelSmoothing_valresult' : LabelSmoothing_valresult})
    
    
    #check LSfit
    LSfit_valresult = False
    if LSfit not in [True, False] or not isinstance(LSfit, bool):
      LSfit_valresult = True
      print("Error: invalid entry passed for LSfit parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'LSfit_valresult' : LSfit_valresult})
    
    
    #check returnedsets
    returnedsets_valresult = False
    if returnedsets not in [True, False, 'test_ID', 'test_labels', 'test_ID_labels']:
      returnedsets_valresult = True
      print("Error: invalid entry passed for returnedsets parameter.")
      print("Acceptable values are one of {True, False, 'test_ID', 'test_labels', 'test_ID_labels'}")
      print()
    elif returnedsets not in ['test_ID', 'test_labels', 'test_ID_labels'] \
    and not isinstance(returnedsets, bool):
      returnedsets_valresult = True
      print("Error: invalid entry passed for returnedsets parameter.")
      print("Acceptable values are one of {True, False, 'test_ID', 'test_labels', 'test_ID_labels'}")
      print()
      
    pm_miscparameters_results.update({'returnedsets_valresult' : returnedsets_valresult})
    
    
    #check shuffletrain
    shuffletrain_valresult = False
    if shuffletrain not in [True, False] or not isinstance(shuffletrain, bool):
      shuffletrain_valresult = True
      print("Error: invalid entry passed for shuffletrain parameter.")
      print("Acceptable values are one of {True, False}")
      print()
      
    pm_miscparameters_results.update({'shuffletrain_valresult' : shuffletrain_valresult})
    
    
    return pm_miscparameters_results
    


  def check_assigncat(self, assigncat):
    """
    #Here we'll do a quick check for any redundant column assignments in the
    #assigncat, if any found return an error message
    """

    assigncat_redundant_dict = {}
    result = False

    for assigncatkey1 in sorted(assigncat):
      #assigncat_list.append(set(assigncat[key]))
      current_set = set(assigncat[assigncatkey1])
      redundant_items = {}
      for assigncatkey2 in assigncat:
        if assigncatkey2 != assigncatkey1:
          second_set = set(assigncat[assigncatkey2])
          common = current_set & second_set
          if len(common) > 0:
            for common_item in common:
              if common_item not in assigncat_redundant_dict:
                assigncat_redundant_dict.update({common_item:[assigncatkey1, assigncatkey2]})
              else:
                if assigncatkey1 not in assigncat_redundant_dict[common_item]:
                  assigncat_redundant_dict[common_item].append(assigncatkey1)
                  #assigncat_redundant_dict[common_item] += key1
                if assigncatkey2 not in assigncat_redundant_dict[common_item]:
                  assigncat_redundant_dict[common_item].append(assigncatkey2)
                  #assigncat_redundant_dict[common_item] += key2

    #assigncat_redundant_dict


    if len(assigncat_redundant_dict) > 0:
      result = True
      print("Error, the following columns assigned to multiple root categories in assigncat:")
      for assigncatkey3 in sorted(assigncat_redundant_dict):
        print("")
        print("Column: ", assigncatkey3)
        print("Found in following assigncat entries:")
        print(assigncat_redundant_dict[assigncatkey3])
        print("")

    return result
  
  def check_assigncat2(self, assigncat, transform_dict):
    """
    #Here we'll do a quick check to ensure all of the keys of passed assigncat
    #have corresponding entries in transform_dict, (which may include user
    #passed entries in transformdict parameter)
    
    #a future extension may also do a comparable check comparing assigncat to 
    #entries of process_dict, however note that a process_dict entry for a
    #root category is only required when that root category is also found as
    #an entry to a family tree primitive in the transform_dict    
    
    #(in other words :)
    #All passed assigncat entries require an entry as a root category in transform_dict, 
    #but only those categories also used as entries in the family tree primitives 
    #for a root category in transform_dict require a corresponding entry in process_dict.
    
    #Note that in many cases a root category may be passed as a family tree primitive 
    #entry to it's own family tree set. The root category is used to access the family tree,
    #and the family tree primitive entries are used to access the transform functions and etc.
    """
    
    #False is good
    result = False
    
    for assigncat_key in list(assigncat):
      
      #eval is a special case, it triggers the application of evalcategory
      #which may be neccesary when automated inference turned off with powertransform
      #so it doesn't need a process_dit entry
      if assigncat_key not in list(transform_dict) and assigncat_key not in ['eval', 'ptfm']:
        
        result = True
        
        print("Error, the following entry to user passed assigncat was not found")
        print("to have a corresponding entry in transform_dict.")
        print("")
        print("assigncat key missing transform_dict entry: ", assigncat_key)
        print("")
        print("All passed assigncat entries require an entry as a root category in transform_dict")
        print("(but only those categories also used as entries in the family tree primitives")
        print("for a root category in transform_dict require a corresponding entry in process_dict.)")

    return result
  
  
  def check_assigncat3(self, assigncat, process_dict, transform_dict):
    """
    #Here's we'll do a third check on assigncat
    #to ensure that for any listed root categories, 
    #any category entries to corresponding family tree primitives in process_dict 
    #have a corresponding entry in the transform_dict
    #note that transformdict entries not required for root categories, 
    #unless they are also entries to a family tree
    """
    
    #False is good
    result = False
    
    for assigncat_key in list(assigncat):
      
      if assigncat_key in list(transform_dict):
        
        familytree_entries = []
        
        for familytree_key in transform_dict[assigncat_key]:
          
          familytree_entries += transform_dict[assigncat_key][familytree_key]
          
        for familytree_entry in familytree_entries:
          
          if familytree_entry != None:

            if familytree_entry not in list(process_dict):

              print("Error, the following category was found as an entry")
              print("in a family tree without a corresponding entry ")
              print("in the process_dict.")
              print("")
              print("family tree entry missing process_dict entry: ", familytree_entry)
              print("this entry was passed in the family tree of root category: ", assigncat_key)

              result = True
        
    return result



  def check_assigninfill(self, assigninfill):
    """
    #Here we'll do a quick check for any redundant column assignments in the
    #assigninfill, if any found return an error message
    """

    assigninfill_redundant_dict = {}
    result = False

    for assigninfill_key1 in sorted(assigninfill):
      #assigncat_list.append(set(assigncat[key]))
      current_set = set(assigninfill[assigninfill_key1])
      redundant_items = {}
      for assigninfill_key2 in assigninfill:
        if assigninfill_key2 != assigninfill_key1:
          second_set = set(assigninfill[assigninfill_key2])
          common = current_set & second_set
          if len(common) > 0:
            for common_item in common:
              if common_item not in assigninfill_redundant_dict:
                assigninfill_redundant_dict.update({common_item:[assigninfill_key1, assigninfill_key2]})
              else:
                if assigninfill_key1 not in assigninfill_redundant_dict[common_item]:
                  assigninfill_redundant_dict[common_item].append(assigninfill_key1)
                  #assigncat_redundant_dict[common_item] += key1
                if assigninfill_key2 not in assigninfill_redundant_dict[common_item]:
                  assigninfill_redundant_dict[common_item].append(assigninfill_key2)
                  #assigncat_redundant_dict[common_item] += key2

    #assigncat_redundant_dict


    if len(assigninfill_redundant_dict) > 0:
      result = True
      print("Error, the following columns assigned to multiple root categories in assigninfill:")
      for assigninfill_key3 in sorted(assigninfill_redundant_dict):
        print("")
        print("Column: ", assigninfill_key3)
        print("Found in following assigninfill entries:")
        print(assigninfill_redundant_dict[assigninfill_key3])
        print("")
    
    return result

  def check_transformdict(self, transformdict):
    """
    #Here we'll do a quick check for any entries in the user passed
    #transformdict which don't have at least one replacement column specified
    #and if not found apply a cousins excl transform
    
    #we'll also do a test for excl tranfsorms as replacement primitives
    #and if found move to a corresponding supplement primitive
    """
    
    result1 = False
    result2 = False
    
    for transformkey in sorted(transformdict):
      replacements = len(transformdict[transformkey]['parents']) \
                     + len(transformdict[transformkey]['auntsuncles'])

      if replacements == 0:
        
        if 'excl' not in transformdict[transformkey]['cousins']:
        
          transformdict[transformkey]['cousins'].append('excl')

          result1 = True
          
      #this ensures 'excl' is final transform in the cousins list
      transformdict[transformkey]['cousins'].append('excl')
      transformdict[transformkey]['cousins'].remove('excl')
          
    for transformkey in sorted(transformdict):
      if 'excl' in transformdict[transformkey]['parents'] \
      or 'excl' in transformdict[transformkey]['auntsuncles'] \
      or 'excl' in transformdict[transformkey]['children'] \
      or 'excl' in transformdict[transformkey]['coworkers'] \
      or 'excl' in transformdict[transformkey]['siblings'] \
      or 'excl' in transformdict[transformkey]['friends'] \
      or 'excl' in transformdict[transformkey]['niecesnephews']:
        
        result2 = True
        
        print("Error warning: 'excl' transform found in in family tree ")
        print("of user passed transformdict in root category: ", transformkey)
        print("'excl' transform is an in-place operator ")
        print("and only allowed as an entry in the cousins primitive.")
        print("Please use 'exc6' instead which is a direct pass-through ")
        print("relying on a copy operation instead of in-place operation.")
        print()


    return result1, result2, transformdict
  
  

  def check_transformdict2(self, transformdict):
    """
    #Here we'll do an additional check on transformdict to ensure
    #no redundant specifications in adjacent primitives
    """
    
    
    result1 = False
    result2 = False
    
    upstream_entries = []
    downstream_entries = []
    
    for transformkey in sorted(transformdict):
      
      for primitive in transformdict[transformkey]:
        
        if primitive in ['parents', 'siblings', 'auntsuncles', 'cousins']:  
          
          for entry in transformdict[transformkey][primitive]:
            
            if entry in upstream_entries:
              
              result1 = True
              
              print("error warning: ")
              print("redundant entries found in the upstream primitives ")
              print("for user-passed transformdict key: ", transformkey)
              
            else:
          
              upstream_entries += [entry]
          
        if primitive in ['children', 'niecesnephews', 'coworkers', 'friends']:  
          
          for entry in transformdict[transformkey][primitive]:
            
            if entry in downstream_entries:
              
              result2 = True
              
              print("error warning: ")
              print("redundant entries found in the downstream primitives ")
              print("for user-passed transformdict key: ", transformkey)
              
            else:
          
              downstream_entries += [entry]

      upstream_entries = []
      downstream_entries = []


    return result1, result2
  
  #
  def check_haltingproblem(self, transformdict, transform_dict, max_check_count = 111):
    """
    #evaluates user passed transformdict entries to check for infinite loops
    #we'll arbitrarily check for a max depth of 111 offspring to keep things manageable
    #we'll assume this takes place after user passed entries have been merged into transform_dict
    #such that transformdict is just user passed entries
    #and transform_dict is both user-passed and internal library
    """
    
    haltingproblem_result = False
    
    for root_category in transformdict:
      
      check_count = 0
      
      offspring_list = []
      
      parents_list = \
      transform_dict[root_category]['parents'] + transform_dict[root_category]['siblings']
      
      for parent in parents_list:
        
        upstream_list = []
        upstream_list = [parent]
        
        offspring_list = \
        transform_dict[parent]['children'] + transform_dict[parent]['niecesnephews']
      
        if len(offspring_list) > 0:

          for offspring in offspring_list:

            check_count += 1

            if offspring in upstream_list:

              haltingproblem_result = True

              print("Error, infinite loop detected in transformdict for root category ", root_category)
              print()

              break


            upstream_list += [offspring]

            if check_count < max_check_count:

              offspring_result, check_count = \
              self.check_offspring(transform_dict, offspring, root_category, \
                                   upstream_list, check_count, max_check_count)
#               offspring_result, check_count = \
#               check_offspring(transform_dict, offspring, root_category, \
#                               upstream_list, check_count, max_check_count)

              if offspring_result is True:

                haltingproblem_result = True

                break

            else:

              print("Number of offspring generations for root category ", root_category)
              print("exceeded 111, infinite loop check halted.")
              print()
              
              break
              
    
    return haltingproblem_result
  
  
  #
  def check_offspring(self, transform_dict, root_category, orig_root_category, \
                      upstream_list, check_count, max_check_count):
    """
    #support function for check_haltingproblem
    """
    
    offspring_result = False

    offspring_list = \
    transform_dict[root_category]['children'] + transform_dict[root_category]['niecesnephews']

    if len(offspring_list) > 0:

      for offspring in offspring_list:
        
        check_count += 1

        if offspring in upstream_list:

          offspring_result = True

          print("Error, infinite loop detected in transformdict for root category ", orig_root_category)
          print()

          break

        else:
            
          upstream_list2 = upstream_list.copy() + [offspring]

          if check_count < max_check_count:

            offspring_result2, check_count = \
            self.check_offspring(transform_dict, offspring, orig_root_category, \
                                 upstream_list, check_count, max_check_count)
#             offspring_result2, check_count = \
#             check_offspring(transform_dict, offspring, orig_root_category, \
#                             upstream_list2, check_count, max_check_count)

            if offspring_result2 is True:

              offspring_result = True

              break

          else:

            print("Number of offspring generations for root category ", root_category)
            print("exceeded 111, infinite loop check halted.")
            print()
            
            break
            
    
    return offspring_result, check_count

  def check_ML_cmnd(self, ML_cmnd):
    """
    #Here we'll do a quick check for any entries in the user passed
    #ML_cmnd and add any missing entries with default values
    #a future extension should validate any entries
    """
    
    result = False
    
    if 'MLinfill_type' not in ML_cmnd:
      ML_cmnd.update({'MLinfill_type':'default'})
    if 'MLinfill_cmnd' not in ML_cmnd:
      ML_cmnd.update({'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}})
    if 'PCA_type' not in ML_cmnd:
      ML_cmnd.update({'PCA_type':'default'})
    if 'PCA_cmnd' not in ML_cmnd:
      ML_cmnd.update({'PCA_cmnd':{}})


    return result
  
  
  def check_assignparam(self, assignparam):
    """
    #Here we'll do a quick check to validate the passed assign param.
    #actually currently drawing a blank for what needs validation
    #let's save building out this function for a future extension
    
    #reminder the format of assign_param is e.g.
    #assignparam = {'splt' : {'column1' : {'minsplit' : 4}}, \
    #               'spl2' : {'column2' : {'minsplit' : 3}}}
    """
    
    result = False
    
    return result
  
  def check_normalization_dict(self, postprocess_dict):
    """
    #Double checks that any postprocess_dict['column_dict'] entries for
    #normalization_dict don't have overlaps for those entries where
    #postprocess functions use normalization_dict keys to 
    #retrieve a column key

    #in other words, for set of required unique normalizaiton_dict entry identifiers
    #ensures they are only used in correct transformation categories
    """

    result = False

    required_unique_normalization_dict_entries = \
    {'textlabelsdict_text'  : 'text', \
     'splt_newcolumns_splt' : 'splt', \
     'splt_newcolumns_spl8' : 'spl8', \
     'bn_width_bnwd'        : 'bnwd', \
     'bn_width_bnwK'        : 'bnwK', \
     'bn_width_bnwM'        : 'bnwM', \
     'bincount_bnep'        : 'bnep', \
     'bincount_bne7'        : 'bne7', \
     'bincount_bne9'        : 'bne9', \
     'buckets_bkt1'         : 'bkt1', \
     'buckets_bkt2'         : 'bkt2'}

    for column_dict_entry in postprocess_dict['column_dict']:

      if column_dict_entry in postprocess_dict['column_dict'][column_dict_entry]['normalization_dict']:

        for normalization_dict_entry in postprocess_dict['column_dict'][column_dict_entry]['normalization_dict'][column_dict_entry]:

          if normalization_dict_entry in list(required_unique_normalization_dict_entries):

            if required_unique_normalization_dict_entries[normalization_dict_entry] != \
            postprocess_dict['column_dict'][column_dict_entry]['category']:

              result = True

              print("Warning of potential error for multi-generation family trees ")
              print("from overlap in normalization_dict entry identifiers")
              print("In a few cases for transforms with multicolumn derived sets")
              print("The postprocess functions rely on uniquity of this item to dervie a key to access parameters")
              print("potential overlap found for category ", postprocess_dict['column_dict'][column_dict_entry]['category'])
              print("In regards to normalization_dict entry ", normalization_dict_entry)
              print("Which overlaps with the category ", required_unique_normalization_dict_entries[normalization_dict_entry])
              print("Overlap found for column ", column_dict_entry)


    return result
  
  def check_columnheaders(self, columnheaders_list):
    """
    #Performs a validation that all of the column headers are unique
    """
    
    result = False
    
    if len(columnheaders_list) > len(set(columnheaders_list)):
      
      result = True
      
      print("Warning of potential error from duplicate column headers.")
      print("")
      
    return result
  
  
  def assigncat_str_convert(self, assigncat):
    """
    #Converts all assigncat entries to string (just in case user passed integer)
    #
    #also converts any single string/integer entries passed without list brackets 
    #into a single entry list
    """
    
    #ignore edge case where user passes empty dictionary
    if assigncat != {}:
    
      for assigncatkey in sorted(assigncat):
        current_list = assigncat[assigncatkey]
        
        #check if current_list is a list, if not populate it as single entry in a list
        if type(current_list) != type([]):
          assigncat[assigncatkey] = [current_list]
          current_list = assigncat[assigncatkey]
        
        #then convert any entries in the list to string type
        assigncat[assigncatkey] = [str(i) for i in current_list]

      del current_list

    return assigncat


  def assigninfill_str_convert(self, assigninfill):
    """
    #Converts all assigninfill entries to string (just in case user passed integer)
    #
    #also converts any single string/integer entries passed without list brackets 
    #into a single entry list
    """
    
    #ignore edge case where user passes empty dictionary
    if assigninfill != {}:

      for assigninfillkey in sorted(assigninfill):
        current_list = assigninfill[assigninfillkey]
          
        #check if current_list is a list, if not populate it as single entry in a list
        if type(current_list) != type([]):
          assigninfill[assigninfillkey] = [current_list]
          current_list = assigninfill[assigninfillkey]
        
        #then convert any entries in the list to string type
        assigninfill[assigninfillkey] = [str(i) for i in current_list]

      del current_list

    return assigninfill
  
  
  def parameter_str_convert(self, parameter):
    """
    #Converts parameter, such as one that might be either list or int or str, to a str or list of str
    #where True or False left unchanged
    """

    if isinstance(parameter, int) and str(parameter) != 'False' and str(parameter) != 'True':
      parameter = str(parameter)
    if isinstance(parameter, float):
      parameter = str(parameter)
    if isinstance(parameter, list):
      parameter = [str(i) for i in parameter]

    return parameter
  
  def assignparam_str_convert(self, assignparam):
    """
    #Converts all column entries to assignparam to string in case user passed integer
    #such as for numpy arrays
    #reminder the format of assign_param is e.g.
    #assignparam = {'splt' : {'column1' : {'minsplit' : 4}}, \
    #               'spl2' : {'column2' : {'minsplit' : 3}}}
    """
  
    #ignore edge case where user passes empty dictionary
    if assignparam != {}:
      
      for categorykey in assignparam:
        
        for columnkey in assignparam[categorykey]:
          
          assignparam[categorykey][str(columnkey)] = assignparam[categorykey].pop(columnkey)
          
    return assignparam
  
  def floatprecision_transform(self, df, columnkeylist, floatprecision):
    """
    #floatprecision is a parameter user passed to automunge
    #allowable values are 16/32/64
    #if 64 do nothing (we'll assume our transofrm functions default to 64)
    #if 16 or 32 then check each column in df for columnkeylist and if
    #float convert to this precision
    """
    
    if isinstance(columnkeylist, str):
      columnkeylist = [columnkeylist]
    
    #if floatprecision in [16, 32, 64]:
    if floatprecision in [16, 32]:
      
      for columnkey in columnkeylist:
        
        if pd.api.types.is_float_dtype(df[columnkey]):
          
          if floatprecision == 32:
            df[columnkey] = df[columnkey].astype(np.float32)
            
          elif floatprecision == 16:
            df[columnkey] = df[columnkey].astype(np.float16)
            
#           elif floatprecision == 64:
#             df[columnkey] = df[columnkey].astype(np.float64)

    
    return df
  
#   def assembleassignparam(self):
#     """
#     #just a placeholder function for consistency with other methods
#     """
    
#     assign_param = {}
    
#     return assign_param
  
  def assemble_assign_param(self, assignparam, list_df_train):
    """
    #assignparam is the user passed dictionary
    #list_df_train is a list of source columns passed to automunge
    #returns an assign_param popuated to extract any default_assignparm parameters
    #and populated in the format for accesing using traditional assign_partam format
    """
    #assignparam is the dictionary passed to automunge
    #we'll convert it to assign_param (with underscore) based on any default_assignparam entries
    
    #assignparam follows form
#     assignparam = {'category1' : {'column1' : {'param1' : 123}, 'column2' : {'param1' : 456}}, \
#                    'cateogry2' : {'column3' : {'param2' : 'abc', 'param3' : 'def'}}}
    
    #if any default_assignparam entries they follow form
#     assignparam = {'category1' : {'column1' : {'param1' : 123}, 'column2' : {'param1' : 456}}, \
#                    'category2' : {'column3' : {'param2' : 'abc'}}, \
#                    'default_assignparam' : {'category2' : {'param2' : 'ghi', 'param3' : 'def'}}}

    #such that the default_assignparam entries would be populated as new assigned parameters
    #for all columns that were not otherwise specified 
    #e.g. in this example, because category2 has column3 specified for param2
    #the population with defaults will defer to that specification, however because
    #the default_assignparam for category2 includes another parameter param3, that 
    #param3 will now be included in the assign_param[category2][column3] entry
  
    #initialize assign_param
    assign_param = {}
    
    if 'default_assignparam' in assignparam:
      
      #key1 are category entries passed in assignparam['default_assignparam']
      for key1 in assignparam['default_assignparam']:
        
        if key1 not in list(assignparam):
          
          #for column passed in df_train
          for sourcecolumn in list_df_train:

            if key1 not in assign_param:

              assign_param.update({key1 : {sourcecolumn : assignparam['default_assignparam'][key1]}})

            else:

              assign_param[key1][sourcecolumn] = assignparam['default_assignparam'][key1]
        
        #else if category entry from default_assignparam key1 not a category key in assignparam
        else:

          for sourcecolumn in list_df_train:

            if sourcecolumn in assignparam[key1]:

              #key2 are the column identifiers embedded as subkey to the assignparam categories
              for key2 in assignparam[key1]:

                #if key2 in list_df_train:
                if key2 == sourcecolumn:

                  #key3_set is a set of parameters specified to specifric columns in assignparam
                  key3_set = set(list(assignparam[key1][key2]))

                  #key4_set is a set of default parameters specified in assignparam['default_assignparam']
                  #for each category
                  key4_set = set(list(assignparam['default_assignparam'][key1]))

                  notyetspecifiedparams = key4_set - key3_set

                  #key5 are the parameters not yet specified for this column in this category
                  for key5 in notyetspecifiedparams:

                    assignparam[key1][key2].update({key5 : assignparam['default_assignparam'][key1][key5]})

                    if key1 not in assign_param:

                      assign_param.update({key1 : {sourcecolumn : assignparam[key1][key2]}})

                    else:

                      if key2 in assign_param[key1]:

                        assign_param[key1][key2].update(assignparam[key1][key2])

                      else:
                        
                        assign_param[key1].update({key2 : assignparam[key1][key2]})

            else:
              
              if key1 not in assign_param:
                
                assign_param.update({key1 : {sourcecolumn : assignparam['default_assignparam'][key1]}})

              else:
                
                if sourcecolumn in assign_param[key1]:
                  
                  assign_param[key1][sourcecolumn].update(assignparam['default_assignparam'][key1])
                  
                else:
                  
                  assign_param[key1].update({sourcecolumn : assignparam['default_assignparam'][key1]})
                  

    #else for those assignparam entries not affected by default_assignparam
    #we'll just pass through to assign_param
    for key6 in assignparam:
    
      if key6 not in assign_param and key6 != 'default_assignparam':
        
        assign_param[key6] = assignparam[key6]
        
    return assign_param
  
  def grab_params(self, assign_param, category, column):
    """
    #takes as input the assign_param dictionary, category string and column identifier string
    #checks if there are any entries in assign_param matching the category and column
    #note that the column string may include suffix apeponders
    #reminder the format of assign_param is e.g.
    #assignparam = {'splt' : {'column1' : {'param1_splt' : 4}}, \
    #               'spl2' : {'column2' : {'param1_spl2' : 3}}}
    #returns either an empty dictionary or the associated parameters
    """
    
    columnkeys = []
    params = {}

    #if assign_param is not empty
    if bool(assign_param):

      #category is the category being passed to process functions, let's see if it's in assign_param
      if category in assign_param:

        assign_param_cat = assign_param[category]

        for key in assign_param_cat:

          if column.startswith(key):

            columnkeys.append(key)

        if len(columnkeys) > 0:

          #we'll take the columnkey with longeest length that matches column
          #note this columnkey may include suffix appenders
          columnkeys.sort(key = len, reverse = True)
          columnkey = columnkeys[0]

          params = assign_param_cat[columnkey]
      
    return params
  
  
#   def apply_LabelSmoothing(self, df, column, epsilon, label_categorylist, label_category):
#     """
#     #applies label smoothing based on user passed epsilon 
#     #based on method described in "Rethinking the Inception Architecture for Computer Vision" by Szegedy et al
#     #hat tip to Stack Overflow user lejlot for some implementaiton suggestions
#     # https://stackoverflow.com/questions/39335535/label-smoothing-soft-targets-in-pandas
    
#     #Note that categorylist is the list of columns originating from same transformation
#     #and we currently exlcude '1010' binary encoded sets from the method
#     #a future extension will address 1010 for MLinfill by smoothing after conversion to one-hot encoding 
#     #such as for MLinfill
#     """
    
#     #if labelsmoothingcolumn is boolean and not binary encoded via 1010
#     if (set(df[column].unique()) == {0,1} \
#     or set(df[column].unique()) == {0} \
#     or set(df[column].unique()) == {1}) \
#     and label_category != '1010' \
#     and len(label_categorylist) > 1:

#       Smoothing_K = len(label_categorylist)
      
#       #==1 value corresponds to 'bnry' encoding, >1 corresonds to one-hot encodings eg 'text'
#       if Smoothing_K > 1:
#         Smoothing_K -= 1

#       df[column] = \
#       df[column] * (epsilon) + \
#       (1 - df[column]) * (1 - epsilon) / Smoothing_K
      
#     return df
  
#   def apply_LabelSmoothing(self, df, targetcolumn, epsilon, label_categorylist, label_category, categorycomplete_dict, LSfit):
#     """
#     #applies label smoothing based on user passed epsilon 
    
#     #if LSfit is False
#     #based on method described in "Rethinking the Inception Architecture for Computer Vision" by Szegedy et al
#     #hat tip to Stack Overflow user lejlot for some implementaiton suggestions
#     # https://stackoverflow.com/questions/39335535/label-smoothing-soft-targets-in-pandas
    
#     #if LSfit is True
#     #based on extension wherein the Smoothing factor K for each column is fit to the
#     #distribution of the set is is a function of the activation column and target column
    
#     #we'll follow convention that if LSfit is False we'll only apply LS to current column
#     #and if LSfit is True we'll apply to all columns in categorylist
#     #and return a diciotnary indicating which columns have recieved
#     #(dicitonary categorycomplete_dict initialized external to function)
    
#     #Note that categorylist is the list of columns originating from same transformation
#     #and we currently exlcude '1010' binary encoded sets from the method
#     #a future extension will address 1010 for MLinfill by smoothing after conversion to one-hot encoding 
#     #such as for MLinfill
#     """
    
# #     unique_set = set(pd.unique(df[label_categorylist].values.ravel('K')))
    
  
# #     if (unique_set == {0,1} \
# #     or unique_set == {0} \
# #     or unique_set == {1}) \
# #     and label_category != '1010' \
# #     and len(label_categorylist) > 1:

      
#     if LSfit is True:
      
#       unique_set = set(pd.unique(df[label_categorylist].values.ravel('K')))
      
#       #plus let's check if this is one-hot encoded (vs like binary encoded or something)
#       #this is a proxy for testing if category is '1010'
#       onehot = True
#       if df[label_categorylist].sum().sum() != df.shape[0]:
#         onehot = False

#       if (unique_set == {0,1} \
#       or unique_set == {0} \
#       or unique_set == {1}) \
#       and onehot is True \
#       and len(label_categorylist) > 1:
        
#         #if LSfit is True we'll apply LS to all columns in categorylist

#         #activation_dcit will track the count of activations for each column in the categorylist
#         activation_dict = {}

#         for column1 in label_categorylist:

#           activations = df[column1].sum()

#           activation_dict.update({column1 : activations})

#         #LS_dict will be where we keep track of the activation distributions associated with each column
#         #note we'll need activation ratios for each column as a function of activated column for a row
#         LS_dict = {}
#         for column1 in label_categorylist:

#           LS_dict.update({column1 : {}})

#           total_activations = 0

#           for column2 in label_categorylist:

#             if column1 != column2:

#               total_activations += activation_dict[column2]

#           for column2 in label_categorylist:

#             if column1 != column2:

#               LS_dict[column1].update({column2 : activation_dict[column2] / total_activations})


#         for column1 in label_categorylist:

#           for column2 in label_categorylist:

#             if column2 != column1:
              
#               Smoothing_K = LS_dict[column2][column1]

#               df[column1] = \
#               np.where(df[column2] == 1, (1 - epsilon) * Smoothing_K, df[column1].values)

#         for column1 in label_categorylist:

#           df[column1] = \
#           np.where(df[column1]==1, df[column1] * (epsilon), df[column1].values)

#           categorycomplete_dict[column1] = True
    
    
#     #if LSfit is not True:
#     #else we'll only apply LS to the passed column with assumption of level distribution for K
#     else:
      
#       unique_set = set(pd.unique(df[targetcolumn]))
      
#       #plus let's check if this is one-hot encoded (vs like binary encoded or something)
#       #this is a proxy for testing if category is '1010'
#       onehot = True
#       if df[label_categorylist].sum().sum() != df.shape[0]:
#         onehot = False

#       if (unique_set == {0,1} \
#       or unique_set == {0} \
#       or unique_set == {1}) \
#       and onehot is True \
#       and len(label_categorylist) > 1:
        
#         for column1 in label_categorylist:
        
#           Smoothing_K = len(label_categorylist)

#           #==1 value corresponds to 'bnry' encoding, >1 corresonds to one-hot encodings eg 'text'
#           if Smoothing_K > 1:
#             Smoothing_K -= 1

#           df[column1] = \
#           df[column1] * (epsilon) + (1 - df[column1]) * (1 - epsilon) / Smoothing_K

#           categorycomplete_dict[column1] = True

    
#     return df, categorycomplete_dict


  def apply_LabelSmoothing(self, df, targetcolumn, epsilon, label_categorylist, label_category, categorycomplete_dict, LSfit, LSfitparams_dict):
    """
    #applies label smoothing based on user passed epsilon 
    
    #if LSfit is False
    #based on method described in "Rethinking the Inception Architecture for Computer Vision" by Szegedy et al
    #hat tip to Stack Overflow user lejlot for some implementaiton suggestions
    # https://stackoverflow.com/questions/39335535/label-smoothing-soft-targets-in-pandas
    
    #if LSfit is True
    #based on extension wherein the Smoothing factor K for each column is fit to the
    #distribution of the set is is a function of the activation column and target column
    
    #we'll follow convention that in both cases label smoothing applied to all columns in categorylist
    #and return a diciotnary indicating which columns have recieved
    #(dictionary categorycomplete_dict initialized external to function)
    
    #we'll also return a dictionary containing any dervied parameters for LSfit such as for
    #subsequent consistent encoding
    
    #Note that categorylist is the list of columns originating from same transformation
    #and we currently exlcude '1010' binary encoded sets from the method
    #a future extension will address 1010 for MLinfill by smoothing after conversion to one-hot encoding 
    #such as for MLinfill
    """
    
#     unique_set = set(pd.unique(df[label_categorylist].values.ravel('K')))
    
  
#     if (unique_set == {0,1} \
#     or unique_set == {0} \
#     or unique_set == {1}) \
#     and label_category != '1010' \
#     and len(label_categorylist) > 1:
    
#     LSfitparams_dict = {}
  
    #initialize store of derived parameters
    for column1 in label_categorylist:
      
      LSfitparams_dict.update({column1 : {'LSfit' : LSfit, \
                                          'epsilon' : epsilon, \
                                          'label_categorylist' : label_categorylist, \
                                          'label_category' : label_category}})
    
    
    if LSfit is True:
      
      unique_set = set(pd.unique(df[label_categorylist].values.ravel('K')))
      
      #plus let's check if this is one-hot encoded (vs like binary encoded or something)
      #this is a proxy for testing if category is '1010'
      onehot = True
      #if df[label_categorylist].sum().sum() != df.shape[0]:
      if df[label_categorylist].sum().sum() > df.shape[0]:
        onehot = False

      if (unique_set == {0,1} \
      or unique_set == {0} \
      or unique_set == {1}) \
      and onehot is True \
      and len(label_categorylist) > 1:
        
        #if LSfit is True we'll apply LS to all columns in categorylist

        #activation_dcit will track the count of activations for each column in the categorylist
        activation_dict = {}

        for column1 in label_categorylist:

          activations = df[column1].sum()

          activation_dict.update({column1 : activations})
        
        #populate this in params dictionary
        for column1 in label_categorylist:
          
          LSfitparams_dict[column1].update({'activation_dict' : activation_dict})

        #LS_dict will be where we keep track of the activation distributions associated with each column
        #note we'll need activation ratios for each column as a function of activated column for a row
        LS_dict = {}
        for column1 in label_categorylist:

          LS_dict.update({column1 : {}})

          total_activations = 0

          for column2 in label_categorylist:

            if column1 != column2:

              total_activations += activation_dict[column2]

          for column2 in label_categorylist:

            if column1 != column2:

              LS_dict[column1].update({column2 : activation_dict[column2] / total_activations})

                                   
        #populate this in params dictionary
        for column1 in label_categorylist:
          
          LSfitparams_dict[column1].update({'LS_dict' : LS_dict})

        for column1 in label_categorylist:

          for column2 in label_categorylist:

            if column2 != column1:
              
              Smoothing_K = LS_dict[column2][column1]

              df[column1] = \
              np.where(df[column2] == 1, (1 - epsilon) * Smoothing_K, df[column1].values)

        for column1 in label_categorylist:

          df[column1] = \
          np.where(df[column1]==1, df[column1] * (epsilon), df[column1].values)

          categorycomplete_dict[column1] = True
    
    
    #if LSfit is not True:
    #else we'll only apply LS to the passed column with assumption of level distribution for K
    else:
      
      unique_set = set(pd.unique(df[targetcolumn]))
      
      #plus let's check if this is one-hot encoded (vs like binary encoded or something)
      #this is a proxy for testing if category is '1010'
      onehot = True
      #if df[label_categorylist].sum().sum() != df.shape[0]:
      if df[label_categorylist].sum().sum() > df.shape[0]:
        onehot = False

      if (unique_set == {0,1} \
      or unique_set == {0} \
      or unique_set == {1}) \
      and onehot is True \
      and len(label_categorylist) > 1:
        
        for column1 in label_categorylist:
        
          Smoothing_K = len(label_categorylist)

          #==1 value corresponds to 'bnry' encoding, >1 corresonds to one-hot encodings eg 'text'
          if Smoothing_K > 1:
            Smoothing_K -= 1

          df[column1] = \
          df[column1] * (epsilon) + (1 - df[column1]) * (1 - epsilon) / Smoothing_K

          categorycomplete_dict[column1] = True

    
    return df, categorycomplete_dict, LSfitparams_dict
                          
                          
  def postapply_LabelSmoothing(self, df, targetcolumn, categorycomplete_dict, LSfitparams_dict):
    """
    #applies label smoothing based on user passed LSfitparams_dict
    #consiostently to label smoothing from corresponding train data
    
    #if LSfit is False
    #based on method described in "Rethinking the Inception Architecture for Computer Vision" by Szegedy et al
    #hat tip to Stack Overflow user lejlot for some implementaiton suggestions
    # https://stackoverflow.com/questions/39335535/label-smoothing-soft-targets-in-pandas
    
    #if LSfit is True
    #based on extension wherein the Smoothing factor K for each column is fit to the
    #distribution of the set is is a function of the activation column and target column
    
    #we'll follow convention that in both cases label smoothing applied to all columns in categorylist
    #and return a diciotnary indicating which columns have recieved
    #(dictionary categorycomplete_dict initialized external to function)
    
    #we'll also return a dictionary containing any dervied parameters for LSfit such as for
    #subsequent consistent encoding
    
    #Note that categorylist is the list of columns originating from same transformation
    #and we currently exlcude '1010' binary encoded sets from the method
    #a future extension will address 1010 for MLinfill by smoothing after conversion to one-hot encoding 
    #such as for MLinfill
    """

    #grab passed parameters from LSfitparams_dict
    LSfit = LSfitparams_dict[targetcolumn]['LSfit']
    epsilon = LSfitparams_dict[targetcolumn]['epsilon']
    label_categorylist = LSfitparams_dict[targetcolumn]['label_categorylist']
    label_category = LSfitparams_dict[targetcolumn]['label_category']
    
    
    if LSfit is True:
      
      unique_set = set(pd.unique(df[label_categorylist].values.ravel('K')))
      
      #plus let's check if this is one-hot encoded (vs like binary encoded or something)
      #this is a proxy for testing if category is '1010'
      onehot = True
      #if df[label_categorylist].sum().sum() != df.shape[0]:
      if df[label_categorylist].sum().sum() > df.shape[0]:
        onehot = False

      if (unique_set == {0,1} \
      or unique_set == {0} \
      or unique_set == {1}) \
      and onehot is True \
      and len(label_categorylist) > 1:
        
#         activation_dict = LSfitparams_dict[targetcolumn]['activation_dict']
        LS_dict = LSfitparams_dict[targetcolumn]['LS_dict']
        
        #if LSfit is True we'll apply LS to all columns in categorylist

#         #activation_dcit will track the count of activations for each column in the categorylist
#         activation_dict = {}

#         for column1 in label_categorylist:

#           activations = df[column1].sum()

#           activation_dict.update({column1 : activations})
        
#         #populate this in params dictionary
#         for column1 in label_categorylist:
          
#           LSfitparams_dict.update({column1 : {'activation_dict' : activation_dict}})

#         #LS_dict will be where we keep track of the activation distributions associated with each column
#         #note we'll need activation ratios for each column as a function of activated column for a row
#         LS_dict = {}
#         for column1 in label_categorylist:

#           LS_dict.update({column1 : {}})

#           total_activations = 0

#           for column2 in label_categorylist:

#             if column1 != column2:

#               total_activations += activation_dict[column2]

#           for column2 in label_categorylist:

#             if column1 != column2:

#               LS_dict[column1].update({column2 : activation_dict[column2] / total_activations})

                                   
#         #populate this in params dictionary
#         for column1 in label_categorylist:
          
#           LSfitparams_dict[column1].update({'LS_dict' : LS_dict})

        for column1 in label_categorylist:

          for column2 in label_categorylist:

            if column2 != column1:
              
              Smoothing_K = LS_dict[column2][column1]

              df[column1] = \
              np.where(df[column2] == 1, (1 - epsilon) * Smoothing_K, df[column1].values)

        for column1 in label_categorylist:

          df[column1] = \
          np.where(df[column1]==1, df[column1] * (epsilon), df[column1].values)

          categorycomplete_dict[column1] = True
    
    
    #if LSfit is not True:
    #else we'll only apply LS to the passed column with assumption of level distribution for K
    else:
      
      unique_set = set(pd.unique(df[targetcolumn]))
      
      #plus let's check if this is one-hot encoded (vs like binary encoded or something)
      #this is a proxy for testing if category is '1010'
      onehot = True
      #if df[label_categorylist].sum().sum() != df.shape[0]:
      if df[label_categorylist].sum().sum() > df.shape[0]:
        onehot = False

      if (unique_set == {0,1} \
      or unique_set == {0} \
      or unique_set == {1}) \
      and onehot is True \
      and len(label_categorylist) > 1:
        
        for column1 in label_categorylist:
        
          Smoothing_K = len(label_categorylist)

          #==1 value corresponds to 'bnry' encoding, >1 corresonds to one-hot encodings eg 'text'
          if Smoothing_K > 1:
            Smoothing_K -= 1

          df[column1] = \
          df[column1] * (epsilon) + (1 - df[column1]) * (1 - epsilon) / Smoothing_K

          categorycomplete_dict[column1] = True

    
    return df, categorycomplete_dict
  
  
  def Binary_convert(self, df_train, df_test, bool_column_list, Binary):
    """
    #Binary_convert takes as input a processed dataframe and a list of boolean encoded columns
    #and applies a dimensionality reduction on the boolean set as a binary encodiong
    
    #returns the set wtih a reduced number of columns and a dictionary containing the
    #parameters of conversion
    
    #note that infill has already been applied on these columns so no need for infill
    #on train set (but yes on test set)
    """
    
    df_train['Binary'] = ''
    df_test['Binary'] = ''
  
    for column in bool_column_list:
  
      df_train['Binary'] = df_train['Binary'] + df_train[column].astype(str)
      df_test['Binary'] = df_test['Binary'] + df_test[column].astype(str)
      
    #this step ensures thqt infill is all zeros since we rely on sort
    df_train['Binary'] = 'B_' + df_train['Binary']
    df_test['Binary'] = 'B_' + df_test['Binary']
    
    
    #now we'll apply process_1010_class
    df_train, df_test, Binary_column_dict_list = \
    self.process_1010_class(df_train, df_test, 'Binary', '1010', {}, {})
    
    Binary_dict = {'column_dict' : {}}
    
    for column_dict in Binary_column_dict_list:
      
      Binary_dict['column_dict'].update(column_dict)
      
    Binary_dict.update({'bool_column_list' : bool_column_list})
    
    del df_train['Binary']
    del df_test['Binary']
    
    #we won't delete the origin columns if Binary passed as 'retain'
    #(such that the binary encoding is a supplement instead of a replacement)
    if Binary not in ['retain']:

      for column in bool_column_list:

        del df_train[column]
        del df_test[column]
    
    
    return df_train, df_test, Binary_dict
  
  
  
  def postBinary_convert(self, df_test, Binary_dict, Binary):
    """
    #Binary_convert takes as input a processed dataframe and a list of boolean encoded columns
    #and applies a dimensionality reduction on the boolean set as a binary encodiong
    
    #returns the set wtih a reduced number of columns and a dictionary containing the
    #parameters of conversion
    
    #note that infill has already been applied on these columns so no need for infill
    #on train set (but yes on test set)
    """
    
    bool_column_list = Binary_dict['bool_column_list']    
    
    df_test['Binary'] = ''
    
    for column in bool_column_list:
  
      df_test['Binary'] = df_test['Binary'] + df_test[column].astype(str)
    
    #this step ensures thqt infill is all zeros since we rely on sort
    df_test['Binary'] = 'B_' + df_test['Binary']
    
    #now we'll apply postprocess_1010_class
    df_test = self.postprocess_1010_class(df_test, 'Binary', Binary_dict, 'columnkey', {})
    
    del df_test['Binary']
    
    #we won't delete the origin columns if Binary passed as 'retain'
    #(such that the binary encoding is a supplement instead of a replacement)
    if Binary not in ['retain']:
      
      for column in bool_column_list:

        del df_test[column]
    
    return df_test
  
  def convert_inf_to_nan(self, df):
    """
    #converts all np.inf values in a dataframe to np.nan
    #similar to pandas pd.options.mode.use_inf_as_na = True
    #except that it works
    """
    
    df[df == np.inf] = np.nan
    df[df == -np.inf] = np.nan
    
    return df
  
  def df_split(self, df, ratio, shuffle_param, randomseed):
    """
    #performs a split of passed dataframe df
    #based on proportions of ratio where 0<ratio<1
    #bool shuffle False means rows taken from bottom of set sequentially
    #bool shuffle True means randomly selected rows 
    #per seeding of randomseed
    #(if run on two df's with same number of rows, will return consistent partitioning)
    #returns two dataframes df1 and df2
    """

    if ratio > 0 and ratio < 1:

      start = int(df.shape[0] * (1-ratio))
      end = df.shape[0]

      if shuffle_param is True:
        df = self.df_shuffle(df, randomseed)

      df1 = df[0:start]
      df2 = df[start:end]

    else:

      df1 = df
      df2 = pd.DataFrame()

    return df1, df2
    
  
  def df_shuffle(self, df, randomseed):
    """
    #Shuffles the rows of a dataframe
    #per seeding of randomseed
    """
    
    df = df.sample(frac=1, random_state=randomseed)
    
    return df  
  
  def df_shuffle_series(self, df, column, randomseed):
    """
    #Shuffles single column in a dataframe
    """
    
    df_temp = df[column].copy()
    df_temp = self.df_shuffle(df_temp, randomseed)
    df[column] = df_temp.values
    
    del df_temp
    
    return df
  
  
  def automunge(self, df_train, df_test = False, \
                labels_column = False, trainID_column = False, testID_column = False, \
                valpercent1=0.0, valpercent2 = 0.0, floatprecision = 32, shuffletrain = True, \
                TrainLabelFreqLevel = False, powertransform = False, binstransform = False, \
                MLinfill = False, infilliterate=1, randomseed = 42, eval_ratio = .5, \
                LabelSmoothing_train = False, LabelSmoothing_test = False, LabelSmoothing_val = False, LSfit = False, \
                numbercategoryheuristic = 63, pandasoutput = False, NArw_marker = False, \
                featureselection = False, featurepct = 1.0, featuremetric = 0.0, featuremethod = 'default', \
                Binary = False, PCAn_components = None, PCAexcl = [], excl_suffix = False, \
                ML_cmnd = {'MLinfill_type':'default', \
                           'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}, \
                           'PCA_type':'default', \
                           'PCA_cmnd':{}}, \
                assigncat = {'nmbr':[], 'retn':[], 'mnmx':[], 'mean':[], 'MAD3':[], 'lgnm':[], \
                             'bins':[], 'bsor':[], 'pwr2':[], 'por2':[], 'bxcx':[], \
                             'addd':[], 'sbtr':[], 'mltp':[], 'divd':[], \
                             'log0':[], 'log1':[], 'logn':[], 'sqrt':[], 'rais':[], 'absl':[], \
                             'bnwd':[], 'bnwK':[], 'bnwM':[], 'bnwo':[], 'bnKo':[], 'bnMo':[], \
                             'bnep':[], 'bne7':[], 'bne9':[], 'bneo':[], 'bn7o':[], 'bn9o':[], \
                             'bkt1':[], 'bkt2':[], 'bkt3':[], 'bkt4':[], \
                             'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], 'tlbn':[], \
                             'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \
                             'mea2':[], 'mea3':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \
                             'dxdt':[], 'd2dt':[], 'd3dt':[], 'dxd2':[], 'd2d2':[], 'd3d2':[], \
                             'nmdx':[], 'nmd2':[], 'nmd3':[], 'mmdx':[], 'mmd2':[], 'mmd3':[], \
                             'bnry':[], 'text':[], 'txt2':[], 'txt3':[], '1010':[], 'or10':[], \
                             'ordl':[], 'ord2':[], 'ord3':[], 'ord4':[], 'om10':[], 'mmor':[], \
                             'Utxt':[], 'Utx2':[], 'Utx3':[], 'Uor3':[], 'Uor6':[], 'U101':[], \
                             'splt':[], 'spl2':[], 'spl5':[], 'sp15':[], \
                             'spl8':[], 'spl9':[], 'sp10':[], 'sp16':[], \
                             'srch':[], 'src2':[], 'src4':[], 'strn':[], 'lngt':[], \
                             'nmrc':[], 'nmr2':[], 'nmr3':[], 'nmcm':[], 'nmc2':[], 'nmc3':[], \
                             'nmr7':[], 'nmr8':[], 'nmr9':[], 'nmc7':[], 'nmc8':[], 'nmc9':[], \
                             'ors2':[], 'ors5':[], 'ors6':[], 'ors7':[], 'ucct':[], 'Ucct':[], \
                             'or11':[], 'or12':[], 'or15':[], 'or17':[], 'or19':[], 'or20':[], \
                             'date':[], 'dat2':[], 'dat6':[], 'wkdy':[], 'bshr':[], 'hldy':[], \
                             'wkds':[], 'wkdo':[], 'mnts':[], 'mnto':[], \
                             'yea2':[], 'mnt2':[], 'mnt6':[], 'day2':[], 'day5':[], \
                             'hrs2':[], 'hrs4':[], 'min2':[], 'min4':[], 'scn2':[], \
                             'excl':[], 'exc2':[], 'exc3':[], 'exc4':[], 'exc5':[], 'exc6':[], \
                             'null':[], 'copy':[], 'shfl':[], 'eval':[], 'ptfm':[]}, \
                assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \
                                'adjinfill':[], 'meaninfill':[], 'medianinfill':[], \
                                'modeinfill':[], 'lcinfill':[]}, \
                assignparam = {'default_assignparam' : {'(category)' : {'(parameter)' : 42}}, \
                                        '(category)' : {'(column)'   : {'(parameter)' : 42}}}, \
                transformdict = {}, processdict = {}, evalcat = False, \
                printstatus = True):
    """
    #This function documented in READ ME, available online at:
    # https://github.com/Automunge/AutoMunge/blob/master/README.md
    """
    
    application_timestamp = dt.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
    application_number = random.randint(100000000000,999999999999)
    indexcolumn = 'Automunge_index_' + str(application_number)
    trainID_column_orig = trainID_column
    testID_column_orig = testID_column


    #quick conversion of any assigncat and assigninfill entries to str (such as for cases if user passed integers)
    assigncat = self.assigncat_str_convert(assigncat)
    assigninfill = self.assigninfill_str_convert(assigninfill)
    assignparam = self.assignparam_str_convert(assignparam)

    #similarily, quick conversion of any passed column idenitfiers to str
    labels_column = self.parameter_str_convert(labels_column)
    trainID_column = self.parameter_str_convert(trainID_column)
    testID_column = self.parameter_str_convert(testID_column)

    #quick check to ensure each column only assigned once in assigncat and assigninfill
    check_assigncat_result = self.check_assigncat(assigncat)
    check_assigninfill_result = self.check_assigninfill(assigninfill)
    check_ML_cmnd_result = self.check_ML_cmnd(ML_cmnd)
    check_assignparam = self.check_assignparam(assignparam)

    #check the range of parameters 
    #(generally speaking other than passed dictionaries, dataframes, or column identifiers)
    miscparameters_results = \
    self.check_am_miscparameters(valpercent1, valpercent2, floatprecision, shuffletrain, \
                                 TrainLabelFreqLevel, powertransform, binstransform, MLinfill, \
                                 infilliterate, randomseed, eval_ratio, LabelSmoothing_train, LabelSmoothing_test, \
                                 LabelSmoothing_val, LSfit, numbercategoryheuristic, pandasoutput, \
                                 NArw_marker, featureselection, featurepct, featuremetric, \
                                 featuremethod, Binary, PCAn_components, PCAexcl, printstatus, excl_suffix)

    miscparameters_results.update({'check_assigncat_result' : check_assigncat_result, \
                                   'check_assigninfill_result' : check_assigninfill_result, \
                                   'check_ML_cmnd_result' : check_ML_cmnd_result, \
                                   'check_assignparam' : check_assignparam})
    


    #initialize processing dicitonaries
    transform_dict = self.assembletransformdict(binstransform, NArw_marker)

    if bool(transformdict) is not False:

      check_transformdict_result1, check_transformdict_result2, transformdict = \
      self.check_transformdict(transformdict)

      miscparameters_results.update({'check_transformdict_result1' : check_transformdict_result1, \
                                     'check_transformdict_result2' : check_transformdict_result2})
      
      check_transformdict2_result1, check_transformdict2_result2 = \
      self.check_transformdict2(transformdict)
      
      miscparameters_results.update({'check_transformdict2_result1' : check_transformdict2_result1, \
                                     'check_transformdict2_result2' : check_transformdict2_result2})
      
      
  #       #first print a notification if we are overwriting anything
  #       for keytd in list(transformdict.keys()):
  #         #keytd = key
  #         if keytd in list(transform_dict.keys()):
  #           print("Note that a key in the user passed transformdict already exists in library")
  #           print("Overwriting entry for trasnformdict key ", keytd)

      #now update the trasnformdict
      transform_dict.update(transformdict)
      
    #check for infinite loops in user passed transformdict
    check_haltingproblem_result = \
    self.check_haltingproblem(transformdict, transform_dict, max_check_count = 111)
    
    miscparameters_results.update({'check_haltingproblem_result' : check_haltingproblem_result})

    #initialize process_dict
    process_dict = self.assembleprocessdict()
    
    #Special case if we are running Binary dimensionality reduction for boolean sets
    #we can replace all '1010' with 'text'
    #I think this may have efficiency improvements but not positive, 
    #(need to run some tests to validate though, pending)
    if Binary is True:
      transform_dict['1010'] = transform_dict['text']
      
      #we'll also have default that if running Binary transform boolean columns 
      #excluded from any PCA unless otherwise specified
      if 'PCA_cmnd' not in ML_cmnd:
        ML_cmnd.update({'PCA_cmnd':{'bool_PCA_excl':True}})
      else:
        if 'bool_PCA_excl' not in ML_cmnd['PCA_cmnd']:
          ML_cmnd['PCA_cmnd'].update({'bool_PCA_excl':True})

    if bool(processdict) is not False:

  #       #first print a notification if we are overwriting anything
  #       for keypd in list(processdict.keys()):
  #         #keypd = key
  #         if keypd in list(processdict.keys()):
  #           print("Note that a key in the user passed processdict already exists in library")
  #           print("Overwriting entry for processdict key ", keypd)

      #now update the processdict
      process_dict.update(processdict)
      
    #here we confirm that all of the keys of assigncat have corresponding entries in process_dict
    check_assigncat_result2 = self.check_assigncat2(assigncat, transform_dict)
    
    #now double check that any category entries in the assigncat have populated family trees
    #with categories that all have entries in the transform_dict
    check_assigncat_result3 = self.check_assigncat3(assigncat, process_dict, transform_dict)
    
    miscparameters_results.update({'check_assigncat_result2' : check_assigncat_result2, \
                                   'check_assigncat_result3' : check_assigncat_result3})


    #feature selection analysis performed here if elected
    if featureselection is True:

      if labels_column is False:
        print("featureselection not available without labels_column in training set")
        
        madethecut = []
        FSmodel = False
        FScolumn_dict = {}
        FS_sorted = {}

      else:
        madethecut, FSmodel, FScolumn_dict, FS_sorted = \
        self.featureselect(df_train, labels_column, trainID_column, \
                          powertransform, binstransform, randomseed, \
                          numbercategoryheuristic, assigncat, transformdict, \
                          processdict, featurepct, featuremetric, featuremethod, \
                          ML_cmnd, process_dict, valpercent1, valpercent2, printstatus, NArw_marker, \
                          assignparam)

      #if featuremethod is report then no further processing just return the results
      if featuremethod == 'report':


        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Feature Importance results returned")
          print("")
          print("_______________")
          print("Automunge Complete")
          print("")


        return [], [], [], \
        [], [], [], \
        [], [], [], \
        [], [], [], \
        [], [], [],  \
        FScolumn_dict, FS_sorted




    else:

      madethecut = []
      FSmodel = False
      FScolumn_dict = {}
      FS_sorted = {}


    #printout display progress
    if printstatus is True:
      print("_______________")
      print("Begin Automunge processing")
      print("")


    #functionality to support passed numpy arrays
    #if passed object was a numpy array, convert to pandas dataframe
    checknp = np.array([])
    if isinstance(checknp, type(df_train)):
      df_train = pd.DataFrame(df_train)
    if isinstance(checknp, type(df_test)):
      df_test = pd.DataFrame(df_test)


    #this converts any numeric columns labels, such as from a passed numpy array, to strings
    trainlabels=[]
    for column in list(df_train):
      trainlabels.append(str(column))
    df_train.columns = trainlabels
    
    #confirm all unique column headers
    check_columnheaders_result = \
    self.check_columnheaders(list(df_train))

    miscparameters_results.update({'check_columnheaders_result' : check_columnheaders_result})
    
        
    #if user passes as True labels_column passed based on final column (including single column scenario)
    #labels_column = True
    if labels_column is True:
      labels_column = trainlabels[-1]
    
    #populate the assign_param now that we've coverted column labels to strings
    if bool(assignparam) is not False:

      #assemble the assign_param
      assign_param = self.assemble_assign_param(assignparam, trainlabels)
      
    else:
      
      assign_param = assignparam
    


    #we'll introduce convention that if df_test provided as False then we'll create
    #a dummy set derived from df_train's first rows
    #test_plug_marker used to identify that this step was taken
    test_plug_marker = False
    if not isinstance(df_test, pd.DataFrame):
#       df_test = df_train[0:10].copy()
      df_test = df_train[0:1].copy()
      testID_column = trainID_column
      test_plug_marker = True
      if labels_column is not False:
        del df_test[labels_column]
        

    #this converts any numeric columns labels, such as from a passed numpy array, to strings
    testlabels=[]
    for column in list(df_test):
      testlabels.append(str(column))
    df_test.columns = testlabels

    #copy input dataframes to internal state so as not to edit exterior objects
    #this helps with recursion in feature importance
#     if inplace is False:
    df_train = df_train.copy()
    df_test = df_test.copy()

#     elif inplace is True:
#       pass

    
    #we'll have convention that if testID_column=False, if trainID_column in df_test etc
    trainID_columns_in_df_test = False
    if testID_column is False:
      if trainID_column is not False:
        trainID_columns_in_df_test = True
        if isinstance(trainID_column, list):
          for trainIDcolumn in trainID_column:
            if trainIDcolumn not in list(df_test):
              trainID_columns_in_df_test = False
              break
        elif isinstance(trainID_column, str):
          if trainID_column not in list(df_test):
            trainID_columns_in_df_test = False
    if trainID_columns_in_df_test is True:
      testID_column = trainID_column


    if type(df_train.index) != pd.RangeIndex:
      #if df_train.index.names == [None]:
      if None in df_train.index.names:
        if len(list(df_train.index.names)) == 1 and df_train.index.dtype == int:
          pass
        elif len(list(df_train.index.names)) == 1 and df_train.index.dtype != int:
          print("error, non integer index passed without columns named")
        else:
          print("error, non integer index passed without columns named")
      else:
        if trainID_column is False:
          trainID_column = []
        elif isinstance(trainID_column, str):
          trainID_column = [trainID_column]
        elif not isinstance(trainID_column, list):
          print("error, trainID_column allowable values are False, string, or list")
        trainID_column = trainID_column + list(df_train.index.names)
        df_train = df_train.reset_index(drop=False)

    if type(df_test.index) != pd.RangeIndex:
      #if df_train.index.names == [None]:
      if None in df_test.index.names:
        if len(list(df_test.index.names)) == 1 and df_test.index.dtype == int:
          pass
        elif len(list(df_test.index.names)) == 1 and df_test.index.dtype != int:
          print("error, non integer index passed without columns named")
        else:
          print("error, non integer index passed without columns named")
      else:
        if testID_column is False:
          testID_column = []
        elif isinstance(testID_column, str):
          testID_column = [testID_column]
        elif not isinstance(testID_column, list):
          print("error, testID_column allowable values are False, string, or list")
        testID_column = testID_column + list(df_test.index.names)
        df_test = df_test.reset_index(drop=False)



    #here we derive a range integer index for inclusion in the ID sets
    df_train_tempID = pd.DataFrame({indexcolumn:range(0,df_train.shape[0])})
    tempIDlist = []
    
    #extract the ID columns from train and test set
    if trainID_column is not False:
      df_trainID = pd.DataFrame(df_train[trainID_column])

      if isinstance(trainID_column, str):
        trainID_column = [trainID_column]
      elif isinstance(trainID_column, list):
        trainID_column = trainID_column
      else:
        print("error, trainID_column value must be False, str, or list")
        
      
      df_train_tempID.index = df_trainID.index
        
      df_trainID = pd.concat([df_trainID, df_train_tempID], axis=1)
    
      for IDcolumn in trainID_column:
        del df_train[IDcolumn]
      
      #then append the indexcolumn to trainID_column list for use in later methods
      trainID_column.append(indexcolumn)
      
    else:
      df_train_tempID.index = df_train.index
      df_trainID = df_train_tempID.copy()
      trainID_column = [indexcolumn]
      
    del df_train_tempID
    
    

    #here we derive a range integer index for inclusion in the test ID sets
    tempIDlist = []
    df_test_tempID = pd.DataFrame({indexcolumn:range(0,df_test.shape[0])})
    
    #extract the ID columns from train and test set
#     if test_plug_marker is False:
    #decided to do this stuff even if there's a dummy set for df_test 
    #to ensure downstream stuff works
    if testID_column is not False:
      df_testID = pd.DataFrame(df_test[testID_column])

      if isinstance(testID_column, str):
        testID_column = [testID_column]
      elif isinstance(testID_column, list):
        testID_column = testID_column
      else:
        print("error, testID_column value must be False, str, or list")
      
      df_test_tempID.index = df_testID.index
      
      df_testID = pd.concat([df_testID, df_test_tempID], axis=1)

      for IDcolumn in testID_column:
        del df_test[IDcolumn]
      
      #then append the indexcolumn to testID_column list for use in later methods
      testID_column.append(indexcolumn)
      
    else:
      df_test_tempID.index = df_test.index
      df_testID = df_test_tempID.copy()
      testID_column = [indexcolumn]

    del df_test_tempID


    #carve out the validation rows

    #set randomness seed number
    answer = randomseed

    #ok now carve out the validation rows. We'll process these later
    #(we're processing train data from validation data seperately to
    #ensure no leakage)

    totalvalidationratio = valpercent1 + valpercent2

    if totalvalidationratio > 0.0:

      val2ratio = valpercent2 / totalvalidationratio
      
      if shuffletrain in [True, 'traintest']:
        shuffle_param=True
      else:
        shuffle_param=False

      #we'll wait to split out the validation labels
      df_train, df_validation1 = \
      self.df_split(df_train, totalvalidationratio, shuffle_param, randomseed)


      if trainID_column is not False:
        df_trainID, df_validationID1 = \
        self.df_split(df_trainID, totalvalidationratio, shuffle_param, randomseed)


      else:
        df_trainID = pd.DataFrame()
        df_validationID1 = pd.DataFrame()


      df_train = df_train.reset_index(drop=True)
      df_validation1 = df_validation1.reset_index(drop=True)
      df_trainID = df_trainID.reset_index(drop=True)
      df_validationID1 = df_validationID1.reset_index(drop=True)

    #else if total validation was <= 0.0
    else:
      df_validation1 = pd.DataFrame()
      df_validationID1 = pd.DataFrame()



    #extract labels from train set
    #an extension to this function could be to delete the training set rows\
    #where the labels are missing or improperly formatted prior to performing\
    #this step
    #initialize a helper 
    labelspresenttrain = False
    labelspresenttest = False
    single_train_column_labels_case = False

    #wasn't sure where to put this seems as a good place as any
    if labels_column is False:
      labelsencoding_dict = {}

    if labels_column is not False:
      df_labels = pd.DataFrame(df_train[labels_column])


      del df_train[labels_column]
      labelspresenttrain = True
      
      #if we only had one (label) column to begin with we'll create a dummy train set
      if df_train.shape[1] == 0:
#         df_train = df_labels[0:10].copy()
        df_train = df_labels[0:1].copy()
        single_train_column_labels_case = True

      #if the labels column is present in test set too
      if labels_column in list(df_test):
        df_testlabels = pd.DataFrame(df_test[labels_column])
        del df_test[labels_column]
        labelspresenttest = True

        
#         #if we only had one (label) column to begin with we'll create a dummy test set
#         if df_test.shape[1] == 0:
#           df_test = df_testlabels[0:10].copy()

    if labelspresenttrain is False:
      df_labels = pd.DataFrame()
    if labelspresenttest is False:

      #we'll introduce convention that if no df_testlabels we'll create
      #a dummy set derived from df_label's first rows
#       df_testlabels = df_labels[0:10].copy()
      df_testlabels = df_labels[0:1].copy()
      
        
    #if we only had one (label) column to begin with we'll create a dummy test set
    if df_test.shape[1] == 0:
#       df_test = df_testlabels[0:10].copy()
      df_test = df_testlabels[0:1].copy()


    #confirm consistency of train an test sets

    #check number of columns is consistent
    if df_train.shape[1] != df_test.shape[1]:
      print("error, different number of columns in train and test sets")
      print("(This assesment excludes labels and ID columns.)")
      return

    #check column headers are consistent (this works independent of order)
    columns_train = set(list(df_train))
    columns_test = set(list(df_test))
    if columns_train != columns_test:
      print("error, different column labels in the train and test set")
      print("(This assesment excludes labels and ID columns.)")
      return

    column_labels_count = len(list(df_train))
    unique_column_labels_count = len(set(list(df_train)))
    if unique_column_labels_count < column_labels_count:
      print("error, redundant column labels found, each column requires unique label")
      return

    columns_train = list(df_train)
    columns_test = list(df_test)
    if columns_train != columns_test:
      print("error, different order of column labels in the train and test set")
      print("(This assesment excludes labels and ID columns.)")
      return

    #extract column lists again but this time as a list
    columns_train = list(df_train)
    columns_test = list(df_test)


    #create an empty dataframe to serve as a store for each column's NArows
    #the column id's for this df will follow convention from NArows of 
    #column+'_NArows' for each column in columns_train
    #these are used in the ML infill methods
    masterNArows_train = pd.DataFrame()
    masterNArows_test = pd.DataFrame()

    #create an empty dictionary to serve as store for categorical transforms lists
    #of associated columns
    multicolumntransform_dict = {}
    
    #create empty dictionary for cases where otherwise not populated with labels
    LSfitparams_dict = {}
    
    #create an empty dictionary to serve as a store of processing variables from \
    #processing that were specific to the train dataset. These can be used for \
    #future processing of a later test set without the need to reprocess the \
    #original train. The dictionary will be populated with an entry for each \
    #column post processing, and will contain a column specific and category \
    #specific (i.e. nmbr, bnry, text, date) set of variable.
    postprocess_dict = {'column_dict' : {}, 'origcolumn' : {}, \
                        'process_dict' : process_dict, \
                        'printstatus' : printstatus, \
                        'randomseed' : randomseed }
    
    #mirror assigncat which will populate the returned categories from eval function
    final_assigncat = deepcopy(assigncat)
    
    #create empty dictionary to serve as store for drift metrics
    drift_dict = {}
    
    #Automunge currently is based on convention that all np.inf are treated as np.nan
    #for purposes of infill
    df_train = self.convert_inf_to_nan(df_train)
    df_labels = self.convert_inf_to_nan(df_labels)
    df_test = self.convert_inf_to_nan(df_test)
    df_testlabels = self.convert_inf_to_nan(df_testlabels)
    
    #For each column, determine appropriate processing function
    #processing function will be based on evaluation of train set
    for column in columns_train:

      #re-initialize the column specific dictionary for later insertion into
      #our postprocess_dict
      column_dict = {}


      #
      categorycomplete = False

      if bool(assigncat) is True:

        for key in assigncat:
          if column in assigncat[key]:
            category = key
            category_test = key
            categorycomplete = True

            #printout display progress
            if printstatus is True:
              print("evaluating column: ", column)

            #special case, if user assigned column to 'eval' then we'll run evalcategory
            #passing a False for powertransform parameter
            if key in ['eval']:
              if evalcat is False:
                category = self.evalcategory(df_train, column, randomseed, eval_ratio, \
                                             numbercategoryheuristic, False, False)
              elif type(evalcat) == types.FunctionType:
                category = evalcat(df_train, column, randomseed, eval_ratio, \
                                   numbercategoryheuristic, False, False)
              else:
                print("error: evalcat must be passed as either False or as a defined function per READ ME")
              
              category_test = category

            
            #or for 'ptfm' passing a True for powertransform parameter
            if key in ['ptfm']:
              if evalcat is False:
                category = self.evalcategory(df_train, column, randomseed, eval_ratio, \
                                             numbercategoryheuristic, True, False)
              elif type(evalcat) == types.FunctionType:
                category = evalcat(df_train, column, randomseed, eval_ratio, \
                                   numbercategoryheuristic, True, False)
              else:
                print("error: evalcat must be passed as either False or as a defined function per READ ME")

              category_test = category


      #
      if categorycomplete is False:

        #printout display progress
        if printstatus is True:
          print("evaluating column: ", column)

        if evalcat is False:
          category = self.evalcategory(df_train, column, randomseed, eval_ratio, \
                                       numbercategoryheuristic, powertransform, False)
        elif type(evalcat) == types.FunctionType:
          category = evalcat(df_train, column, randomseed, eval_ratio, \
                             numbercategoryheuristic, powertransform, False)
        else:
          print("error: evalcat must be passed as either False or as a defined function per READ ME")
          
        #populate the result in the final_assigncat as informational resource
        if category in final_assigncat:
          final_assigncat[category].append(column)
        else:
          final_assigncat.update({category:[column]})


      #Previously had a few methods here to validate consistensy of data between train
      #and test sets. Found it was introducing too much complexity and was having trouble
      #keeping track of all the edge cases. So let's just make outright assumption that
      #test data if passed is consistently formatted as train data (for now)
      #added benefit that this reduces running time

      ##
      #to support the postprocess_dict entry below, let's first create a temp
      #list of columns
      templist1 = list(df_train)

      #create NArows (column of True/False where True coresponds to missing data)
      trainNArows, drift_dict = self.getNArows(df_train, column, category, postprocess_dict, drift_dict=drift_dict, driftassess=True)
      testNArows = self.getNArows(df_test, column, category, postprocess_dict)

      #now append that NArows onto a master NA rows df
      masterNArows_train = pd.concat([masterNArows_train, trainNArows], axis=1)
      masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)

      #printout display progress
      if printstatus is True:
        print("processing column: ", column)
        print("    root category: ", category)


      ##
      #now process family
      df_train, df_test, postprocess_dict = \
      self.processfamily(df_train, df_test, column, category, category, process_dict, \
                        transform_dict, postprocess_dict, assign_param)
      ##
      #this is the validation to ensure no overlap error
      templist3 = list(df_train)
      overlapkeylist = list(set(templist3) - set(templist1))
      if len(set(overlapkeylist) & set(columns_train)) > 0:
        print("*****************")
        print("Warning of potential error")
        print("The set of columns returned from transformations applied to column ", column)
        print("Has an overlap with column headers for those columns originally passed to automunge(.):")
        print(set(overlapkeylist) & set(columns_train))
        print("")
        print("Some potential quick fixes for this error include:")
        print("- rename columns to integers before passing to automunge(.)")
        print("- strip underscores '_' from column header titles (convention is all suffix appenders include an underscore)")
        print("")
        print("Please note any updates to column headers will need to be carried through to assignment parameters.")
        print("*****************")
        print("")

        miscparameters_results['columnoverlap_valresults'].update({column : {'result' : True, \
                                                                             'overlap' : set(overlapkeylist) & set(columns_train)}})
      ##
      else:

        miscparameters_results['columnoverlap_valresults'].update({column : {'result' : False, \
                                                                             'overlap' : set()}})

      ##
      #now delete columns that were subject to replacement
      df_train, df_test, postprocess_dict = \
      self.circleoflife(df_train, df_test, column, category, category, process_dict, \
                        transform_dict, postprocess_dict)
      ##
      #here's another templist to support the postprocess_dict entry below
      templist2 = list(df_train)

      #ok now we're going to pick one of the new entries in templist2 to serve 
      #as a "columnkey" for pulling datas from the postprocess_dict down the road
      #columnkeylist = list(set(templist2) - set(templist1))[0]
      #columnkeylist = list(set(templist2) - set(templist1))
      columnkeylist = []
      for templist2_entry in templist2:
        if templist2_entry not in templist1:
          columnkeylist.append(templist2_entry)

#       #now we'll apply the floatprecision transformation
#       df_train = self.floatprecision_transform(df_train, columnkeylist, floatprecision)
#       df_test = self.floatprecision_transform(df_test, columnkeylist, floatprecision)

      ##
      #so last line I believe returns string if only one entry, so let's run a test
      if isinstance(columnkeylist, str):
        columnkey = columnkeylist
      else:
        #if list is empty
        if len(columnkeylist) == 0:
          columnkey = column
        else:
          columnkey = columnkeylist[0]

      ##
      #we're going to create an entry to postprocess_dict to
      #store a columnkey for each of the original columns
      postprocess_dict['origcolumn'].update({column : {'category' : category, \
                                                       'columnkeylist' : columnkeylist, \
                                                       'columnkey' : columnkey}})

#           for newcolumn in postprocess_dict['origcolumn'][column]['columnkeylist']:
#             postprocess_dict['newcolumn'].update({newcolumn : {'origcolumn' : column}})
      ##
      #printout display progress
      if printstatus is True:
        print(" returned columns:")
        print(postprocess_dict['origcolumn'][column]['columnkeylist'])
        print("")




    #ok here's where we address labels

    if labels_column is not False:        

      #for now we'll just assume consistent processing approach for labels as for data
      #a future extension may segregate this approach

      #initialize processing dicitonaries (we'll use same as for train set)
      #a future extension may allow custom address for labels
      labelstransform_dict = transform_dict

      labelsprocess_dict = process_dict

      #we'll allow user to assign category to labels as well via assigncat call
      categorycomplete = False

      if bool(assigncat) is True:

        for key in assigncat:
          if labels_column in assigncat[key]:
            labelscategory = key
            categorycomplete = True

            #printout display progress
            if printstatus is True:
              print("______")
              print("")
              print("evaluating label column: ", labels_column)
              
            #special case, if user assigned column to 'eval' then we'll run evalcategory
            #passing a False for powertransform parameter
            if key in ['eval']:
              if evalcat is False:
                category = self.evalcategory(df_labels, labels_column, randomseed, eval_ratio, \
                                             numbercategoryheuristic, False, True)
              elif type(evalcat) == types.FunctionType:
                category = evalcat(df_labels, labels_column, randomseed, eval_ratio, \
                                   numbercategoryheuristic, False, True)
              else:
                print("error: evalcat must be passed as either False or as a defined function per READ ME")

              labelscategory = category
              
            #or for 'ptfm' passing a True for powertransform parameter
            if key in ['ptfm']:
              if evalcat is False:
                category = self.evalcategory(df_labels, labels_column, randomseed, eval_ratio, \
                                             numbercategoryheuristic, True, True)
              elif type(evalcat) == types.FunctionType:
                category = evalcat(df_labels, labels_column, randomseed, eval_ratio, \
                                   numbercategoryheuristic, True, True)
              else:
                print("error: evalcat must be passed as either False or as a defined function per READ ME")

              labelscategory = category
              

      if categorycomplete is False:
        
        #ok this isnt; exactly elegant, but going to add a special case for LabelSmoothing
        #first we[ll determine if labelsmoothing present
        labelsmoothingpresent = False
        if (LabelSmoothing_train > 0.0 and LabelSmoothing_train < 1.0 and str(LabelSmoothing_train) != 'False') \
        or (LabelSmoothing_test > 0.0 and LabelSmoothing_test < 1.0 and str(LabelSmoothing_test) != 'False') \
        or (LabelSmoothing_val > 0.0 and LabelSmoothing_val < 1.0 and str(LabelSmoothing_val) != 'False'):
          labelsmoothingpresent = True

        #printout display progress
        if printstatus is True:
          print("______")
          print("")
          print("evaluating label column: ", labels_column)

        #determine labels category and apply appropriate function
        #labelscategory = self.evalcategory(df_labels, labels_column, numbercategoryheuristic, powertransform)

        #we'll follow convention that default powertransform option not applied to labels
        #user can apply instead by passing column to ptfm in assigncat
        if evalcat is False:
          labelscategory = self.evalcategory(df_labels, labels_column, randomseed, eval_ratio, \
                                             numbercategoryheuristic, False, True)
        elif type(evalcat) == types.FunctionType:
          labelscategory = evalcat(df_labels, labels_column, randomseed, eval_ratio, \
                                   numbercategoryheuristic, False, True)
        else:
          print("error: evalcat must be passed as either False or as a defined function per READ ME")
          
        #special case for label smoothing we'll replace evaluated category bnry with text
        #label smoothing doesn't work with bnry, needs one hot encoding
        if labelsmoothingpresent and labelscategory == 'bnry':
          labelscategory = 'text'
          
        #populate the result in the final_assigncat as informational resource
        if labelscategory in final_assigncat:
          final_assigncat[labelscategory].append(labels_column)
        else:
          final_assigncat.update({labelscategory:[labels_column]})
        

        #this now moved into evalcategory function:
  #         #we've previously introduced the convention that for default numeric label sets
  #         #we forgo z-score normalization, here let's make that distinction
  #         if labelscategory in ['nmbr']:
  #           labelscategory = 'exc3'

      #printout display progress
      if printstatus is True:

        print("processing label column: ", labels_column)
        print("    root label category: ", labelscategory)
        print("")



      #initialize a dictionary to serve as the store between labels and their \
      #associated encoding
      labelsencoding_dict = {labelscategory:{}}

      #to support the postprocess_dict entry below, let's first create a temp
      #list of columns
      templist1 = list(df_labels)


      #now process family
      df_labels, df_testlabels, postprocess_dict = \
      self.processfamily(df_labels, df_testlabels, labels_column, labelscategory, labelscategory, \
                        labelsprocess_dict, labelstransform_dict, postprocess_dict, assign_param)
      
      #this is the validation to ensure no overlap error
      templist3 = list(df_labels)
      overlapkeylist = list(set(templist3) - set(templist1))
      if len(set(overlapkeylist) & set([labels_column])) > 0:
        print("*****************")
        print("Warning of potential error")
        print("The set of columns returned from transformations applied to label column ", labels_column)
        print("Has an overlap with column headers for those columns originally passed to automunge(.):")
        print(set(overlapkeylist) & set([labels_column]))
        print("")
        print("Some potential quick fixes for this error include:")
        print("- rename columns to integers before passing to automunge(.)")
        print("- strip underscores '_' from column header titles (convention is all suffix appenders include an underscore)")
        print("")
        print("Please note any updates to column headers will need to be carried through to assignment parameters.")
        print("*****************")
        print("")

        miscparameters_results['columnoverlap_valresults'].update({column : {'result' : True, \
                                                                             'overlap' : set(overlapkeylist) & set([labels_column])}})

      else:

        miscparameters_results['columnoverlap_valresults'].update({column : {'result' : False, \
                                                                             'overlap' : set()}})
      
      
      #now delete columns subject to replacement
      df_labels, df_testlabels, postprocess_dict = \
      self.circleoflife(df_labels, df_testlabels, labels_column, labelscategory, labelscategory, \
                        labelsprocess_dict, labelstransform_dict, postprocess_dict)

      #here's another templist to support the postprocess_dict entry below
      templist2 = list(df_labels)

      #ok now we're going to pick one of the new entries in templist2 to serve 
      #as a "columnkey" for pulling datas from the postprocess_dict down the road
      #columnkeylist = list(set(templist2) - set(templist1))[0]
      #columnkeylist = list(set(templist2) - set(templist1))
      columnkeylist = []
      for templist2_entry in templist2:
        if templist2_entry not in templist1:
          columnkeylist.append(templist2_entry)
      
#       #now we'll apply the floatprecision transformation
#       df_labels = self.floatprecision_transform(df_labels, columnkeylist, floatprecision)
#       df_testlabels = self.floatprecision_transform(df_testlabels, columnkeylist, floatprecision)

      if isinstance(columnkeylist, str):
        columnkey = columnkeylist
      else:
        #if list is empty
        if len(columnkeylist) == 0:
          columnkey = labels_column
        else:
          columnkey = columnkeylist[0]


      finalcolumns_labels = list(df_labels)


      labelsnormalization_dict = postprocess_dict['column_dict'][finalcolumns_labels[0]]['normalization_dict']


      #we're going to create an entry to postprocess_dict to
      #store a columnkey for each of the original columns
      postprocess_dict['origcolumn'].update({labels_column : {'category' : labelscategory, \
                                                              'columnkeylist' : finalcolumns_labels, \
                                                              'columnkey' : columnkey}})
      
      #labelsencoding_dict is returned from automunge(.) and supports the reverse encoding of labels after predictions
      labelsencoding_dict[labelscategory] = labelsnormalization_dict



      #markers for label smoothing printouts
      trainsmoothing = False
      testsmoothing = False
        
      #apply label smoothing to train set if elected
      if LabelSmoothing_train > 0.0 and LabelSmoothing_train < 1.0 and str(LabelSmoothing_train) != 'False':
        
        trainsmoothing = True
        
        LSfitparams_dict = {}
        
        #this will be our marker to indicate if labelsmoothing is already conducted
        categorycomplete_dict = dict(zip(postprocess_dict['origcolumn'][labels_column]['columnkeylist'], [False]*len(postprocess_dict['origcolumn'][labels_column]['columnkeylist'])))
        
        #for column in label columns
        for labelsmoothingcolumn in postprocess_dict['origcolumn'][labels_column]['columnkeylist']:
          
          if categorycomplete_dict[labelsmoothingcolumn] is False:

            label_categorylist = postprocess_dict['column_dict'][labelsmoothingcolumn]['categorylist']
            label_category = postprocess_dict['column_dict'][labelsmoothingcolumn]['category']
            
            df_labels, categorycomplete_dict, LSfitparams_dict = \
            self.apply_LabelSmoothing(df_labels, labelsmoothingcolumn, LabelSmoothing_train, label_categorylist, label_category, categorycomplete_dict, LSfit, LSfitparams_dict)
      

  
      #else prepare empty LSfitparams_dict
      else:
        
        LSfitparams_dict = {}
        
        for labelsmoothingcolumn in postprocess_dict['origcolumn'][labels_column]['columnkeylist']:
          
          LSfitparams_dict.update({labelsmoothingcolumn : {'LSfit' : LSfit, \
                                                          'epsilon' : LabelSmoothing_train, \
                                                          'label_categorylist' : postprocess_dict['column_dict'][labelsmoothingcolumn]['categorylist'], \
                                                          'label_category' : postprocess_dict['column_dict'][labelsmoothingcolumn]['category']}})
          
      #marker
      match_testLS_to_train = False
      
      #apply label smoothing to test set if elected
      if str(LabelSmoothing_test) == 'True':
        LabelSmoothing_test = LabelSmoothing_train
        match_testLS_to_train = True
      
      if LabelSmoothing_test > 0.0 and LabelSmoothing_test < 1.0 and str(LabelSmoothing_test) != 'False':
        
        testsmoothing = True
        
        #this will be our marker to indicate if labelsmoothing is already conducted
        categorycomplete_dict = dict(zip(postprocess_dict['origcolumn'][labels_column]['columnkeylist'], [False]*len(postprocess_dict['origcolumn'][labels_column]['columnkeylist'])))                                   
        
        #for column in label columns
        for labelsmoothingcolumn in postprocess_dict['origcolumn'][labels_column]['columnkeylist']:
          
          if categorycomplete_dict[labelsmoothingcolumn] is False:
            
            if match_testLS_to_train is False:
                                     
              label_categorylist = postprocess_dict['column_dict'][labelsmoothingcolumn]['categorylist']
              label_category = postprocess_dict['column_dict'][labelsmoothingcolumn]['category']

              df_testlabels, categorycomplete_dict, _1 = \
              self.apply_LabelSmoothing(df_testlabels, labelsmoothingcolumn, LabelSmoothing_test, label_categorylist, label_category, categorycomplete_dict, LSfit, {})
              
              del _1
              
            else:
              
              df_testlabels, categorycomplete_dict = \
              self.postapply_LabelSmoothing(df_testlabels, labelsmoothingcolumn, categorycomplete_dict, LSfitparams_dict)


      #printout display progress
      if printstatus is True:
        print(" returned columns:")
        print(postprocess_dict['origcolumn'][labels_column]['columnkeylist'])
        print("")
        
        if trainsmoothing or testsmoothing:
          
          if trainsmoothing and testsmoothing:
            
            print("Label Smoothing applied to both train and test set labels")
            print("")
          
          elif trainsmoothing:
            
            print("Label Smoothing applied to train set labels")
            print("")
            
          elif testsmoothing:
            
            print("Label Smoothing applied to test set labels")
            print("")


    #now that all transforms are applied on training set and labels
    #we'll do quick validation of normalization_dict entries
    
    check_normalization_dict_result = \
    self.check_normalization_dict(postprocess_dict)

    miscparameters_results.update({'check_normalization_dict_result' : check_normalization_dict_result})
    

    #now that we've pre-processed all of the columns, let's run through them again\
    #using infill to derive plug values for the previously missing cells
    
    #printout display progress
    if printstatus is True:
      print("______")
      print("")

    infillcolumns_list = list(df_train)


    #Here is the application of assemblepostprocess_assigninfill
    #This assembles the posttransform column headers for each infill type
    postprocess_assigninfill_dict = \
    self.assemblepostprocess_assigninfill(assigninfill, infillcolumns_list, \
                                          columns_train, postprocess_dict, MLinfill)

    #now apply infill
    df_train, df_test, postprocess_dict = \
    self.apply_am_infill(df_train, df_test, postprocess_assigninfill_dict, \
                        postprocess_dict, infilliterate, printstatus, infillcolumns_list, \
                        masterNArows_train, masterNArows_test, process_dict, randomseed, ML_cmnd)

    
    #quickly gather a list of columns before any dimensionalioty reductions for populating mirror trees
    pre_dimred_finalcolumns_train = list(df_train)
    
    
    #Here's where we'll trim the columns that were stricken as part of featureselection method

    #trim branches here associated with featureselect

    if featureselection is True:

      #get list of columns currently included
      currentcolumns = list(df_train)
      
      #this is to address an edge case for featuremethod == 'default'
      if featuremethod in ['default', 'report'] or FSmodel is False \
      or (featuremethod in ['pct'] and featurepct == 1.0):
        madethecut = currentcolumns
      
      #get list of columns to trim
      madethecutset = set(madethecut)
      trimcolumns = [b for b in currentcolumns if b not in madethecutset]

      if len(trimcolumns) > 0:
        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Begin feature importance dimensionality reduction")
          print("")
          print("   method: ", featuremethod)
  #           if featuremethod == 'default':
  #             print("no feature importance dimensionality reductions")          
          if featuremethod == 'pct':
            print("threshold: ", featurepct)
          if featuremethod == 'metric':
            print("threshold: ", featuremetric)
          if featuremethod == 'report':
            print("only returning results")
          print("")
          print("trimmed columns: ")
          print(trimcolumns)
          print("")
          print("returned columns: ")
          print(madethecut)
          print("")

      #trim columns
      for trimmee in trimcolumns:

        del df_train[trimmee]
        del df_test[trimmee]


    prePCAcolumns = list(df_train)
    
    #marker if PCA applied
    PCA_applied = False

    #if user passed anything to automunbge argument PCAn_components 
    #(either the number of columns integer or a float between 0-1)

    #ok this isn't the cleanest implementation, fixing that we may want to 
    #assign a new n_components

    n_components = PCAn_components
    if ML_cmnd['PCA_type'] == 'default':

      _1, n_components = \
      self.evalPCA(df_train, PCAn_components, ML_cmnd)

    #if PCAn_components != None:
    if n_components != None:

      #this is for cases where automated PCA methods performed and we want to carry through 
      #results to postmunge through postprocess_dict
      PCAn_components = n_components

      #If user passed bool_PCA_excl as True in ML_cmnd['PCA_cmnd']
      #Then add boolean columns to the PCAexcl list of columns
      #and bool_PCAexcl just tracks what columns were added

      #PCAexcl, bool_PCAexcl = self.boolexcl(ML_cmnd, df_train, PCAexcl)
      if 'bool_PCA_excl' in ML_cmnd['PCA_cmnd'] \
      or 'bool_ordl_PCAexcl' in ML_cmnd['PCA_cmnd']:
        PCAexcl, bool_PCAexcl = self.boolexcl(ML_cmnd, df_train, PCAexcl, postprocess_dict)
      else:
        bool_PCAexcl = []

      #only perform PCA if the specified/defrived number of columns < the number of
      #columns after removing the PCAexcl columns
      #if (n_components < len(list(df_train)) - len(PCAexcl) and n_components >= 1.0):
      if n_components < (len(list(df_train)) - len(PCAexcl)) \
      and (n_components != 0) \
      and (n_components != None) \
      and (n_components is not False):

        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Applying PCA dimensionality reduction")
          print("")
          if len(bool_PCAexcl) > 0:
            print("columns excluded from PCA: ")
            print(bool_PCAexcl)
            print("")
        
        #PCA applied marker set to true
        PCA_applied = True

        #this is to carve the excluded columns out from the set
        PCAset_train, PCAset_test, PCAexcl_posttransform = \
        self.createPCAsets(df_train, df_test, PCAexcl, postprocess_dict)

        #this is to train the PCA model and perform transforms on train and test set
        PCAset_train, PCAset_test, postprocess_dict, PCActgy = \
        self.PCAfunction(PCAset_train, PCAset_test, PCAn_components, postprocess_dict, \
                         randomseed, ML_cmnd)

        #printout display progress
        if printstatus is True:
          print("PCA model applied: ")
          print(PCActgy)
          print("")
          

        #reattach the excluded columns to PCA set
#         df_train = pd.concat([PCAset_train, df_train[PCAexcl_posttransform]], axis=1)
#         df_test = pd.concat([PCAset_test, df_test[PCAexcl_posttransform]], axis=1)
        df_train = pd.concat([PCAset_train.set_index(df_train.index), df_train[PCAexcl_posttransform]], axis=1)
        df_test = pd.concat([PCAset_test.set_index(df_test.index), df_test[PCAexcl_posttransform]], axis=1)

        #printout display progress
        if printstatus is True:
          print("returned PCA columns: ")
          print(list(PCAset_train))
          print("")

      else:
        #else we'll just populate the PCAmodel slot in postprocess_dict with a placeholder
        postprocess_dict.update({'PCAmodel' : None})

    else:
      #else we'll just populate the PCAmodel slot in postprocess_dict with a placeholder
      postprocess_dict.update({'PCAmodel' : None})

    #Binary dimensionality reduction goes here
    
    #we'll only apply to training and test data not labels
    #making an executive decvision for now that ordinal encoded columns will be excluded
    if Binary in [True, 'retain']:
      
      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin Binary dimensionality reduction")
        print("")
        print("Before transform train set column count = ")
        print(df_train.shape[1])
        print("")
      
      bool_column_list = []
      
      for column in list(df_train):
        
        if column in postprocess_dict['column_dict']:

          column_category = postprocess_dict['column_dict'][column]['category']

          if process_dict[column_category]['MLinfilltype'] in \
          ['singlect', 'multirt', 'multisp', 'binary', '1010', 'boolexclude', 'concurrent_act']:

            bool_column_list.append(column)
          
      df_train, df_test, Binary_dict = self.Binary_convert(df_train, df_test, bool_column_list, Binary)
      
      
      if printstatus is True:
        print("Boolean column count = ")
        print(len(bool_column_list))
        print("")
        print("After transform train set column count = ")
        print(df_train.shape[1])
        print("")
      
    else:
      
      Binary_dict = {'bool_column_list' : [], 'column_dict' : {}}
    

    #here is the process to levelize the frequency of label rows in train data
    #currently only label categories of 'bnry' or 'text' are considered
    #a future extension will include numerical labels by adding supplemental 
    #label columns to designate inclusion in some fractional bucket of the distribution
    #e.g. such as quintiles for instance
    if TrainLabelFreqLevel in [True, 'traintest'] \
    and labels_column is not False:

      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin label rebalancing")
        print("")
        print("Before rebalancing train set row count = ")
        print(df_labels.shape[0])
        print("")


      if trainID_column is not False:
        df_train = pd.concat([df_train, df_trainID], axis=1)                        

        
      #apply LabelFrequencyLevelizer defined function
      df_train, df_labels = \
      self.LabelFrequencyLevelizer(df_train, df_labels, labelsencoding_dict, \
                                   postprocess_dict, process_dict, LabelSmoothing_train)


      #extract trainID
      if trainID_column is not False:

        df_trainID = pd.DataFrame(df_train[trainID_column])

        if isinstance(trainID_column, str):
          tempIDlist = [trainID_column]
        elif isinstance(trainID_column, list):
          tempIDlist = trainID_column
        for IDcolumn in tempIDlist:
          del df_train[IDcolumn]


      #printout display progress
      if printstatus is True:

        print("")
        print("After rebalancing train set row count = ")
        print(df_labels.shape[0])
        print("")
          

    if TrainLabelFreqLevel in ['test', 'traintest'] \
    and labelspresenttest is True:

      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin test set label rebalancing")
        print("")
        print("Before rebalancing test set row count = ")
        print(df_testlabels.shape[0])
        print("")
        

      if testID_column is not False:
        df_test = pd.concat([df_test, df_testID], axis=1)                        

        
      if LabelSmoothing_test is True:
        LabelSmoothing_test = LabelSmoothing_train

      #apply LabelFrequencyLevelizer defined function
      df_test, df_testlabels = \
      self.LabelFrequencyLevelizer(df_test, df_testlabels, labelsencoding_dict, \
                                   postprocess_dict, process_dict, LabelSmoothing_test)
        
        
      #extract testID
      if testID_column is not False:

        df_testID = pd.DataFrame(df_test[testID_column])

        if isinstance(testID_column, str):
          tempIDlist = [testID_column]
        elif isinstance(testID_column, list):
          tempIDlist = testID_column
        for IDcolumn in tempIDlist:
          del df_test[IDcolumn]
          
      #printout display progress
      if printstatus is True:

        print("")
        print("After rebalancing test set row count = ")
        print(df_testlabels.shape[0])
        print("")
        

    #then if shuffle was elected perform here    
    if shuffletrain is True or shuffletrain == 'traintest':
      
      #shuffle training set and labels
      df_train = self.df_shuffle(df_train, answer)
      
      if labels_column is not False:
        df_labels = self.df_shuffle(df_labels, answer)

      if trainID_column is not False:
        df_trainID = self.df_shuffle(df_trainID, answer)
      
      
    if shuffletrain == 'traintest':
      
      df_test = self.df_shuffle(df_test, answer)
      
      if labelspresenttest is True:
        df_testlabels = self.df_shuffle(df_testlabels, answer)

      if testID_column is not False:
        df_testID = self.df_shuffle(df_testID, answer)
      


    #great the data is processed now let's do a few moore global training preps

    #a special case, those columns that we completely excluded from processing via excl
    #we'll scrub the suffix appender
    
    #first let's create a list of excl columns with and without suffix, just in case might come in handy
    postprocess_dict.update({'excl_columns_with_suffix':[], 'excl_columns_without_suffix':[]})
    for cd_column in postprocess_dict['column_dict']:
      if postprocess_dict['column_dict'][cd_column]['category'] == 'excl':
        postprocess_dict['excl_columns_with_suffix'].append(cd_column)
        postprocess_dict['excl_columns_without_suffix'].append(cd_column[:-5])
        
    if excl_suffix is False:
      #we'll duplicate excl columns in postprocess_dict['column_dict'] to have entries both with and without suffix as keys
      for excl_column_with_suffix in postprocess_dict['excl_columns_with_suffix']:
        excl_index = postprocess_dict['excl_columns_with_suffix'].index(excl_column_with_suffix)
        excl_column_without_suffix = postprocess_dict['excl_columns_without_suffix'][excl_index]
        postprocess_dict['column_dict'].update({excl_column_without_suffix : \
                                                deepcopy(postprocess_dict['column_dict'][excl_column_with_suffix])})
    
    #(we won't perform this step to train and test sets if PCA was applied)
    if PCA_applied is False and excl_suffix is False:
      df_train.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                          else column for column in df_train.columns]
      df_test.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                         else column for column in df_test.columns]
      
    if labels_column is not False and excl_suffix is False:
      df_labels.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                           else column for column in df_labels.columns]

    if labelspresenttest is True and excl_suffix is False:
      df_testlabels.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                               else column for column in df_testlabels.columns]

    #this is admiuttedly kind of a weird spot to put this, we had introduced earlier
    #convention of a dummy testlabels set, here we'll delete if no labels in test set
    #(seems as good a place as any)
    elif labelspresenttest is False:
      df_testlabels = pd.DataFrame()


    #here's a list of final column names saving here since the translation to \
    #numpy arrays scrubs the column names
    finalcolumns_train = list(df_train)
    finalcolumns_test = list(df_test)


    #we'll create some tags specific to the application to support postprocess_dict versioning
    automungeversion = '4.00'
#     application_number = random.randint(100000000000,999999999999)
#     application_timestamp = dt.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
    version_combined = '_' + str(automungeversion) + '_' + str(application_number) + '_' \
                       + str(application_timestamp)

    #temp note: LabelSmoothing_train = False, LabelSmoothing_test = False, LabelSmoothing_val = False, \
    #here we'll populate the postprocess_dci8t that is returned from automunge
    #as it. will be. used in the postmunge call beow to process validation sets
    postprocess_dict.update({'origtraincolumns' : columns_train, \
                             'finalcolumns_train' : finalcolumns_train, \
                             'pre_dimred_finalcolumns_train' : pre_dimred_finalcolumns_train, \
                             'labels_column' : labels_column, \
                             'finalcolumns_labels' : list(df_labels), \
                             'single_train_column_labels_case' : single_train_column_labels_case, \
                             'trainID_column_orig' : trainID_column_orig, \
                             'trainID_column' : trainID_column, \
                             'finalcolumns_trainID' : list(df_trainID), \
                             'testID_column_orig' : testID_column_orig, \
                             'testID_column' : testID_column, \
                             'indexcolumn' : indexcolumn, \
                             'valpercent1' : valpercent1, \
                             'valpercent2' : valpercent2, \
                             'floatprecision' : floatprecision, \
                             'shuffletrain' : shuffletrain, \
                             'TrainLabelFreqLevel' : TrainLabelFreqLevel, \
                             'MLinfill' : MLinfill, \
                             'infilliterate' : infilliterate, \
                             'eval_ratio' : eval_ratio, \
                             'powertransform' : powertransform, \
                             'binstransform' : binstransform, \
                             'LabelSmoothing_train' : LabelSmoothing_train, \
                             'LabelSmoothing_test' : LabelSmoothing_test, \
                             'LabelSmoothing_val' : LabelSmoothing_val, \
                             'LSfit' : LSfit, \
                             'LSfitparams_dict' : LSfitparams_dict, \
                             'numbercategoryheuristic' : numbercategoryheuristic, \
                             'pandasoutput' : pandasoutput, \
                             'NArw_marker' : NArw_marker, \
                             'labelsencoding_dict' : labelsencoding_dict, \
                             'featureselection' : featureselection, \
                             'featurepct' : featurepct, \
                             'featuremetric' : featuremetric, \
                             'featuremethod' : featuremethod, \
                             'FSmodel' : FSmodel, \
                             'FScolumn_dict' : FScolumn_dict, \
                             'FS_sorted' : FS_sorted, \
                             'drift_dict' : drift_dict, \
                             'Binary' : Binary, \
                             'Binary_dict' : Binary_dict, \
                             'PCA_applied' : PCA_applied, \
                             'PCAn_components' : PCAn_components, \
                             'PCAexcl' : PCAexcl, \
                             'prePCAcolumns' : prePCAcolumns, \
                             'madethecut' : madethecut, \
                             'excl_suffix' : excl_suffix, \
                             'assigncat' : assigncat, \
                             'final_assigncat' : final_assigncat, \
                             'assigninfill' : assigninfill, \
                             'transformdict' : transformdict, \
                             'transform_dict' : transform_dict, \
                             'processdict' : processdict, \
                             'process_dict' : process_dict, \
                             'postprocess_assigninfill_dict' : postprocess_assigninfill_dict, \
                             'assignparam' : assignparam, \
                             'assign_param' : assign_param, \
                             'ML_cmnd' : ML_cmnd, \
                             'miscparameters_results' : miscparameters_results, \
                             'printstatus' : printstatus, \
                             'automungeversion' : automungeversion, \
                             'application_number' : application_number, \
                             'application_timestamp' : application_timestamp, \
                             'version_combined' : version_combined})

    
    #mirror tree assembly functions go here, these mirror the progression of transformation functions
    #where categorytree is forward pass and inverse_categorytree is backward pass
    
    #don't currently use categorytree explicitly but populating in case downstream users find use
    categorytree = self.populate_categorytree(postprocess_dict)
    
    #the inverse tree supports inversion in postmunge
    inverse_categorytree = self.populate_inverse_categorytree(postprocess_dict)
    
    #we'll create another structure, this one flatted, similar to origcolumn or column_dict
    #this one as inputcolumn_dict
    inputcolumn_dict = self.populate_inputcolumn_dict(postprocess_dict)
    
    #the trees are returned in postprocess_dict
    postprocess_dict.update({'categorytree' : categorytree, \
                             'inverse_categorytree' : inverse_categorytree, \
                             'inputcolumn_dict' : inputcolumn_dict})
    

    if totalvalidationratio > 0:

      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin Validation set processing with Postmunge")
        print("")
        

      #(postmunge shuffletrain not needed since data was already shuffled)

      #process validation set consistent to train set with postmunge here
      #df_validation1, _2, _3, _4, _5 = \
      df_validation1, _2, df_validationlabels1, _4, _5 = \
      self.postmunge(postprocess_dict, df_validation1, testID_column = False, \
                    labelscolumn = labels_column, pandasoutput = True, printstatus = printstatus, \
                    LabelSmoothing = LabelSmoothing_val, LSfit = LSfit, \
                    shuffletrain = False)




    #ok now that validation is processed, carve out second validation set if one was elected

    if totalvalidationratio > 0.0:

      if val2ratio > 0.0:


        if labels_column is not False:
          #split validation2 sets from training and labels
          
          df_validation1, df_validation2 = \
          self.df_split(df_validation1, val2ratio, False, answer)
          
          df_validationlabels1, df_validationlabels2 = \
          self.df_split(df_validationlabels1, val2ratio, False, answer)
          

        else:

          df_validation1, df_validation2 = \
          self.df_split(df_validation1, val2ratio, False, answer)

          df_validationlabels2 = pd.DataFrame()

        if trainID_column is not False:
          df_validationID1, df_validationID2 = \
          self.df_split(df_validationID1, val2ratio, False, answer)
          
        else:
          df_trainID = pd.DataFrame()
          df_validationID2 = pd.DataFrame()

      else:
        df_validation2 = pd.DataFrame()
        df_validationlabels2 = pd.DataFrame()
        df_validationID2 = pd.DataFrame()

    #else if totalvalidationratio <= 0.0
    else:
      df_validation1 = pd.DataFrame()
      df_validationlabels1 = pd.DataFrame()
      df_validationID1 = pd.DataFrame()
      df_validation2 = pd.DataFrame()
      df_validationlabels2 = pd.DataFrame()
      df_validationID2 = pd.DataFrame()


    if testID_column is not False:
      df_testID = df_testID
    else:
      df_testID = pd.DataFrame()
      

    #now if user never passed a test set and we just created a dummy set 
    #then reset returned test sets to empty
    if test_plug_marker is True:
      df_test = pd.DataFrame()
      df_testID = pd.DataFrame()    

    #now we'll apply the floatprecision transformation
    df_train = self.floatprecision_transform(df_train, finalcolumns_train, floatprecision)
    if test_plug_marker is False:
      df_test = self.floatprecision_transform(df_test, finalcolumns_train, floatprecision)
    if labels_column is not False:
      finalcolumns_labels = list(df_labels)
      df_labels = self.floatprecision_transform(df_labels, finalcolumns_labels, floatprecision)
      if labelspresenttest is True:
        df_testlabels = self.floatprecision_transform(df_testlabels, finalcolumns_labels, floatprecision)


    #printout display progress
    if printstatus is True:

      print("______")
      print("")
      print("versioning serial stamp:")
      print(version_combined)
      print("")

      print("Automunge returned column set: ")
      print(list(df_train))
      print("")

      if df_labels.empty is False:
        print("Automunge returned label column set: ")
        print(list(df_labels))
        print("")
          
    #else set output to numpy arrays
    if pandasoutput is False:

      df_train = df_train.values
      df_trainID = df_trainID.values
      df_labels = df_labels.values
      df_testlabels = df_testlabels.values
      df_validation1 = df_validation1.values
      df_validationID1 = df_validationID1.values
      df_validationlabels1 = df_validationlabels1.values
      df_validation2 = df_validation2.values
      df_validationID2 = df_validationID2.values
      df_validationlabels2 = df_validationlabels2.values
      df_test = df_test.values
      df_testID = df_testID.values

      #apply ravel to labels if appropriate - converts from eg [[1,2,3]] to [1,2,3]
      if df_labels.ndim == 2 and df_labels.shape[1] == 1:
        df_labels = np.ravel(df_labels)
      if df_validationlabels1.ndim == 2 and df_validationlabels1.shape[1] == 1:
        df_validationlabels1 = np.ravel(df_validationlabels1)
      if df_validationlabels2.ndim == 2 and df_validationlabels2.shape[1] == 1:
        df_validationlabels2 = np.ravel(df_validationlabels2)



    #then at completion of automunge(.), do an additional printout if any column overlap error to be sure user sees message
    for overlapcolumn in postprocess_dict['origtraincolumns']:
      if postprocess_dict['miscparameters_results']['columnoverlap_valresults'][overlapcolumn]['result'] is True:
        print("*****************")
        print("Warning of potential error")
        print("The set of columns returned from transformations applied to column ", overlapcolumn)
        print("Had an overlap with column headers for those columns originally passed to automunge(.):")
        print(miscparameters_results['columnoverlap_valresults'][overlapcolumn]['overlap'])
        print("")
        print("Some potential quick fixes for this error include:")
        print("- rename columns to integers before passing to automunge(.)")
        print("- strip underscores '_' from column header titles (convention is all suffix appenders include an underscore)")
        print("")
        print("Please note any updates to column headers will need to be carried through to assignment parameters.")
        print("*****************")
        print("")
        
    #a reasonable extension would be to perform some validation functions on the\
    #sets here (or also prior to transform to numpy arrays) and confirm things \
    #like consistency between format of columns and data between our train and \
    #test sets and if any issues return a coresponding error message to alert user


    #printout display progress
    if printstatus is True:

      print("_______________")
      print("Automunge Complete")
      print("")

    return df_train, df_trainID, df_labels, \
    df_validation1, df_validationID1, df_validationlabels1, \
    df_validation2, df_validationID2, df_validationlabels2, \
    df_test, df_testID, df_testlabels, \
    labelsencoding_dict, finalcolumns_train, finalcolumns_test,  \
    FScolumn_dict, postprocess_dict




  def postprocessfamily(self, df_test, column, category, origcategory, process_dict, \
                        transform_dict, postprocess_dict, columnkey, assign_param):
    '''
    #as automunge runs a for loop through each column in automunge, this is the  
    #processing function applied which runs through the family primitives
    #populated in the transform_dict by assembletransformdict.
    
    #we will run in order of
    #parents, auntsuncles, siblings, cousins
    '''

  #     print("postprocessfamily")
  #     print("column = ", column)
  #     print("category = ", category)

  
    #process the parents (with downstream, with replacement)
    for parent in transform_dict[category]['parents']:

  #       print("parent = ", parent)

      if parent != None:
        df_test = \
        self.postprocessparent(df_test, column, parent, origcategory, process_dict, \
                              transform_dict, postprocess_dict, columnkey, assign_param)
        
    #process the auntsuncles (no downstream, with replacement)
    for auntuncle in transform_dict[category]['auntsuncles']:

  #       print("auntuncle = ", auntuncle)

      if auntuncle != None:
        df_test = \
        self.postprocesscousin(df_test, column, auntuncle, origcategory, process_dict, \
                                transform_dict, postprocess_dict, columnkey, assign_param)
        
    #process the siblings (with downstream, supplemental)
    for sibling in transform_dict[category]['siblings']:

  #       print("sibling = ", sibling)

      if sibling != None:
        #note we use the processparent function here
        df_test = \
        self.postprocessparent(df_test, column, sibling, origcategory, process_dict, \
                              transform_dict, postprocess_dict, columnkey, assign_param)
  
    #process the cousins (no downstream, supplemental)
    for cousin in transform_dict[category]['cousins']:

  #       print("cousin = ", cousin)

      if cousin != None:
        #note we use the processsibling function here
        df_test = \
        self.postprocesscousin(df_test, column, cousin, origcategory, process_dict, \
                                transform_dict, postprocess_dict, columnkey, assign_param)


  #     #if we had replacement transformations performed then delete the original column 
  #     #(circle of life)
  #     if len(transform_dict[category]['auntsuncles']) + len(transform_dict[category]['parents']) > 0:
  #       del df_test[column]

    return df_test


  def postcircleoflife(self, df_test, column, category, origcategory, process_dict, \
                        transform_dict, postprocess_dict, columnkey):
    '''
    This functino deletes source columns for family primitives that included replacement.
    '''

    #if we had replacement transformations performed on first generation \
    #then delete the original column
    if len(transform_dict[category]['auntsuncles']) \
    + len(transform_dict[category]['parents']) > 0:
      del df_test[column]

    #if we had replacement transformations performed on downstream generation \
    #then delete the associated parent column 
    for columndict_column in postprocess_dict['column_dict']:
      if postprocess_dict['column_dict'][columndict_column]['deletecolumn'] is True:

        #first we'll remove the column from columnslists 
        for columnslistcolumn in postprocess_dict['column_dict'][columndict_column]['columnslist']:

          if columndict_column in postprocess_dict['column_dict'][columnslistcolumn]['columnslist']:

            postprocess_dict['column_dict'][columnslistcolumn]['columnslist'].remove(columndict_column)



        #now we'll delete column
        #note this only worksa on single column  parents, need to incioroprate categorylist
        #for multicolumn parents (future extension)
        if columndict_column in list(df_test):
          del df_test[columndict_column]

    return df_test




  def postprocesscousin(self, df_test, column, cousin, origcategory, process_dict, \
                       transform_dict, postprocess_dict, columnkey, assign_param):
    
    params = self.grab_params(assign_param, cousin, column)
    
    if bool(params):
      
      #if this is a dual process function
      if process_dict[cousin]['postprocess'] != None:
        df_test = \
        process_dict[cousin]['postprocess'](df_test, column, postprocess_dict, \
                                             columnkey, params)

      #else if this is a single process function
      elif process_dict[cousin]['singleprocess'] != None:

        df_test, _1 = \
        process_dict[cousin]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict, params)
      
    else:
    
      #if this is a dual process function
      if process_dict[cousin]['postprocess'] != None:
        df_test = \
        process_dict[cousin]['postprocess'](df_test, column, postprocess_dict, \
                                             columnkey)

      #else if this is a single process function
      elif process_dict[cousin]['singleprocess'] != None:

        df_test, _1 = \
        process_dict[cousin]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict)

    return df_test




  def postprocessparent(self, df_test, column, parent, origcategory, process_dict, \
                      transform_dict, postprocess_dict, columnkey, assign_param):
    """
    #we want to apply in order of
    #upstream process, children, coworkers, niecesnephews, friends
    """
    
    
    #this is used to derive the new columns from the trasform
    origcolumnsset = set(list(df_test))
    
    params = self.grab_params(assign_param, parent, column)
    
    if bool(params):
      
      #if this is a dual process function
      if process_dict[parent]['postprocess'] != None:

        df_test = \
        process_dict[parent]['postprocess'](df_test, column, postprocess_dict, \
                                             columnkey, params)

      #else if this is a single process function process train and test seperately
      elif process_dict[parent]['singleprocess'] != None:

        df_test, _1 = \
        process_dict[parent]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict, params)
      
    else:

      #if this is a dual process function
      if process_dict[parent]['postprocess'] != None:

        df_test = \
        process_dict[parent]['postprocess'](df_test, column, postprocess_dict, \
                                             columnkey)

      #else if this is a single process function process train and test seperately
      elif process_dict[parent]['singleprocess'] != None:

        df_test, _1 = \
        process_dict[parent]['singleprocess'](df_test, column, origcategory, \
                                              postprocess_dict)

    #this is used to derive the new columns from the trasform
    newcolumnsset = set(list(df_test))

    #derive the new columns from the trasform
    categorylist = list(origcolumnsset^newcolumnsset)


    if len(categorylist) > 1:
      #future extension
      pass

    elif len(categorylist) > 0:
      parentcolumn = categorylist[0]

      #process any children
      for child in transform_dict[parent]['children']:

        if child != None:

          #process the child
          #note the function applied is postprocessparent (using recursion)
          #parent column
          df_test = \
          self.postprocessparent(df_test, parentcolumn, child, origcategory, process_dict, \
                                 transform_dict, postprocess_dict, columnkey, assign_param)
  #         self.postprocessfamily(df_test, parentcolumn, child, origcategory, process_dict, \
  #                               transform_dict, postprocess_dict, columnkey)
      

      #process any coworkers
      for coworker in transform_dict[parent]['coworkers']:

        if coworker != None:

          #process the coworker
          #note the function applied is processcousin
          df_test = \
          self.postprocesscousin(df_test, parentcolumn, coworker, origcategory, \
                                 process_dict, transform_dict, postprocess_dict, columnkey, assign_param)

          
      #process any niecesnephews
      #note the function applied is comparable to processsibling, just a different
      #parent column
      for niecenephew in transform_dict[parent]['niecesnephews']:

        if niecenephew != None:

          #process the niecenephew
          #note the function applied is postprocessparent (using recursion)
          df_test = \
          self.postprocessparent(df_test, parentcolumn, niecenephew, origcategory, \
                                 process_dict, transform_dict, postprocess_dict, columnkey, assign_param)
  #         self.postprocessfamily(df_test, parentcolumn, niecenephew, origcategory, \
  #                                process_dict, transform_dict, postprocess_dict, columnkey)
          
          
      #process any friends
      for friend in transform_dict[parent]['friends']:

        if friend != None:

          #process the friend
          #note the function applied is processcousin
          df_test = \
          self.postprocesscousin(df_test, parentcolumn, friend, origcategory, \
                                 process_dict, transform_dict, postprocess_dict, columnkey, assign_param)


  #     #if we had replacement transformations performed then delete the original column 
  #     #(circle of life)
  #     if len(transform_dict[parent]['children']) \
  #     + len(transform_dict[parent]['coworkers']) > 0:
  #       del df_test[parentcolumn]


    return df_test
  
  
  
  
  def postprocess_numerical_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_numerical_class(mdf_test, column, postprocess_dict, columnkey)
    #function to normalize data to mean of 0 and standard deviation of 1 from training distribution
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column'), and the mean and std from the train set \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of remaining values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_nmbr'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    std = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['std']
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    cap = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['cap']
    floor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['floor']
    

    #copy original column for implementation
    mdf_test[column + '_nmbr'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_nmbr'] = pd.to_numeric(mdf_test[column + '_nmbr'], errors='coerce')

    #get mean of training data
    mean = mean  
    
    if cap is not False:
      #replace values in test > cap with cap
      mdf_test.loc[mdf_test[column + '_nmbr'] > cap, (column + '_nmbr')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_test.loc[mdf_test[column + '_nmbr'] < floor, (column + '_nmbr')] \
      = floor

    #replace missing data with training set mean
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'].fillna(mean)

    #subtract mean from column
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'] - mean

    #get standard deviation of training data
    std = std

    #divide column values by std
    #offset, multiplier are parameters that defaults to zero, one
    mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'] / std * multiplier + offset

#     #change data type for memory savings
#     mdf_test[column + '_nmbr'] = mdf_test[column + '_nmbr'].astype(np.float32)

    return mdf_test
  

    
  def postprocess_MADn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_MADn_class(mdf_test, column, postprocess_dict, columnkey)
    #function to normalize data to mean of 0 and mean absolute deviation of 1 from training distribution
    #takes as arguement pandas dataframe of test data (mdf_test)\
    #and the name of the column string ('column'), and the mean and MAD from the train set \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of remaining values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better than z-score for when the numerical distribution isn't thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_MADn'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    MAD = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['MAD']

    #copy original column for implementation
    mdf_test[column + '_MADn'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_MADn'] = pd.to_numeric(mdf_test[column + '_MADn'], errors='coerce')

    #get mean of training data
    mean = mean  

    #replace missing data with training set mean
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'].fillna(mean)

    #subtract mean from column
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'] - mean

    #get mean absolute deviation of training data
    MAD = MAD

    #divide column values by std
    mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'] / MAD

#     #change data type for memory savings
#     mdf_test[column + '_MADn'] = mdf_test[column + '_MADn'].astype(np.float32)

    return mdf_test

    
  def postprocess_MAD3_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_MADn_class(mdf_test, column, postprocess_dict, columnkey)
    #function to normalize data by subtracting max and dividing by mean absolute deviation from training distribution
    #takes as arguement pandas dataframe of test data (mdf_test)\
    #and the name of the column string ('column'), and the mean and MAD from the train set \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of remaining values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better than z-score for when the numerical distribution isn't thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_MAD3'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    MAD = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['MAD']
    datamax = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['datamax']

    #copy original column for implementation
    mdf_test[column + '_MAD3'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_MAD3'] = pd.to_numeric(mdf_test[column + '_MAD3'], errors='coerce')

    #get mean of training data
    mean = mean  

    #replace missing data with training set mean
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'].fillna(mean)

    #subtract datamax from column
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'] - datamax

    #get mean absolute deviation of training data
    MAD = MAD

    #divide column values by std
    mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'] / MAD

#     #change data type for memory savings
#     mdf_test[column + '_MAD3'] = mdf_test[column + '_MAD3'].astype(np.float32)

    return mdf_test
    
    
  def postprocess_mnmx_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_mnmx_class(mdf_test, column, postprocess_dict, columnkey)
    #function to scale data to minimum of 0 and maximum of 1 based on training distribution
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column'), and the normalization parameters \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of training values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_mnmx'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']
    
    cap = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['cap']
    
    floor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['floor']

    #copy original column for implementation
    mdf_test[column + '_mnmx'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_mnmx'] = pd.to_numeric(mdf_test[column + '_mnmx'], errors='coerce')

    #get mean of training data
    mean = mean  

    #replace missing data with training set mean
    mdf_test[column + '_mnmx'] = mdf_test[column + '_mnmx'].fillna(mean)
    
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0:
      maxminusmin = 1

    #perform min-max scaling to test set using values from train
    mdf_test[column + '_mnmx'] = (mdf_test[column + '_mnmx'] - minimum) / \
                                 (maxminusmin)

    if cap is not False:
      #replace values in test > cap with cap
      mdf_test.loc[mdf_test[column + '_mnmx'] > (cap - minimum)/maxminusmin, (column + '_mnmx')] \
      = (cap - minimum)/maxminusmin
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_test.loc[mdf_test[column + '_mnmx'] < (floor - minimum)/maxminusmin, (column + '_mnmx')] \
      = (floor - minimum)/maxminusmin


    return mdf_test


  def postprocess_mnm3_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_mnmx_class(mdf_test, column, postprocess_dict, columnkey)
    #function to scale data to minimum of 0 and maximum of 1 based on training distribution
    #quantiles with values exceeding quantiles capped
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column'), and the normalization parameters \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of training values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''


    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_mnm3'

    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    quantilemin = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['quantilemin']

    quantilemax = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['quantilemax']

    #copy original column for implementation
    mdf_test[column + '_mnm3'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_mnm3'] = pd.to_numeric(mdf_test[column + '_mnm3'], errors='coerce')

#     #get mean of training data
#     mean = mean    
    
    #replace values > quantilemax with quantilemax
    mdf_test.loc[mdf_test[column + '_mnm3'] > quantilemax, (column + '_mnm3')] \
    = quantilemax
    #replace values < quantile10 with quantile10
    mdf_test.loc[mdf_test[column + '_mnm3'] < quantilemin, (column + '_mnm3')] \
    = quantilemin
    
    #replace missing data with training set mean
    mdf_test[column + '_mnm3'] = mdf_test[column + '_mnm3'].fillna(mean)
    
    #avoid outlier div by zero when max = min
    maxminusmin = quantilemax - quantilemin
    if maxminusmin == 0:
      maxminusmin = 1

    #perform min-max scaling to test set using values from train
    mdf_test[column + '_mnm3'] = (mdf_test[column + '_mnm3'] - quantilemin) / \
                                 (maxminusmin)

#     #change data type for memory savings
#     mdf_test[column + '_mnm3'] = mdf_test[column + '_mnm3'].astype(np.float32)

    return mdf_test


  def postprocess_mnm6_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_mnm6_class(mdf_test, column, postprocess_dict, columnkey)
    #function to scale data to minimum of 0 and maximum of 1 based on training distribution
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column'), and the normalization parameters \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of training values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    
    #note that this differs from mnmx in that a floor is placed on the test set at min(train)
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_mnm6'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']

    #copy original column for implementation
    mdf_test[column + '_mnm6'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_mnm6'] = pd.to_numeric(mdf_test[column + '_mnm6'], errors='coerce')

    #get mean of training data
    mean = mean

    #replace missing data with training set mean
    mdf_test[column + '_mnm6'] = mdf_test[column + '_mnm6'].fillna(mean)
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0:
      maxminusmin = 1

    #perform min-max scaling to test set using values from train
    mdf_test[column + '_mnm6'] = (mdf_test[column + '_mnm6'] - minimum) / \
                                 (maxminusmin)
    
    #replace values in test < 0 with 0
    mdf_test.loc[mdf_test[column + '_mnm6'] < 0, (column + '_mnm6')] \
    = 0

#     #change data type for memory savings
#     mdf_test[column + '_mnm6'] = mdf_test[column + '_mnm6'].astype(np.float32)

    return mdf_test


  def postprocess_retn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_retn_class(mdf_train, mdf_test, column, category)
    #function to scale data as follows:
    
    # if max >= 0 and min <= 0:
  
    #   #scaling based on 
    #   x = x / (max - min)

    # elif max >= 0 and min >= 0:

    #   #traditional min/max
    #   x = (x - min) / (max - min)
    
    # elif max <= 0 and min <= 0:

    #   #max/min (retains negative values)
    #   x = (x - max) / (max - min)
    
    #replaces missing or improperly formatted data with mean of remaining values
    
    #returns same dataframes with new column of name column + '_retn'
    #note this is a "dualprocess" function since is applied to both dataframes
    
    #note with parameters divisor can also be set as standard deviation
    #also aprameters accepted for cap/floor/mulitplier/offset
    #where cap/floor based on pretransform values
    #multiplier/offset based on posttransform values, muoltiplier applied betfore offset
    """
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_retn'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']
    scalingapproach = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['scalingapproach']
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    cap = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['cap']
    floor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['floor']
    divisor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['divisor']

    
    #copy original column for implementation
    mdf_test[column + '_retn'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_retn'] = pd.to_numeric(mdf_test[column + '_retn'], errors='coerce')

    
    if cap is not False:
      #replace values in test > cap with cap
      mdf_test.loc[mdf_test[column + '_retn'] > cap, (column + '_retn')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_test.loc[mdf_test[column + '_retn'] < floor, (column + '_retn')] \
      = floor
    
    
    #get mean of training data
    mean = mean  

    #replace missing data with training set mean
    mdf_test[column + '_retn'] = mdf_test[column + '_retn'].fillna(mean)
    
    
    #avoid outlier div by zero when max = min
    maxminusmin = maximum - minimum
    if maxminusmin == 0:
      maxminusmin = 1
    
    
    if scalingapproach == 'retn':
      
      mdf_test[column + '_retn'] = (mdf_test[column + '_retn']) / \
                                    (divisor) * multiplier + offset
      
    elif scalingapproach == 'mnmx':
    
      #perform min-max scaling to test set using values from train
      mdf_test[column + '_retn'] = (mdf_test[column + '_retn'] - minimum) / \
                                   (divisor) * multiplier + offset
      
    elif scalingapproach == 'mxmn':
    
      #perform min-max scaling to test set using values from train
      mdf_test[column + '_retn'] = (mdf_test[column + '_retn'] - maximum) / \
                                   (divisor) * multiplier + offset


    return mdf_test


  def postprocess_mean_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_mean_class(mdf_test, column, postprocess_dict, columnkey)
    #function to scale data to minimum of 0 and maximum of 1 based on training distribution
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column'), and the normalization parameters \
    #stored in postprocess_dict
    #replaces missing or improperly formatted data with mean of training values
    #leaves original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_mean'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']
    
    maxminusmin = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxminusmin']
    
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']
    
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    
    cap = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['cap']
    
    floor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['floor']

    #copy original column for implementation
    mdf_test[column + '_mean'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_mean'] = pd.to_numeric(mdf_test[column + '_mean'], errors='coerce')
    
    if cap is not False:
      #replace values in test > cap with cap
      mdf_test.loc[mdf_test[column + '_mean'] > cap, (column + '_mean')] \
      = cap
    
    if floor is not False:
      #replace values in test < floor with floor
      mdf_test.loc[mdf_test[column + '_mean'] < floor, (column + '_mean')] \
      = floor

    #get mean of training data
    mean = mean  

    #replace missing data with training set mean
    mdf_test[column + '_mean'] = mdf_test[column + '_mean'].fillna(mean)
    

    #perform min-max scaling to test set using values from train
    mdf_test[column + '_mean'] = (mdf_test[column + '_mean'] - mean) / \
                                 (maxminusmin) * multiplier + offset

#     #change data type for memory savings
#     mdf_test[column + '_mnmx'] = mdf_test[column + '_mnmx'].astype(np.float32)

    return mdf_test

  
  def postprocess_binary_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_binary_class(mdf, column, postprocess_dict, columnkey)
    #converts binary classification values to 0 or 1
    #takes as arguement a pandas dataframe (mdf_test), \
    #the name of the column string ('column') \
    #and the string classification to assign to missing data ('missing')
    #saved in the postprocess_dict
    #replaces original specified column in dataframe
    #returns transformed dataframe
    #missing category must be identical to one of the two existing categories
    #returns error message if more than two categories remain
    '''
    
    #retrieve normalization parameters
    normkey = column + '_bnry'
    binary_missing_plug = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['missing']
    
#     onevalue = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['onevalue']
    
#     zerovalue = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['zerovalue']

    onevalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][1]
    
    zerovalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][0]

    #change column name to column + '_bnry'
    mdf_test[column + '_bnry'] = mdf_test[column].copy()


    #replace missing data with specified classification
    mdf_test[column + '_bnry'] = mdf_test[column + '_bnry'].fillna(binary_missing_plug)

    #this addressess issue where nunique for mdftest > than that for mdf_train
    #note is currently an oportunity for improvement that NArows won't identify these poinsts as candiadates
    #for user specified infill, and as currently addressed will default to infill with most common value
    #in the mean time a workaround could be for user to manually replace extra values with nan prior to
    #postmunge application such as if they want to apply ML infill
    #this will only be an issue when nunique for df_train == 2, and nunique for df_test > 2
    #if len(mdf_test[column + '_bnry'].unique()) > 2:
    uniqueintest = mdf_test[column + '_bnry'].unique()
    for unique in uniqueintest:
      if unique not in [onevalue, zerovalue]:
        mdf_test[column + '_bnry'] = \
        np.where(mdf_test[column + '_bnry'] == unique, binary_missing_plug, mdf_test[column + '_bnry'])
   
    
    #convert column to binary 0/1 classification (replaces scikit LabelBinarizer)
    mdf_test[column + '_bnry'] = np.where(mdf_test[column + '_bnry'] == onevalue, 1, 0)

    #create list of columns
    bnrycolumns = [column + '_bnry']
    

    #change data types to 8-bit (1 byte) integers for memory savings
    mdf_test[column + '_bnry'] = mdf_test[column + '_bnry'].astype(np.int8)


    return mdf_test
  
  def postprocess_binary2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_binary_class(mdf, column, postprocess_dict, columnkey)
    #converts binary classification values to 0 or 1
    #takes as arguement a pandas dataframe (mdf_test), \
    #the name of the column string ('column') \
    #and the string classification to assign to missing data ('missing')
    #saved in the postprocess_dict
    #replaces original specified column in dataframe
    #returns transformed dataframe
    #missing category must be identical to one of the two existing categories
    #returns error message if more than two categories remain
    '''
    
    #retrieve normalization parameters
    normkey = column + '_bnr2'
    binary_missing_plug = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['missing']
    
#     onevalue = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['onevalue']
    
#     zerovalue = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['zerovalue']

    onevalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][1]
    
    zerovalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][0]

    #change column name to column + '_bnry'
    mdf_test[column + '_bnr2'] = mdf_test[column].copy()


    #replace missing data with specified classification
    mdf_test[column + '_bnr2'] = mdf_test[column + '_bnr2'].fillna(binary_missing_plug)

    #this addressess issue where nunique for mdftest > than that for mdf_train
    #note is currently an oportunity for improvement that NArows won't identify these poinsts as candiadates
    #for user specified infill, and as currently addressed will default to infill with most common value
    #in the mean time a workaround could be for user to manually replace extra values with nan prior to
    #postmunge application such as if they want to apply ML infill
    #this will only be an issue when nunique for df_train == 2, and nunique for df_test > 2
    #if len(mdf_test[column + '_bnry'].unique()) > 2:
    uniqueintest = mdf_test[column + '_bnr2'].unique()
    for unique in uniqueintest:
      if unique not in [onevalue, zerovalue]:
        mdf_test[column + '_bnr2'] = \
        np.where(mdf_test[column + '_bnr2'] == unique, binary_missing_plug, mdf_test[column + '_bnr2'])
    
    #convert column to binary 0/1 classification (replaces scikit LabelBinarizer)
    mdf_test[column + '_bnr2'] = np.where(mdf_test[column + '_bnr2'] == onevalue, 1, 0)

    #create list of columns
    bnrycolumns = [column + '_bnr2']
    

    #change data types to 8-bit (1 byte) integers for memory savings
    mdf_test[column + '_bnr2'] = mdf_test[column + '_bnr2'].astype(np.int8)


    return mdf_test
  
  
  def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_text_class(mdf_test, column, postprocess_dict, columnkey)
    #process column with text classifications
    #takes as arguement pandas dataframe containing test data  
    #(mdf_test), and the name of the column string ('column'), and an array of
    #the associated transformed column s from the train set (textcolumns)
    #which is saved in the postprocess_dict
    #note this aligns formatting of transformed columns to the original train set
    #fromt he original treatment with automunge
    #retains the original column from master dataframe and
    #adds onehot encodings
    #with columns named after column_ + text classifications
    #missing data replaced with category label 'missing'+column
    #any categories missing from the training set removed from test set
    #any category present in training but missing from test set given a column of zeros for consistent formatting
    #ensures order of all new columns consistent between both sets
    #returns two transformed dataframe (mdf_train, mdf_test) \
    #and a list of the new column names (textcolumns)
    '''
    
    #note it is kind of a hack here to create a column for missing values with \
    #two underscores (__) in the column name to ensure appropriate order for cases\
    #where NaN present in test data but not train data, if a category starts with|
    #an underscore such that it preceeds '__missing' alphabetically in this scenario\
    #this might create error due to different order of columns, address of this \
    #potential issue will be a future extension

#     #add _NArw to textcolumns to ensure a column gets populated even if no missing
#     textcolumns = [column + '_NArw'] + textcolumns

    

    #note this will need to be revised in a future extension where 
    #downstream transforms are performed on multicolumn parents 
    #by pulling the categorylist instead of columnslist (noting that will require
    #a more exact evaluation for columnkey somehow)
    
    tempcolumn = column + '_:;:_temp'
    
    #create copy of original column for later retrieval
    mdf_test[tempcolumn] = mdf_test[column].copy()

    #convert column to category
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype('category')

#     #if set is categorical we'll need the plug value for missing values included
#     mdf_test[column] = mdf_test[column].cat.add_categories(['NArw'])

#     #replace NA with a dummy variable
#     mdf_test[column] = mdf_test[column].fillna('NArw')
    
    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_test[tempcolumn].cat.categories:
      mdf_test[tempcolumn] = mdf_test[tempcolumn].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_test[tempcolumn] = mdf_test[tempcolumn].fillna('zzzinfill')

    #replace numerical with string equivalent
    #mdf_train[column] = mdf_train[column].astype(str)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype(str)
    
    #moved this to after the initial infill
    #new method for retrieving a columnkey
    normkey = False
#     for unique in mdf_test[tempcolumn].unique():
#       if column + '_' + str(unique) in postprocess_dict['column_dict']:
#         normkey = column + '_' + str(unique)
#         break
    
    #this second method for normkey retrieval addresses outlier scenarios when 
    #no unique valuies in test set match those in train set
    if normkey is False:

      if column in postprocess_dict['origcolumn']:

        columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']

      else:

        origcolumn = postprocess_dict['column_dict'][column]['origcolumn']

        columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']

      for columnkey in columnkeylist:
        
        if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

          if 'textlabelsdict_text' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

            normkey = columnkey
          
    if normkey is not False:

      #textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
      textcolumns = postprocess_dict['column_dict'][normkey]['categorylist']


      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array

      #we'll get the category names from the textcolumns array by stripping the \
      #prefixes of column name + '_'
      prefixlength = len(column)+1
      labels_train = textcolumns[:]
      for textcolumn in labels_train:
        textcolumn = textcolumn[prefixlength :]
      #labels_train.sort(axis=0)
      labels_train.sort()
      labels_test = mdf_test[tempcolumn].unique()
      labels_test.sort(axis=0)


      #apply onehotencoding
      df_test_cat = pd.get_dummies(mdf_test[tempcolumn])

      #append column header name to each category listing
      #note the iteration is over a numpy array hence the [...] approach
      labels_test[...] = column + '_' + labels_test[...]

      #convert sparse array to pandas dataframe with column labels
      df_test_cat.columns = labels_test



      #Get missing columns in test set that are present in training set
      missing_cols = set( textcolumns ) - set( df_test_cat.columns )

      #Add a missing column in test set with default value equal to 0
      for c in missing_cols:
          df_test_cat[c] = 0

      #Ensure the order of column in the test set is in the same order than in train set
      #Note this also removes categories in test set that aren't present in training set
      df_test_cat = df_test_cat[textcolumns]


      #concatinate the sparse set with the rest of our training data
      mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)


      del mdf_test[tempcolumn]

      #delete support NArw2 column
      columnNAr2 = column + '_zzzinfill'
      if columnNAr2 in list(mdf_test):
        del mdf_test[columnNAr2]

      #change data types to 8-bit (1 byte) integers for memory savings
      for textcolumn in textcolumns:

        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

    else:
      
      del mdf_test[tempcolumn]
    
    return mdf_test
  
  
  def postprocess_textsupport_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #just like the postprocess_text_class function but uses different approach for
    #normalizaation key (uses passed columnkey). This function supports some of the
    #other methods.
    '''
    
    #note it is kind of a hack here to create a column for missing values with \
    #two underscores (__) in the column name to ensure appropriate order for cases\
    #where NaN present in test data but not train data, if a category starts with|
    #an underscore such that it preceeds '__missing' alphabetically in this scenario\
    #this might create error due to different order of columns, address of this \
    #potential issue will be a future extension

#     #add _NArw to textcolumns to ensure a column gets populated even if no missing
#     textcolumns = [column + '_NArw'] + textcolumns

    

    #note this will need to be revised in a future extension where 
    #downstream transforms are performed on multicolumn parents 
    #by pulling the categorylist instead of columnslist (noting that will require
    #a more exact evaluation for columnkey somehow)
    
    
    #textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    textcolumns = postprocess_dict['column_dict'][columnkey]['categorylist']
    
    tempcolumn = column + '_:;:_temp'
    
    #create copy of original column for later retrieval
    mdf_test[tempcolumn] = mdf_test[column].copy()

    #convert column to category
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype('category')

#     #if set is categorical we'll need the plug value for missing values included
#     mdf_test[column] = mdf_test[column].cat.add_categories(['NArw'])

#     #replace NA with a dummy variable
#     mdf_test[column] = mdf_test[column].fillna('NArw')
    
    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_test[tempcolumn].cat.categories:
      mdf_test[tempcolumn] = mdf_test[tempcolumn].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_test[tempcolumn] = mdf_test[tempcolumn].fillna('zzzinfill')

    #replace numerical with string equivalent
    #mdf_train[column] = mdf_train[column].astype(str)
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype(str)


    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array

    #we'll get the category names from the textcolumns array by stripping the \
    #prefixes of column name + '_'
    prefixlength = len(column)+1
    labels_train = textcolumns[:]
    for textcolumn in labels_train:
      textcolumn = textcolumn[prefixlength :]
    #labels_train.sort(axis=0)
    labels_train.sort()
    labels_test = mdf_test[tempcolumn].unique()
    labels_test.sort(axis=0)


    #apply onehotencoding
    df_test_cat = pd.get_dummies(mdf_test[tempcolumn])
    
    #append column header name to each category listing
    #note the iteration is over a numpy array hence the [...] approach
    labels_test[...] = column + '_' + labels_test[...]
    
    #convert sparse array to pandas dataframe with column labels
    df_test_cat.columns = labels_test
    


    #Get missing columns in test set that are present in training set
    missing_cols = set( textcolumns ) - set( df_test_cat.columns )

    #Add a missing column in test set with default value equal to 0
    for c in missing_cols:
        df_test_cat[c] = 0

    #Ensure the order of column in the test set is in the same order than in train set
    #Note this also removes categories in test set that aren't present in training set
    df_test_cat = df_test_cat[textcolumns]


    #concatinate the sparse set with the rest of our training data
    mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)
    

    del mdf_test[tempcolumn]
    
    #delete support NArw2 column
    columnNAr2 = column + '_zzzinfill'
    if columnNAr2 in list(mdf_test):
      del mdf_test[columnNAr2]
    
    #change data types to 8-bit (1 byte) integers for memory savings
    for textcolumn in textcolumns:
      
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

    
    return mdf_test
  
  def postprocess_splt_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_splt_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    
    #here in postprocess we only create columns foir those overlaps that were identified 
    #fromthe train set
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'splt_newcolumns_splt' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_splt']
      
      concurrent_activations = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['concurrent_activations']


      #now for mdf_test we'll only consider those overlaps already identified from train set

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for key in train_keys:

        test_overlap_dict.update({key:[]})

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                
                if concurrent_activations is False:

                  break


      newcolumns = []

      for dict_key in overlap_dict:

        newcolumn = column + '_splt_' + dict_key
        
#         mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

#         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  
  def postprocess_spl2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_spl2_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries with their redecued overlap
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' replaced cells with north in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted to follow with an ordl encoding
    
    #here in postprocess we only replace entries for those overlaps that were identified 
    #from the train set
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
#     if column in postprocess_dict['origcolumn']:
      
#       columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
#     else:
      
#       origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
#       columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
#     for columnkey in columnkeylist:
    
#       if 'splt_newcolumns' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:
        
#         normkey = columnkey

    normkey = column + '_spl2'
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

#       newcolumns = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_newcolumns']


      #now for mdf_test we'll only consider those overlaps already identified from train set

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for key in train_keys:

        test_overlap_dict.update({key:[]})

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                
                break
                
                
      #then we'll populate the spl2 replacement dict

      spl2_test_overlap_dict = {}

      test_overlap_key_list = list(test_overlap_dict)

      test_overlap_key_list.sort()
      test_overlap_key_list.sort(key = len, reverse=True)

      for overlap_key in test_overlap_key_list:

        for entry in test_overlap_dict[overlap_key]:

          if entry not in spl2_test_overlap_dict:

            spl2_test_overlap_dict.update({entry : overlap_key})


#       newcolumns = []

#       for dict_key in overlap_dict:

      newcolumn = column + '_spl2'

#         mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

#       mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)


      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)

#       newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  
  def postprocess_spl5_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_spl5_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted to follow with an ordl encoding
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
#     if column in postprocess_dict['origcolumn']:
      
#       columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
#     else:
      
#       origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
#       columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
#     for columnkey in columnkeylist:
    
#       if 'splt_newcolumns' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:
        
#         normkey = columnkey

    normkey = column + '_spl5'
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

#       newcolumns = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_newcolumns']


      #now for mdf_test we'll only consider those overlaps already identified from train set

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for key in train_keys:

        test_overlap_dict.update({key:[]})

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                
                break
                
                
      #then we'll populate the spl2 replacement dict

      spl2_test_overlap_dict = {}

      test_overlap_key_list = list(test_overlap_dict)

      test_overlap_key_list.sort()
      test_overlap_key_list.sort(key = len, reverse=True)

      for overlap_key in test_overlap_key_list:

        for entry in test_overlap_dict[overlap_key]:

          if entry not in spl2_test_overlap_dict:

            spl2_test_overlap_dict.update({entry : overlap_key})
            
            
      #here's where we identify values to set to 0 for spl5
      spl5_test_zero_dict = {}
      for entry in unique_list_test:
        if entry not in spl2_test_overlap_dict:
          spl5_test_zero_dict.update({entry : 0})


#       newcolumns = []

#       for dict_key in overlap_dict:

      newcolumn = column + '_spl5'

#         mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

#       mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  def postprocess_spl7_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #(spl7 is same as spl5 but uses min overlap character legnth of 1 instead of 5)
    #postprocess_spl5_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted to follow with an ordl encoding
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
#     if column in postprocess_dict['origcolumn']:
      
#       columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
#     else:
      
#       origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
#       columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
#     for columnkey in columnkeylist:
    
#       if 'splt_newcolumns' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:
        
#         normkey = columnkey

    normkey = column + '_spl7'
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

#       newcolumns = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_newcolumns']


      #now for mdf_test we'll only consider those overlaps already identified from train set

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for key in train_keys:

        test_overlap_dict.update({key:[]})

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                
                break
                
                
      #then we'll populate the spl2 replacement dict

      spl2_test_overlap_dict = {}

      test_overlap_key_list = list(test_overlap_dict)

      test_overlap_key_list.sort()
      test_overlap_key_list.sort(key = len, reverse=True)

      for overlap_key in test_overlap_key_list:

        for entry in test_overlap_dict[overlap_key]:

          if entry not in spl2_test_overlap_dict:

            spl2_test_overlap_dict.update({entry : overlap_key})
            
            
      #here's where we identify values to set to 0 for spl5
      spl5_test_zero_dict = {}
      for entry in unique_list_test:
        if entry not in spl2_test_overlap_dict:
          spl5_test_zero_dict.update({entry : 0})


#       newcolumns = []

#       for dict_key in overlap_dict:

      newcolumn = column + '_spl7'

#         mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

#       mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       newcolumns.append(newcolumn)
    
    
    return mdf_test

  def postprocess_spl8_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_splt_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    
    #here in postprocess we only create columns foir those overlaps that were identified 
    #fromthe train set
    
    #comparable to splt but
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'splt_newcolumns_spl8' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_spl8']


      #now for mdf_test we'll only consider those overlaps already identified from train set
      
      #this assumes that set of unique values is same or subset of those for train set
      #for more efficient application in postmunge

#       unique_list_test = list(mdf_test[column].unique())

#       unique_list_test = list(map(str, unique_list_test))

#       test_overlap_dict = {}

#       train_keys = list(overlap_dict)

#       train_keys.sort(key = len, reverse=True)

#       for key in train_keys:

#         test_overlap_dict.update({key:[]})

#       for dict_key in train_keys:

#         for unique_test in unique_list_test:

#           len_key = len(dict_key)

#           if len(unique_test) >= len_key:

#             nbr_iterations4 = len(unique_test) - len_key

#             for l in range(nbr_iterations4 + 1):

#               extract4 = unique_test[l:(len_key+l)]

#               if extract4 == dict_key:

#                 test_overlap_dict[dict_key].append(unique_test)


      newcolumns = []

      for dict_key in overlap_dict:

        newcolumn = column + '_spl8_' + dict_key
        
#         mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

#         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        #mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  def postprocess_spl9_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_spl2_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries with their redecued overlap
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' replaced cells with north in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted to follow with an ordl encoding
    
    #here in postprocess we only replace entries for those overlaps that were identified 
    #from the train set
    
    #spl9 is comparable to spl2 but makes assumption that set of unique values
    #in test set is same or subset of train set for more efficient processing
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
#     if column in postprocess_dict['origcolumn']:
      
#       columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
#     else:
      
#       origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
#       columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
#     for columnkey in columnkeylist:
    
#       if 'splt_newcolumns' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:
        
#         normkey = columnkey

    normkey = column + '_spl9'
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
      
      spl2_overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_overlap_dict']
      
      unique_list = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']

#       newcolumns = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_newcolumns']


#       #now for mdf_test we'll only consider those overlaps already identified from train set

#       unique_list_test = list(mdf_test[column].unique())

#       unique_list_test = list(map(str, unique_list_test))

#       test_overlap_dict = {}

#       train_keys = list(overlap_dict)

#       train_keys.sort(key = len, reverse=True)

#       for key in train_keys:

#         test_overlap_dict.update({key:[]})

#       for dict_key in train_keys:

#         for unique_test in unique_list_test:

#           len_key = len(dict_key)

#           if len(unique_test) >= len_key:

#             nbr_iterations4 = len(unique_test) - len_key

#             for l in range(nbr_iterations4 + 1):

#               extract4 = unique_test[l:(len_key+l)]

#               if extract4 == dict_key:

#                 test_overlap_dict[dict_key].append(unique_test)

                
      #for spl9 we'll just copy train set overlap_dict
      test_overlap_dict = deepcopy(overlap_dict)
                
      #then we'll populate the spl2 replacement dict
      
      
      spl2_test_overlap_dict = deepcopy(spl2_overlap_dict)

      unique_list_test = list(mdf_test[column].unique())
      unique_list_test = list(map(str, unique_list_test))

      extra_unique_test = list(set(unique_list_test) - set(unique_list))

      for extra_unique in extra_unique_test:

        spl2_test_overlap_dict.update({str(extra_unique) : str(extra_unique)})
        
        #I'm suspect will eventually be a headache for someone that we're populating
        #this with str(extra_unique) as entry, trying to be consistent with train set
        #if you want to extract numbers for now run seperately a nmrc transform
      
      

#       spl2_test_overlap_dict = {}

#       test_overlap_key_list = list(test_overlap_dict)

#       test_overlap_key_list.sort()
#       test_overlap_key_list.sort(key = len, reverse=True)

#       for overlap_key in test_overlap_key_list:

#         for entry in test_overlap_dict[overlap_key]:

#           if entry not in spl2_test_overlap_dict:

#             spl2_test_overlap_dict.update({entry : overlap_key})


#       newcolumns = []

#       for dict_key in overlap_dict:

      newcolumn = column + '_spl9'

#         mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

#       mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)


      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)

#       newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  def postprocess_sp10_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_spl5_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and replaces entries with their redecued overlap
    #replaces entries without overlap to 0 (unique to spl5)
    #for example, if a categorical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created in which the entry 'north' replaced cells with north in their entries
    #(here for north and northeast)
    #and cells with west would be set to 0
    #returns as column titled origcolumn_spl2
    #missing values are ignored by default
    #this alternative to splt may be benficial for instance if one wanted to follow with an ordl encoding
    
    #sp10 is like spl5 but makes assumption that set of unique test values is
    #same or subset of train values for more efficient application in postmunge
    #that's spelled s p one zero
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
#     if column in postprocess_dict['origcolumn']:
      
#       columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
#     else:
      
#       origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
#       columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
#     for columnkey in columnkeylist:
    
#       if 'splt_newcolumns' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:
        
#         normkey = columnkey

    normkey = column + '_sp10'
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
      
      spl2_overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_overlap_dict']
      
      unique_list = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']

#       newcolumns = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['spl2_newcolumns']


      #now for mdf_test we'll only consider those overlaps already identified from train set

#       unique_list_test = list(mdf_test[column].unique())

#       unique_list_test = list(map(str, unique_list_test))

#       test_overlap_dict = {}

#       train_keys = list(overlap_dict)

#       train_keys.sort(key = len, reverse=True)

#       for key in train_keys:

#         test_overlap_dict.update({key:[]})

#       for dict_key in train_keys:

#         for unique_test in unique_list_test:

#           len_key = len(dict_key)

#           if len(unique_test) >= len_key:

#             nbr_iterations4 = len(unique_test) - len_key

#             for l in range(nbr_iterations4 + 1):

#               extract4 = unique_test[l:(len_key+l)]

#               if extract4 == dict_key:

#                 test_overlap_dict[dict_key].append(unique_test)

      #for sp10 we'll just copy train set overlap_dict
      test_overlap_dict = deepcopy(overlap_dict)
                
                
#       #then we'll populate the spl2 replacement dict

#       spl2_test_overlap_dict = {}

#       test_overlap_key_list = list(test_overlap_dict)

#       test_overlap_key_list.sort()
#       test_overlap_key_list.sort(key = len, reverse=True)

#       for overlap_key in test_overlap_key_list:

#         for entry in test_overlap_dict[overlap_key]:

#           if entry not in spl2_test_overlap_dict:

#             spl2_test_overlap_dict.update({entry : overlap_key})
            
            
#       #here's where we identify values to set to 0 for spl5
#       spl5_test_zero_dict = {}
#       for entry in unique_list_test:
#         if entry not in spl2_test_overlap_dict:
#           spl5_test_zero_dict.update({entry : 0})


      spl2_test_overlap_dict = deepcopy(spl2_overlap_dict)

      unique_list_test = list(mdf_test[column].unique())
      unique_list_test = list(map(str, unique_list_test))

      #here's where we identify values to set to 0 for spl5
      spl5_test_zero_dict = {}
      for entry in unique_list_test:
        if entry not in spl2_test_overlap_dict:
          spl5_test_zero_dict.update({entry : 0})


#       newcolumns = []

#       for dict_key in overlap_dict:

      newcolumn = column + '_sp10'

#         mdf_test[newcolumn] = mdf_test[column].isin(test_overlap_dict[dict_key])
#         mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

#       mdf_train[newcolumn] = mdf_train[column].copy()
      mdf_test[newcolumn] = mdf_test[column].copy()

#       mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
      mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl2_test_overlap_dict)
      mdf_test[newcolumn] = mdf_test[newcolumn].replace(spl5_test_zero_dict)

#       newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  def postprocess_sp15_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_splt_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    
    #here in postprocess we only create columns foir those overlaps that were identified 
    #fromthe train set
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'splt_newcolumns_sp15' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_sp15']
      
      concurrent_activations = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['concurrent_activations']



      #now for mdf_test we'll only consider those overlaps already identified from train set

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for key in train_keys:

        test_overlap_dict.update({key:[]})

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                
                if concurrent_activations is False:

                  break


      newcolumns = []

      for dict_key in overlap_dict:

        newcolumn = column + '_sp15_' + dict_key
        
#         mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

#         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  def postprocess_sp16_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_sp16_class(mdf_test, column, postprocess_dict, category)
    #preprocess column with categorical entries as strings
    #identifies overlaps of subsets of those strings and records
    #as a new boolan column
    #for example, if a categoical set consisted of unique values ['west', 'north', 'northeast']
    #then a new column would be created idenitifying cells which included 'north' in their entries
    #(here for north and northeast)
    #returns as column titled origcolumn_splt_entry    
    #missing values are ignored by default
    
    #here in postprocess we only create columns foir those overlaps that were identified 
    #fromthe train set
    
    #comparable to splt but
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    
    #sp16 is comparable to spl8 but multiple concurrent activations allowed
    #so requires a different MLinfilltype in processdict
    '''
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'splt_newcolumns_sp16' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_sp16']


      #now for mdf_test we'll only consider those overlaps already identified from train set
      
      #this assumes that set of unique values is same or subset of those for train set
      #for more efficient application in postmunge

#       unique_list_test = list(mdf_test[column].unique())

#       unique_list_test = list(map(str, unique_list_test))

#       test_overlap_dict = {}

#       train_keys = list(overlap_dict)

#       train_keys.sort(key = len, reverse=True)

#       for key in train_keys:

#         test_overlap_dict.update({key:[]})

#       for dict_key in train_keys:

#         for unique_test in unique_list_test:

#           len_key = len(dict_key)

#           if len(unique_test) >= len_key:

#             nbr_iterations4 = len(unique_test) - len_key

#             for l in range(nbr_iterations4 + 1):

#               extract4 = unique_test[l:(len_key+l)]

#               if extract4 == dict_key:

#                 test_overlap_dict[dict_key].append(unique_test)


      newcolumns = []

      for dict_key in overlap_dict:

        newcolumn = column + '_sp16_' + dict_key
        
#         mdf_train[newcolumn] = mdf_train[column].copy()
        mdf_test[newcolumn] = mdf_test[column].copy()

#         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

        #mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

        newcolumns.append(newcolumn)
    
    
    return mdf_test
  
  
  def postprocess_srch_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_srch_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #note this differs from original srch in that makes use of pandas str.contains
    #which is expected to be more efficient for unbounded sets
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'
    
    #note this returns all zeros in a column if search value not found
    
    #note returned coluymns are named by search term, e
    #e.g. column + '_srch_' + str(search)
    
    #note that search terms are converted to strings and compared to columns cast as strings

    #missing values are ignored by default
    """
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'srch_newcolumns_srch' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      search_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search_dict']
      inverse_search_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['inverse_search_dict']
      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['srch_newcolumns_srch']
#       newcolumns_before_aggregation = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['newcolumns_before_aggregation']
#       search = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search']
#       search_preflattening = \
#       postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search_preflattening']
      aggregated_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['aggregated_dict']
      case = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['case']
      
      for newcolumn in search_dict:

        mdf_test[newcolumn] = \
        np.where(mdf_test[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)
        
      #now we consolidate activations
      #note that this only runs when aggregated_dict was populated with an embedded list of search terms
      for aggregated_dict_key in aggregated_dict:
        aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]

        for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
          target_for_aggregation_column = inverse_search_dict[target_for_aggregation]

          mdf_test[aggregated_dict_key_column] = \
          np.where(mdf_test[target_for_aggregation_column] == 1, 1, mdf_test[aggregated_dict_key_column])

          del mdf_test[target_for_aggregation_column]

      for newcolumn in newcolumns:

        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)
    
    
    return mdf_test

  
  def postprocess_src2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_src2_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'

    #missing values are ignored by default
    
    #assumes that unique values of test set are same or subset of train set
    #for more efficient application in postmunge
    """
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'src2_newcolumns_src2' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['newcolumns_before_aggregation']
      
      search = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search']
      
      inverse_search_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['inverse_search_dict']
      
      aggregated_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['aggregated_dict']

      
#       #now for mdf_test

#       unique_list_test = list(mdf_test[column].unique())

#       unique_list_test = list(map(str, unique_list_test))

#       test_overlap_dict = {}

#       for search_string in search:

#         test_overlap_dict.update({search_string : []})


#       train_keys = list(overlap_dict)

#       train_keys.sort(key = len, reverse=True)

#       for dict_key in train_keys:

#         for unique_test in unique_list_test:

#           len_key = len(dict_key)

#           if len(unique_test) >= len_key:

#             nbr_iterations4 = len(unique_test) - len_key

#             for l in range(nbr_iterations4 + 1):

#               extract4 = unique_test[l:(len_key+l)]

#               if extract4 == dict_key:

#                 test_overlap_dict[dict_key].append(unique_test)
                


      newcolumns = []

      for dict_key in overlap_dict:
        
        if len(overlap_dict[dict_key]) > 0:

          newcolumn = column + '_src2_' + dict_key

  #         mdf_train[newcolumn] = mdf_train[column].copy()
          mdf_test[newcolumn] = mdf_test[column].copy()

  #         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
          mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

          mdf_test[newcolumn] = mdf_test[newcolumn].isin(overlap_dict[dict_key])
#           mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

          newcolumns.append(newcolumn)
          
          
      #now we consolidate activations
      #note that this only runs when aggregated_dict was populated with an embedded list of search terms
      for aggregated_dict_key in aggregated_dict:
        aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]

        for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
          target_for_aggregation_column = inverse_search_dict[target_for_aggregation]

          mdf_test[aggregated_dict_key_column] = \
          np.where(mdf_test[target_for_aggregation_column] == 1, 1, mdf_test[aggregated_dict_key_column])

          del mdf_test[target_for_aggregation_column]

          newcolumns.remove(target_for_aggregation_column)

      for newcolumn in newcolumns:

        mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)
    
    return mdf_test
  
  

  def postprocess_src3_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_src3_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'

    #missing values are ignored by default
    
    #where srch is preferred for unbounded range of unique values
    
    #and src2 preferred when have bounded range of unique values for both train & test
    
    #and speculation is that src3 may be preferred when have a bounded
    #range of unique values but still want capacity to handle values in 
    #test set not found in train set
    """
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'srch_newcolumns_src3' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          normkey = columnkey
        
    if normkey is not False:

      #great now we can grab normalization parameters
      overlap_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

      newcolumns = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['srch_newcolumns_src3']
      
      search = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search']

      
      #now for mdf_test

      unique_list_test = list(mdf_test[column].unique())

      unique_list_test = list(map(str, unique_list_test))

      test_overlap_dict = {}

      for search_string in search:

        test_overlap_dict.update({search_string : []})


      train_keys = list(overlap_dict)

      train_keys.sort(key = len, reverse=True)

      for dict_key in train_keys:

        for unique_test in unique_list_test:

          len_key = len(dict_key)

          if len(unique_test) >= len_key:

            nbr_iterations4 = len(unique_test) - len_key

            for l in range(nbr_iterations4 + 1):

              extract4 = unique_test[l:(len_key+l)]

              if extract4 == dict_key:

                test_overlap_dict[dict_key].append(unique_test)
                


      newcolumns = []

      for dict_key in overlap_dict:
        
        if len(overlap_dict[dict_key]) > 0:

          newcolumn = column + '_src3_' + dict_key

  #         mdf_train[newcolumn] = mdf_train[column].copy()
          mdf_test[newcolumn] = mdf_test[column].copy()

  #         mdf_train[newcolumn] = mdf_train[newcolumn].astype(str)
          mdf_test[newcolumn] = mdf_test[newcolumn].astype(str)

          mdf_test[newcolumn] = mdf_test[newcolumn].isin(test_overlap_dict[dict_key])
          mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

          newcolumns.append(newcolumn)
    
    
    return mdf_test
  

  def postprocess_src4_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_srch_class(mdf_train, mdf_test, column, category)
    #preprocess column with categorical entries as strings
    #relies on user passed list of strings in search parameter
    #string parses unique entries to identify overlaps with search strings
    #when overlap found returns a column with boolean activation identifiers
    
    #note this differs from original srch in that makes use of pandas str.contains
    #which is expected to be more efficient for unbounded sets
    
    #for example, if a categoical set consisted of unique values 
    #['west', 'north', 'northwest']
    #and a user passed the search parameter as ['west']
    #then a new column would be returned 
    #with activations corresponding to entries of 'west' and 'northwest'
    
    #note this returns all zeros in a column if search value not found
    
    #note returned coluymns are named by search term, e
    #e.g. column + '_srch_' + str(search)
    
    #note that search terms are converted to strings and compared to columns cast as strings

    #missing values are ignored by default
    
    #src4 builds on the srch by converting to an ordinal activation
    #with 0 reserved for no activations
    #note that if an entry was activated for multiple search terms
    #the order of entries in search parameter will dictate the final encoding
    #(e.g. entries at end of list are prioritized over beginning)
    """
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = column + '_src4'

    #great now we can grab normalization parameters
    search_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search_dict']

    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['srch_newcolumns_src4']

    search = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search']

    case = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['case']

    ordl_dict1 = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_dict1']

    ordl_dict2 = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_dict2']

    aggregated_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['aggregated_dict']
    
    inverse_search_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['inverse_search_dict']
    
    if len(search_dict) == 0:
      mdf_test[column + '_src4'] = 0
      
    else:

      for newcolumn in search_dict:

        mdf_test[newcolumn] = \
        np.where(mdf_test[column].astype(str).str.contains(search_dict[newcolumn], case=case, regex=False), 1, 0)

  #     for newcolumn in newcolumns:

  #       mdf_test[newcolumn] = mdf_test[newcolumn].astype(np.int8)

      mdf_test[column + '_src4'] = 0

      for newcolumn in newcolumns:
        mdf_test[column + '_src4'] = \
        np.where(mdf_test[newcolumn] == 1, ordl_dict2[newcolumn], mdf_test[column + '_src4'])
        del mdf_test[newcolumn]
        
        
      #now we consolidate activations
      #note that this only runs when aggregated_dict was populated with an embedded list of search terms
      for aggregated_dict_key in aggregated_dict:
        aggregated_dict_key_column = inverse_search_dict[aggregated_dict_key]
        aggregated_dict_key_encoding = ordl_dict2[aggregated_dict_key_column]

        for target_for_aggregation in aggregated_dict[aggregated_dict_key]:
          target_for_aggregation_column = inverse_search_dict[target_for_aggregation]
          target_for_aggregation_encoding = ordl_dict2[target_for_aggregation_column]

          mdf_test[column + '_src4'] = \
          np.where(mdf_test[column + '_src4'] == target_for_aggregation_encoding, aggregated_dict_key_encoding, mdf_test[column + '_src4'])


      #we'll base the integer type on number of ordinal entries
      if len(ordl_dict1) < 254:
        mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint8)
      elif len(ordl_dict1) < 65530:
        mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint16)
      else:
        mdf_test[column + '_src4'] = mdf_test[column + '_src4'].astype(np.uint32)
    
    
    return mdf_test
  
  
  def postprocess_nmr4_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_nmr4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #assumes set of entries in test data is same or subset of train data for
    #more efficient postmunge than vs nmrc
    """
    
    #get normkey
    normkey = column + '_nmr4'
    
    #retrieve normalization parameters
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    unique_list = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']
    
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill (mean)
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    for test_unique in extra_test_unique:
      test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmr4'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmr4'] = mdf_test[column + '_nmr4'].replace(test_overlap_dict)
    

    #replace missing data with training set mean as default infill
    mdf_test[column + '_nmr4'] = mdf_test[column + '_nmr4'].fillna(mean)
    
        
    return mdf_test
  
  def postprocess_nmr7_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_nmr4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #comparable to nmr4 but instead of making blanket assumption that unique values in
    #test set are found in train set, implements parsing for test set entries not found in train set
    """
    
    #get normkey
    normkey = column + '_nmr7'
    
    #retrieve normalization parameters
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    unique_list = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']
    
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    maxlength = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxlength']

    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill (mean)
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    
    
#     unique_list = list(mdf_train[column].unique())

#     unique_list = list(map(str, unique_list))
    
    testmaxlength = max(len(x) for x in unique_list)
    
    if testmaxlength > maxlength:
      maxlength = testmaxlength
    
    overlap_lengths = list(range(maxlength, 0, -1))

#     overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in extra_test_unique:
        
        if unique not in test_overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    test_overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    test_overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                test_overlap_dict.update({unique : np.nan})

    
#     test_overlap_dict = deepcopy(overlap_dict)
    
    
#     for test_unique in extra_test_unique:
#       test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmr7'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmr7'] = mdf_test[column + '_nmr7'].replace(test_overlap_dict)
    

    #replace missing data with training set mean as default infill
    mdf_test[column + '_nmr7'] = mdf_test[column + '_nmr7'].fillna(mean)
    
        
    return mdf_test
  
  def postprocess_nmc4_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_nmr4_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #assumes set of entries in test data is same or subset of train data for
    #more efficient postmunge than vs nmc
    """
    
    #get normkey
    normkey = column + '_nmc4'
    
    #retrieve normalization parameters
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    unique_list = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']
    
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']

    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill (mean)
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
    for test_unique in extra_test_unique:
      test_overlap_dict.update({str(test_unique) : np.nan})
    
    mdf_test[column + '_nmc4'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmc4'] = mdf_test[column + '_nmc4'].replace(test_overlap_dict)
    

    #replace missing data with training set mean as default infill
    mdf_test[column + '_nmc4'] = mdf_test[column + '_nmc4'].fillna(mean)
    
        
    return mdf_test
  
  def postprocess_nmc7_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    """
    #process_nmr7_class(df, column, category, postprocess_dict)
    #parses string entries and if any numbers present returns numbers
    #entries without numbers present subject to infill
    #comparable to nmc4 but instead of making blanket assumption that unique values in
    #test set are found in train set, implements parsing for test set entries not found in train set
    """
    
    #get normkey
    normkey = column + '_nmc7'
    
    #retrieve normalization parameters
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    unique_list = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['unique_list']
    
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    maxlength = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxlength']

    
    
    #note this assumes that set of entries for test data is same or a subset of train data
    #any entries not present will be subject to default infill (mean)
    #note that getNArows will not capture these infill for infill with assigninfill
    
    test_unique_list = list(mdf_test[column].unique())
    test_unique_list = list(map(str, test_unique_list))
    extra_test_unique = list(set(test_unique_list) - set(unique_list))
    
    test_overlap_dict = deepcopy(overlap_dict)
#     for test_unique in extra_test_unique:
#       test_overlap_dict.update({str(test_unique) : np.nan})


#     unique_list = list(mdf_train[column].unique())

#     unique_list = list(map(str, unique_list))
    
    testmaxlength = max(len(x) for x in unique_list)
    
    if testmaxlength > maxlength:
      maxlength = testmaxlength
    
    overlap_lengths = list(range(maxlength, 0, -1))

#     overlap_dict = {}
    
    for overlap_length in overlap_lengths:

      for unique in extra_test_unique:
        
        if unique not in test_overlap_dict:

          len_unique = len(unique)

          if len_unique >= overlap_length:
            
            if overlap_length > 1:

              nbr_iterations = len_unique - overlap_length

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    test_overlap_dict.update({unique : float(extract)})
                
            #else if overlap_length == 1    
            else:
              
              nbr_iterations = len_unique - overlap_length
              
              in_dict = False

              for i in range(nbr_iterations + 1):
                
                if unique not in test_overlap_dict:

                  extract = unique[i:(overlap_length+i)]

  #                 extract_already_in_overlap_dict = False

                  if self.is_number(extract):

                    in_dict = True

                    test_overlap_dict.update({unique : float(extract)})
                  
              if in_dict is False:

                test_overlap_dict.update({unique : np.nan})

    
    mdf_test[column + '_nmc7'] = mdf_test[column].astype(str)
    mdf_test[column + '_nmc7'] = mdf_test[column + '_nmc7'].replace(test_overlap_dict)
    

    #replace missing data with training set mean as default infill
    mdf_test[column + '_nmc7'] = mdf_test[column + '_nmc7'].fillna(mean)
    
        
    return mdf_test
  
  
  def postprocess_ordl_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_ordl_class(mdf_test, column, postprocess_dict, columnkey)
    #preprocess column with categories into ordinal (sequentuial integer) sets
    #corresponding to (sorted) categories
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories presetn in test set not present in train set use this 'zzz' category
    '''
    
    normkey = column + '_ordl'
    
    #grab normalization parameters from postprocess_dict
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    overlap_replace = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_overlap_replace']
    
    #create new column for trasnformation
    mdf_test[column + '_ordl'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')
    
    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_test[column + '_ordl'].cat.categories:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].fillna('zzzinfill')
    
    #replace numerical with string equivalent
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(str)    
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    #train categories are in the ordinal_dict we p[ulled from normalization_dict
    labels_train = list(ordinal_dict.keys())
    labels_train.sort()
    labels_test = list(mdf_test[column + '_ordl'].unique())
    labels_test.sort()
    
    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
      labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
      
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_replace) > 0:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(overlap_replace)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].replace(ordinal_dict)
    
    #just want to make sure these arent' being saved as floats for memory considerations
    if len(ordinal_dict) < 254:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint8)
    elif len(ordinal_dict) < 65530:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint16)
    else:
      mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype(np.uint32)
    
        
#     #convert column to category
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')
    
    return mdf_test
  
  def postprocess_ord3_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_ord3_class(mdf_test, column, postprocess_dict, columnkey)
    #preprocess column with categories into ordinal (sequentuial integer) sets
    #corresponding to categories sorted by frequency of occurance
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories presetn in test set not present in train set use this 'zzz' category
    '''
    
    normkey = column + '_ord3'
    
    #grab normalization parameters from postprocess_dict
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    overlap_replace = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_overlap_replace']
    
    #create new column for trasnformation
    mdf_test[column + '_ord3'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype('category')
    
    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_test[column + '_ord3'].cat.categories:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].fillna('zzzinfill')
    
    #replace numerical with string equivalent
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(str)    
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    #train categories are in the ordinal_dict we p[ulled from normalization_dict
    labels_train = list(ordinal_dict.keys())
#     labels_train.sort()
    labels_test = list(mdf_test[column + '_ord3'].unique())
    labels_test.sort()
    
    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
#       labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
      
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_replace) > 0:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(overlap_replace)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].replace(ordinal_dict)
    
    #just want to make sure these arent' being saved as floats for memory considerations
    if len(ordinal_dict) < 254:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint8)
    elif len(ordinal_dict) < 65530:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint16)
    else:
      mdf_test[column + '_ord3'] = mdf_test[column + '_ord3'].astype(np.uint32)
    
        
#     #convert column to category
#     mdf_test[column + '_ordl'] = mdf_test[column + '_ordl'].astype('category')
    
    return mdf_test
  
  
  def postprocess_ucct_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #process_ucct_class(mdf_train, mdf_test, column, category)
    #preprocess column with categories into unique class count sets
    #normalized by total row count
    #e.g. for each class in train set, 
    #counts instances and divides by total train set row count
    #(so values will fall in range 0-1)
    #test sets recive comparable encoding
    '''
    
    normkey = column + '_ucct'
    
    #grab normalization parameters from postprocess_dict
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    overlap_replace = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_overlap_replace']
    
    #create new column for trasnformation
    mdf_test[column + '_ucct'] = mdf_test[column].copy()
    
    #convert column to category
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].astype('category')
    
    #if set is categorical we'll need the plug value for missing values included
    if 'zzzinfill' not in mdf_test[column + '_ucct'].cat.categories:
      mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].cat.add_categories(['zzzinfill'])

    #replace NA with a dummy variable
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].fillna('zzzinfill')
    
    #replace numerical with string equivalent
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].astype(str)    
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array
    #train categories are in the ordinal_dict we p[ulled from normalization_dict
    labels_train = list(ordinal_dict.keys())
#     labels_train.sort()
    labels_test = list(mdf_test[column + '_ucct'].unique())
    labels_test.sort()
    
    #if infill not present in train set, insert
    if 'zzzinfill' not in labels_train:
      labels_train = labels_train + ['zzzinfill']
#       labels_train.sort()
    if 'zzzinfill' not in labels_test:
      labels_test = labels_test + ['zzzinfill']
      labels_test.sort()
      
    #here we replace the overlaps with version with jibberish suffix
    if len(overlap_replace) > 0:
      mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(overlap_replace)
    
    #in test set, we'll need to strike any categories that weren't present in train
    #first let'/s identify what applies
    testspecificcategories = list(set(labels_test)-set(labels_train))
    
    #so we'll just replace those items with our plug value
    testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(testplug_dict)
    
    #now we'll apply the ordinal transformation to the test set
    mdf_test[column + '_ucct'] = mdf_test[column + '_ucct'].replace(ordinal_dict)
    
    
    return mdf_test
  
  
  def postprocess_1010_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_1010_class(mdf_test, column, postprocess_dict, columnkey)
    #preprocess column with categories into binary encoded sets
    #corresponding to (sorted) categories of >2 values
    #adresses infill with new point which we arbitrarily set as 'zzzinfill'
    #intended to show up as last point in set alphabetically
    #for categories presetn in test set not present in train set use this 'zzz' category
    '''
    
    normkey = column + '_1010_0'
    
    if normkey in postprocess_dict['column_dict']:
    
      #grab normalization parameters from postprocess_dict
      binary_encoding_dict = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['_1010_binary_encoding_dict']

      overlap_replace = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['_1010_overlap_replace']

      binary_column_count = \
      postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['_1010_binary_column_count']


      #create new column for trasnformation
      mdf_test[column + '_1010'] = mdf_test[column].copy()    

      #convert column to category
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].astype('category')

      #if set is categorical we'll need the plug value for missing values included
      if 'zzzinfill' not in mdf_test[column + '_1010'].cat.categories:
        mdf_test[column + '_1010'] = mdf_test[column + '_1010'].cat.add_categories(['zzzinfill'])

      #replace NA with a dummy variable
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].fillna('zzzinfill')

      #replace numerical with string equivalent
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].astype(str)

      #extract categories for column labels
      #note that .unique() extracts the labels as a numpy array
      #train categories are in the ordinal_dict we p[ulled from normalization_dict
      labels_train = list(binary_encoding_dict.keys())
      labels_train.sort()
      labels_test = list(mdf_test[column + '_1010'].unique())
      labels_test.sort()

      #if infill not present in train set, insert
      if 'zzzinfill' not in labels_train:
        labels_train = labels_train + ['zzzinfill']
        labels_train.sort()
      if 'zzzinfill' not in labels_test:
        labels_test = labels_test + ['zzzinfill']
        labels_test.sort()    

      #here we replace the overlaps with version with jibberish suffix
      if len(overlap_replace) > 0:
        mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(overlap_replace)

      #in test set, we'll need to strike any categories that weren't present in train
      #first let'/s identify what applies
      testspecificcategories = list(set(labels_test)-set(labels_train))

      #so we'll just replace those items with our plug value
      testplug_dict = dict(zip(testspecificcategories, ['zzzinfill'] * len(testspecificcategories)))
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(testplug_dict)    

      #now we'll apply the 1010 transformation to the test set
      mdf_test[column + '_1010'] = mdf_test[column + '_1010'].replace(binary_encoding_dict)   

      #ok let's create a list of columns to store each entry of the binary encoding
      _1010_columnlist = []

      for i in range(binary_column_count):

        _1010_columnlist.append(column + '_1010_' + str(i))

      #now let's store the encoding
      i=0
      for _1010_column in _1010_columnlist:

        mdf_test[_1010_column] = mdf_test[column + '_1010'].str.slice(i,i+1).astype(np.int8)

        i+=1


      #now delete the support column
      del mdf_test[column + '_1010']
    
    
    return mdf_test
  


  def postprocess_year_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_year_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for year
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_year'
    
    meanyear = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanyear']
    
    stdyear = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdyear']

    #create copy of original column
    mdf_test[column + '_year'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_year'] = pd.to_datetime(mdf_test[column + '_year'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab year entries for test set
    mdf_test[column + '_year'] = mdf_test[column + '_year'].dt.year

    #replace missing data with training set mean
    mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(meanyear)

    #subtract mean from column for both train and test
    mdf_test[column + '_year'] = mdf_test[column + '_year'] - meanyear


    #divide column values by std for both training and test data
    mdf_test[column + '_year'] = mdf_test[column + '_year'] / stdyear

#     #now replace NaN with 0
#     mdf_test[column + '_year'] = mdf_test[column + '_year'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_year'] = mdf_test[column + '_year'].astype(np.float32)


    return mdf_test


  def postprocess_mnth_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mnth_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for year
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mnth'
    
    meanmonth = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanmonth']
    
    stdmonth = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdmonth']

    #create copy of original column
    mdf_test[column + '_mnth'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mnth'] = pd.to_datetime(mdf_test[column + '_mnth'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].dt.month

    #replace missing data with training set mean
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].fillna(meanmonth)

    #subtract mean from column for both train and test
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'] - meanmonth


    #divide column values by std for both training and test data
    mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'] / stdmonth

#     #now replace NaN with 0
#     mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_mnth'] = mdf_test[column + '_mnth'].astype(np.float32)

    return mdf_test



  def postprocess_mnsn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mnsn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for month
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mnsn'
    
    mean_mnsn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mnsn']

    #create copy of original column
    mdf_test[column + '_mnsn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mnsn'] = pd.to_datetime(mdf_test[column + '_mnsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].dt.month
    
    #apply sin transform
    mdf_test[column + '_mnsn'] = np.sin(mdf_test[column + '_mnsn'] * 2 * np.pi / 12 )

    #replace missing data with training set mean
    mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].fillna(mean_mnsn)
    
#     #change data type for memory savings
#     mdf_test[column + '_mnsn'] = mdf_test[column + '_mnsn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mncs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mncs_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for month
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mncs'
    
    mean_mncs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mncs']

    #create copy of original column
    mdf_test[column + '_mncs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mncs'] = pd.to_datetime(mdf_test[column + '_mncs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].dt.month
    
    #apply cos transform
    mdf_test[column + '_mncs'] = np.cos(mdf_test[column + '_mncs'] * 2 * np.pi / 12 )

    #replace missing data with training set mean
    mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].fillna(mean_mncs)
    
#     #change data type for memory savings
#     mdf_test[column + '_mncs'] = mdf_test[column + '_mncs'].astype(np.float32)
    
    return mdf_test

  
  def postprocess_mdsn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mdsn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined columns for month and day
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mdsn'
    
    mean_mdsn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mdsn']

    #create copy of original column
    mdf_test[column + '_mdsn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mdsn'] = pd.to_datetime(mdf_test[column + '_mdsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month

    #convert months to number of days in a temp column to support periodicity trasnform

    mdf_test[column + '_mdsn' + '_temp'] = mdf_test[column + '_mdsn'].copy()
    mdf_test[column + '_mdsn' + '_temp_leap'] = mdf_test[column + '_mdsn'].copy()

    mdf_test[column + '_mdsn' + '_temp'] = mdf_test[column + '_mdsn' + '_temp'].dt.month
    mdf_test[column + '_mdsn' + '_temp_leap'] = mdf_test[column + '_mdsn' + '_temp_leap'].dt.is_leap_year

    mdf_test[column + '_mdsn' + '_temp_leap'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp_leap'], 29, 28)
    
    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([4,6,9,11]), 30, mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([2]), mdf_test[column + '_mdsn' + '_temp_leap'], \
    mdf_test[column + '_mdsn' + '_temp'].values)

    mdf_test[column + '_mdsn' + '_temp'] = \
    np.where(mdf_test[column + '_mdsn' + '_temp'].isin([28,29,30,31]), mdf_test[column + '_mdsn' + '_temp'].values, 30.42)
    
    #apply sin transform to combined day and month, note average of 30.42 days in a month, 12 months in a year
    mdf_test[column + '_mdsn'] = np.sin((mdf_test[column + '_mdsn'].dt.month + mdf_test[column + '_mdsn'].dt.day / \
    mdf_test[column + '_mdsn' + '_temp']) * 2 * np.pi / 12 )

    #delete the support column
    del mdf_test[column + '_mdsn' + '_temp']
    del mdf_test[column + '_mdsn' + '_temp_leap']

    #replace missing data with training set mean
    mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].fillna(mean_mdsn)
    
#     #change data type for memory savings
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mdcs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mdsn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined columns for month and day
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mdcs'
    
    mean_mdcs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mdcs']

    #create copy of original column
    mdf_test[column + '_mdcs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mdcs'] = pd.to_datetime(mdf_test[column + '_mdcs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month

    #convert months to number of days in a temp column to support periodicity trasnform

    mdf_test[column + '_mdcs' + '_temp'] = mdf_test[column + '_mdcs'].copy()
    mdf_test[column + '_mdcs' + '_temp_leap'] = mdf_test[column + '_mdcs'].copy()

    mdf_test[column + '_mdcs' + '_temp'] = mdf_test[column + '_mdcs' + '_temp'].dt.month
    mdf_test[column + '_mdcs' + '_temp_leap'] = mdf_test[column + '_mdcs' + '_temp_leap'].dt.is_leap_year

    mdf_test[column + '_mdcs' + '_temp_leap'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp_leap'], 29, 28)
    
    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([1,3,5,7,8,10,12]), 31, mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([4,6,9,11]), 30, mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([2]), mdf_test[column + '_mdcs' + '_temp_leap'], \
    mdf_test[column + '_mdcs' + '_temp'].values)

    mdf_test[column + '_mdcs' + '_temp'] = \
    np.where(mdf_test[column + '_mdcs' + '_temp'].isin([28,29,30,31]), mdf_test[column + '_mdcs' + '_temp'].values, 30.42)
    
    #apply sin transform to combined day and month, note average of 30.42 days in a month, 12 months in a year
    mdf_test[column + '_mdcs'] = np.cos((mdf_test[column + '_mdcs'].dt.month + mdf_test[column + '_mdcs'].dt.day / \
    mdf_test[column + '_mdcs' + '_temp']) * 2 * np.pi / 12 )

    #delete support column
    del mdf_test[column + '_mdcs' + '_temp']
    del mdf_test[column + '_mdcs' + '_temp_leap']

    #replace missing data with training set mean
    mdf_test[column + '_mdcs'] = mdf_test[column + '_mdcs'].fillna(mean_mdcs)
    
#     #change data type for memory savings
#     mdf_test[column + '_mdcs'] = mdf_test[column + '_mdcs'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_days_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_days_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for days
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_days'
    
    meanday = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanday']
    
    stdday = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdday']

    #create copy of original column
    mdf_test[column + '_days'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_days'] = pd.to_datetime(mdf_test[column + '_days'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_days'] = mdf_test[column + '_days'].dt.day

    #replace missing data with training set mean
    mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(meanday)

    #subtract mean from column for both train and test
    mdf_test[column + '_days'] = mdf_test[column + '_days'] - meanday


    #divide column values by std for both training and test data
    mdf_test[column + '_days'] = mdf_test[column + '_days'] / stdday

#     #now replace NaN with 0
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].astype(np.float32)

    return mdf_test



  def postprocess_dysn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_dysn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for days
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_dysn'
    
    mean_dysn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_dysn']

    #create copy of original column
    mdf_test[column + '_dysn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_dysn'] = pd.to_datetime(mdf_test[column + '_dysn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].dt.day
    
    #apply sin transform
    #average number of days in a month is 30.42
    #7 days in a week
    mdf_test[column + '_dysn'] = np.sin(mdf_test[column + '_dysn'] * 2 * np.pi / 7 )

    #replace missing data with training set mean
    mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].fillna(mean_dysn)
    
#     #change data type for memory savings
#     mdf_test[column + '_dysn'] = mdf_test[column + '_dysn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_dycs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_dycs_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for days
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_dycs'
    
    mean_dycs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_dycs']

    #create copy of original column
    mdf_test[column + '_dycs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_dycs'] = pd.to_datetime(mdf_test[column + '_dycs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].dt.day
    
    #apply sin transform
    #average number of days in a month is 30.42
    #7 days in a week
    mdf_test[column + '_dycs'] = np.cos(mdf_test[column + '_dycs'] * 2 * np.pi / 7 )

    #replace missing data with training set mean
    mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].fillna(mean_dycs)
    
#     #change data type for memory savings
#     mdf_test[column + '_dycs'] = mdf_test[column + '_dycs'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_dhms_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_dhms_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for day, hours, and minutes
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_dhms'
    
    mean_dhms = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_dhms']

    #create copy of original column
    mdf_test[column + '_dhms'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_dhms'] = pd.to_datetime(mdf_test[column + '_dhms'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined day hour minute
    #7 days in a week
    mdf_test[column + '_dhms'] = np.sin((mdf_test[column + '_dhms'].dt.day + mdf_test[column + '_dhms'].dt.hour / 24 + mdf_test[column + '_dhms'].dt.minute / 24 / 60) * 2 * np.pi / 7 )

    
    #replace missing data with training set mean
    mdf_test[column + '_dhms'] = mdf_test[column + '_dhms'].fillna(mean_dhms)
    
#     #change data type for memory savings
#     mdf_test[column + '_dhms'] = mdf_test[column + '_dhms'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_dhmc_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_dhmc_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for day, hours, and minutes
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_dhmc'
    
    mean_dhmc = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_dhmc']

    #create copy of original column
    mdf_test[column + '_dhmc'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_dhmc'] = pd.to_datetime(mdf_test[column + '_dhmc'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined day hour minute
    #7 days in. a week
    mdf_test[column + '_dhmc'] = np.cos((mdf_test[column + '_dhmc'].dt.day + mdf_test[column + '_dhmc'].dt.hour / 24 + mdf_test[column + '_dhmc'].dt.minute / 24 / 60) * 2 * np.pi / 7 )

    
    #replace missing data with training set mean
    mdf_test[column + '_dhmc'] = mdf_test[column + '_dhmc'].fillna(mean_dhmc)
    
#     #change data type for memory savings
#     mdf_test[column + '_dhmc'] = mdf_test[column + '_dhmc'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_hour_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_hour_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for hours
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_hour'
    
    meanhour = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanhour']
    
    stdhour = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdhour']

    #create copy of original column
    mdf_test[column + '_hour'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_hour'] = pd.to_datetime(mdf_test[column + '_hour'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'].dt.hour

    #replace missing data with training set mean
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'].fillna(meanhour)

    #subtract mean from column for both train and test
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'] - meanhour


    #divide column values by std for both training and test data
    mdf_test[column + '_hour'] = mdf_test[column + '_hour'] / stdhour

#     #now replace NaN with 0
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_hour'] = mdf_test[column + '_hour'].astype(np.float32)

    return mdf_test



  def postprocess_hrsn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_dysn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for hours
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_hrsn'
    
    mean_hrsn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_hrsn']

    #create copy of original column
    mdf_test[column + '_hrsn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_hrsn'] = pd.to_datetime(mdf_test[column + '_hrsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].dt.hour
    
    #apply sin transform
    #average number of hours in a day is ~24
    mdf_test[column + '_hrsn'] = np.sin(mdf_test[column + '_hrsn'] * 2 * np.pi / 24 )

    #replace missing data with training set mean
    mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].fillna(mean_hrsn)
    
#     #change data type for memory savings
#     mdf_test[column + '_hrsn'] = mdf_test[column + '_hrsn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_hrcs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_hrcs_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for hours
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_hrcs'
    
    mean_hrcs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_hrcs']

    #create copy of original column
    mdf_test[column + '_hrcs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_hrcs'] = pd.to_datetime(mdf_test[column + '_hrcs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].dt.hour
    
    #apply sin transform
    #average number of hours in a day is ~24
    mdf_test[column + '_hrcs'] = np.cos(mdf_test[column + '_hrcs'] * 2 * np.pi / 24 )

    #replace missing data with training set mean
    mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].fillna(mean_hrcs)
    
#     #change data type for memory savings
#     mdf_test[column + '_hrcs'] = mdf_test[column + '_hrcs'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_hmss_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_hmss_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for hours, minutes, and seconds
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_hmss'
    
    mean_hmss = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_hmss']

    #create copy of original column
    mdf_test[column + '_hmss'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_hmss'] = pd.to_datetime(mdf_test[column + '_hmss'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined hour minute sec
    #24 hours in a day
    mdf_test[column + '_hmss'] = np.sin((mdf_test[column + '_hmss'].dt.hour + mdf_test[column + '_hmss'].dt.minute / 60 + mdf_test[column + '_hmss'].dt.second / 60 / 60) * 2 * np.pi / 24 )

    
    #replace missing data with training set mean
    mdf_test[column + '_hmss'] = mdf_test[column + '_hmss'].fillna(mean_hmss)
    
#     #change data type for memory savings
#     mdf_test[column + '_hmss'] = mdf_test[column + '_hmss'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_hmsc_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_hmsc_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for hours, minutes, and seconds
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_hmsc'
    
    mean_hmsc = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_hmsc']

    #create copy of original column
    mdf_test[column + '_hmsc'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_hmsc'] = pd.to_datetime(mdf_test[column + '_hmsc'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined hour minute sec
    #24 hours in a. day
    mdf_test[column + '_hmsc'] = np.cos((mdf_test[column + '_hmsc'].dt.hour + mdf_test[column + '_hmsc'].dt.minute / 60 + mdf_test[column + '_hmsc'].dt.second / 60 / 60) * 2 * np.pi / 24 )

    
    #replace missing data with training set mean
    mdf_test[column + '_hmsc'] = mdf_test[column + '_hmsc'].fillna(mean_hmsc)
    
#     #change data type for memory savings
#     mdf_test[column + '_hmsc'] = mdf_test[column + '_hmsc'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mint_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mint_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for minutes
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mint'
    
    meanmint = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanmint']
    
    stdmint = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdmint']

    #create copy of original column
    mdf_test[column + '_mint'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mint'] = pd.to_datetime(mdf_test[column + '_mint'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'].dt.minute

    #replace missing data with training set mean
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'].fillna(meanmint)

    #subtract mean from column for both train and test
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'] - meanmint


    #divide column values by std for both training and test data
    mdf_test[column + '_mint'] = mdf_test[column + '_mint'] / stdmint

#     #now replace NaN with 0
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_mint'] = mdf_test[column + '_mint'].astype(np.float32)

    return mdf_test



  def postprocess_misn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_misn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for minutes
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_misn'
    
    mean_misn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_misn']

    #create copy of original column
    mdf_test[column + '_misn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_misn'] = pd.to_datetime(mdf_test[column + '_misn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_misn'] = mdf_test[column + '_misn'].dt.minute
    
    #apply sin transform
    #60 minutes in an hour, 24 hours in day
    mdf_test[column + '_misn'] = np.sin(mdf_test[column + '_misn'] * 2 * np.pi / 60 / 24 )

    #replace missing data with training set mean
    mdf_test[column + '_misn'] = mdf_test[column + '_misn'].fillna(mean_misn)
    
#     #change data type for memory savings
#     mdf_test[column + '_misn'] = mdf_test[column + '_misn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mics_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mics_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for minutes
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mics'
    
    mean_mics = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mics']

    #create copy of original column
    mdf_test[column + '_mics'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mics'] = pd.to_datetime(mdf_test[column + '_mics'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_mics'] = mdf_test[column + '_mics'].dt.minute
    
    #apply sin transform
    #60 minutes in an hour, 24 hours in day
    mdf_test[column + '_mics'] = np.cos(mdf_test[column + '_mics'] * 2 * np.pi / 60 / 24 )

    #replace missing data with training set mean
    mdf_test[column + '_mics'] = mdf_test[column + '_mics'].fillna(mean_mics)
    
#     #change data type for memory savings
#     mdf_test[column + '_mics'] = mdf_test[column + '_mics'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mssn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mssn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for minutes, and seconds
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mssn'
    
    mean_mssn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mssn']

    #create copy of original column
    mdf_test[column + '_mssn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mssn'] = pd.to_datetime(mdf_test[column + '_mssn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined minute sec
    mdf_test[column + '_mssn'] = np.sin((mdf_test[column + '_mssn'].dt.minute + mdf_test[column + '_mssn'].dt.second / 60 ) * 2 * np.pi / 60 / 24 )

    
    #replace missing data with training set mean
    mdf_test[column + '_mssn'] = mdf_test[column + '_mssn'].fillna(mean_mssn)
    
#     #change data type for memory savings
#     mdf_test[column + '_mssn'] = mdf_test[column + '_mssn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_mscs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_mscs_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds combined column for minutes, and seconds
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_mscs'
    
    mean_mscs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_mscs']

    #create copy of original column
    mdf_test[column + '_mscs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_mscs'] = pd.to_datetime(mdf_test[column + '_mscs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

#     #grab month entries for test set
#     mdf_test[column + '_mdsn'] = mdf_test[column + '_mdsn'].dt.month
    
    #apply sin transform to combined minute sec
    mdf_test[column + '_mscs'] = np.cos((mdf_test[column + '_mscs'].dt.minute + mdf_test[column + '_mscs'].dt.second / 60 ) * 2 * np.pi / 60 / 24 )

    
    #replace missing data with training set mean
    mdf_test[column + '_mscs'] = mdf_test[column + '_mscs'].fillna(mean_mscs)
    
#     #change data type for memory savings
#     mdf_test[column + '_mscs'] = mdf_test[column + '_mscs'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_scnd_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_scnd_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for seconds
    #z score normalized to the mean and std from original train set, 
    #with missing values plugged with the mean
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_scnd'
    
    meanscnd = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['meanscnd']
    
    stdscnd = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['stdscnd']

    #create copy of original column
    mdf_test[column + '_scnd'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_scnd'] = pd.to_datetime(mdf_test[column + '_scnd'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].dt.second

    #replace missing data with training set mean
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].fillna(meanscnd)

    #subtract mean from column for both train and test
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'] - meanscnd


    #divide column values by std for both training and test data
    mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'] / stdscnd

#     #now replace NaN with 0
#     mdf_test[column + '_days'] = mdf_test[column + '_days'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_scnd'] = mdf_test[column + '_scnd'].astype(np.float32)


    return mdf_test



  def postprocess_scsn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_scsn_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for seconds
    #with sin transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_scsn'
    
    mean_scsn = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_scsn']

    #create copy of original column
    mdf_test[column + '_scsn'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_scsn'] = pd.to_datetime(mdf_test[column + '_scsn'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].dt.second
    
    #apply sin transform
    #60 seconds in a minute
    mdf_test[column + '_scsn'] = np.sin(mdf_test[column + '_scsn'] * 2 * np.pi / 60 )

    #replace missing data with training set mean
    mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].fillna(mean_scsn)
    
#     #change data type for memory savings
#     mdf_test[column + '_scsn'] = mdf_test[column + '_scsn'].astype(np.float32)
    
    return mdf_test
  
  
  def postprocess_sccs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):

    '''
    #postprocess_sccs_class(mdf_test, column, postprocess_dict, columnkey)
    #postprocess test column with of date category
    #takes as arguement pandas dataframe containing test data 
    #(mdf_test), the name of the column string ('column'), and the timenormalization_dict 
    #from the original application of automunge to the associated date column from train set
    #(saved in the postprocess_dict)
    #retains the original column from master dataframe and
    #adds distinct columns for seconds
    #with cos transform, 
    #with missing values plugged with the mean from train set after sin transform
    #with columns named after column_ + time category
    #returns mdf_test
    '''
    
    #retrieve normalization parameters from postprocess_dict
    datekey = column + '_sccs'
    
    mean_sccs = \
    postprocess_dict['column_dict'][datekey]['normalization_dict'][datekey]['mean_sccs']

    #create copy of original column
    mdf_test[column + '_sccs'] = mdf_test[column].copy()

    #apply pd.to_datetime to column, note that the errors = 'coerce' needed for messy data
    mdf_test[column + '_sccs'] = pd.to_datetime(mdf_test[column + '_sccs'], errors = 'coerce')

    #mdf_train[column].replace(-np.Inf, np.nan)
    #mdf_test[column].replace(-np.Inf, np.nan)

    #grab month entries for test set
    mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].dt.second
    
    #apply sin transform
    #60 seconds in a minute
    mdf_test[column + '_sccs'] = np.cos(mdf_test[column + '_sccs'] * 2 * np.pi / 60 )

    #replace missing data with training set mean
    mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].fillna(mean_sccs)
    
#     #change data type for memory savings
#     mdf_test[column + '_sccs'] = mdf_test[column + '_sccs'].astype(np.float32)
    
    return mdf_test
  
  
  
  
  
  def postprocess_bxcx_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    Applies box-cox method within postmunge function.
    '''
  #   #df_train, nmbrcolumns, nmbrnormalization_dict, categorylist = \
  #   mdf_train, column_dict_list = \
  #   self.process_bxcx_support(mdf_train, column, category, 1, bxcx_lmbda = None, \
  #                             trnsfrm_mean = None, trnsfrm_std = None)

  #   #grab the normalization_dict associated with the bxcx category
  #   columnkeybxcx = column + '_bxcx'
  #   for column_dict in column_dict_list:
  #     if columnkeybxcx in column_dict:
  #       bxcxnormalization_dict = column_dict[columnkeybxcx]['normalization_dict'][columnkey]


    #bxcxkey = columnkey[:-5] + '_bxcx'


    #grab the normalization_dict associated with the bxcx category
    normkey = column+'_bxcx'
    bxcxnormalization_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]
    #postprocess_dict['column_dict'][bxcxkey]['normalization_dict'][bxcxkey]
    
    temporigcategoryplug = 'bxcx'

    #df_test, nmbrcolumns, _1, _2 = \
    mdf_test, _1 = \
    self.process_bxcx_support(mdf_test, column, temporigcategoryplug, 1, bxcx_lmbda = \
                             bxcxnormalization_dict['bxcx_lmbda'], \
                             trnsfrm_mean = bxcxnormalization_dict['trnsfrm_mean'])

    return mdf_test



  def postprocess_log0_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
        
    '''
    #function to apply logatrithmic transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a logarithmic transform (base 10)
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_log0'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_log0'
    
    meanlog = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['meanlog']

    #copy original column for implementation
    mdf_test[column + '_log0'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_log0'] = pd.to_numeric(mdf_test[column + '_log0'], errors='coerce')
    
    #replace all non-positive with nan for the log operation
    mdf_test.loc[mdf_test[column + '_log0'] <= 0, (column + '_log0')] = np.nan
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with meanlog
    mdf_test[column + '_log0'] = np.log10(mdf_test[column + '_log0'])
    

    #get mean of training data
    meanlog = meanlog  

    #replace missing data with training set mean
    mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(meanlog)

#     #replace missing data with 0
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    return mdf_test

  def postprocess_logn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
        
    '''
    #function to apply natural logatrithmic transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a logarithmic transform (base e)
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_logn'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_logn'
    
    meanlog = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['meanlog']

    #copy original column for implementation
    mdf_test[column + '_logn'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_logn'] = pd.to_numeric(mdf_test[column + '_logn'], errors='coerce')
    
    #replace all non-positive with nan for the log operation
    mdf_test.loc[mdf_test[column + '_logn'] <= 0, (column + '_logn')] = np.nan
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with meanlog
    mdf_test[column + '_logn'] = np.log(mdf_test[column + '_logn'])
    

    #get mean of training data
    meanlog = meanlog  

    #replace missing data with training set mean
    mdf_test[column + '_logn'] = mdf_test[column + '_logn'].fillna(meanlog)

#     #replace missing data with 0
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    return mdf_test
    
  
  def postprocess_pwrs_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #we'll use an initial plug value of 0
    '''
    
    #retrieve normalization parameters from postprocess_dict
    for power in range(20):
      power = str(power)
      if (column + '_10^' + power) in postprocess_dict['column_dict']:
        if (column + '_10^' + power) in postprocess_dict['column_dict'][(column + '_10^' + power)]['normalization_dict']:
            normkey = (column + '_10^' + power)
    
    #normkey = columnkey
    
    meanlog = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['meanlog']
    maxlog = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxlog']
    powerlabelsdict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['powerlabelsdict']
    
    textcolumns = postprocess_dict['column_dict'][normkey]['categorylist']
    
    tempcolumn = column + '_:;:_temp'
    
    #store original column for later reversion
    mdf_test[tempcolumn] = mdf_test[column].copy()
    
    #convert all values to either numeric or NaN
    mdf_test[tempcolumn] = pd.to_numeric(mdf_test[tempcolumn], errors='coerce')
    
    #convert all values <= 0 to Nan
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] <= 0, np.nan, mdf_test[tempcolumn].values)
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with meanlog
#     mdf_test[column] = np.floor(np.log10(mdf_test[column]))
    mdf_test[tempcolumn] = \
    np.where(mdf_test[tempcolumn] != np.nan, np.floor(np.log10(mdf_test[tempcolumn])), mdf_test[tempcolumn].values)
    
#     #replace missing data with training set mean
#     mdf_test[column] = mdf_test[column].fillna(meanlog)

    #replace missing data with 0
    mdf_test[tempcolumn] = mdf_test[tempcolumn].fillna(0)
    
    #replace numerical with string equivalent
    mdf_test[tempcolumn] = mdf_test[tempcolumn].astype(int).astype(str)
    
    #extract categories for column labels
    #note that .unique() extracts the labels as a numpy array

    #we'll get the category names from the textcolumns array by stripping the \
    #prefixes of column name + '_'
    prefixlength = len(column)+1
    labels_train = textcolumns[:]
    for textcolumn in labels_train:
      textcolumn = textcolumn[prefixlength :]
    #labels_train.sort(axis=0)
    labels_train.sort()
    labels_test = mdf_test[tempcolumn].unique()
    labels_test.sort(axis=0)
    
    #apply onehotencoding
    df_test_cat = pd.get_dummies(mdf_test[tempcolumn])
    
    #append column header name to each category listing
    #note the iteration is over a numpy array hence the [...] approach
    labels_test[...] = column + '_10^' + labels_test[...]
    
    #convert sparse array to pandas dataframe with column labels
    df_test_cat.columns = labels_test
    
    #Get missing columns in test set that are present in training set
    missing_cols = set( textcolumns ) - set( df_test_cat.columns )
    
    #Add a missing column in test set with default value equal to 0
    for c in missing_cols:
        df_test_cat[c] = 0

    #Ensure the order of column in the test set is in the same order than in train set
    #Note this also removes categories in test set that aren't present in training set
    df_test_cat = df_test_cat[textcolumns]


    #concatinate the sparse set with the rest of our training data
    mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)
    

    del mdf_test[tempcolumn]
    
#     #delete support NArw2 column
#     columnNAr2 = column + '_NAr2'
#     if columnNAr2 in list(mdf_test):
#       del mdf_test[columnNAr2]
    
    #change data types to 8-bit (1 byte) integers for memory savings
    for textcolumn in textcolumns:
      
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

    
    return mdf_test
  
  def postprocess_pwor_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #we'll use an initial plug value of 0
    '''
            
    normkey = column + '_pwor'
    
    #normkey = columnkey
    
    meanlog = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['meanlog']
    maxlog = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxlog']
    
    pworcolumn = column + '_pwor'
    
    #store original column for later reversion
    mdf_test[pworcolumn] = mdf_test[column].copy()
    
    #convert all values to either numeric or NaN
    mdf_test[pworcolumn] = pd.to_numeric(mdf_test[pworcolumn], errors='coerce')
    
    #convert all values <= 0 to Nan
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] <= 0, np.nan, mdf_test[pworcolumn].values)
    
    #log transform column
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] != np.nan, np.floor(np.log10(mdf_test[pworcolumn])), mdf_test[pworcolumn].values)

    #replace missing data with 0
    mdf_test[pworcolumn] = mdf_test[pworcolumn].fillna(0)

    
    return mdf_test
  
  def postprocess_pwr2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in one hot encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #columns are consistent with those from the processed train set
    
    #postiive values encoded under column 'column' + '_10^#' where # is power of 10
    #negative values encoded under column 'column' + '_-10^#' where # is power of 10
    
    #infill for non numeric or 0, infill has no activation
    
    #if all values are infill no columns returned
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = False
    for power in range(-20, 20):
      power = str(power)
      if (column + '_10^' + power) in postprocess_dict['column_dict']:
        if (column + '_10^' + power) in postprocess_dict['column_dict'][(column + '_10^' + power)]['normalization_dict']:
            normkey = (column + '_10^' + power)
    if normkey is False:
      for power in range(-20, 20):
        power = str(power)
        if (column + '_-10^' + power) in postprocess_dict['column_dict']:
          if (column + '_-10^' + power) in postprocess_dict['column_dict'][(column + '_-10^' + power)]['normalization_dict']:
            normkey = (column + '_-10^' + power)
    if normkey is not False:

      #normkey = columnkey

      powerlabelsdict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['powerlabelsdict']
      labels_train = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['labels_train']

      textcolumns = postprocess_dict['column_dict'][normkey]['categorylist']

      tempcolumn = column + '_:;:_temp'
      
      #store original column for later reversion
      mdf_test[tempcolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[tempcolumn] = pd.to_numeric(mdf_test[tempcolumn], errors='coerce')


      #create copy with negative values
      negtempcolumn = column + '_negtemp'
      mdf_test[negtempcolumn] = mdf_test[tempcolumn].copy()

      #convert all values in negtempcolumn >= 0 to Nan
      mdf_test[negtempcolumn] = \
      np.where(mdf_test[negtempcolumn] >= 0, np.nan, mdf_test[negtempcolumn].values)

      #convert all values <= 0 to Nan
      mdf_test[tempcolumn] = \
      np.where(mdf_test[tempcolumn] <= 0, np.nan, mdf_test[tempcolumn].values)

      #log transform column

      #take abs value of negtempcolumn
      mdf_test[negtempcolumn] = mdf_test[negtempcolumn].abs()

      mdf_test[negtempcolumn] = \
      np.where(mdf_test[negtempcolumn] != np.nan, np.floor(np.log10(mdf_test[negtempcolumn])), mdf_test[negtempcolumn].values)


      test_neg_dict = {}
      negunique = mdf_test[negtempcolumn].unique()
      for unique in negunique:
        if unique != unique:
          newunique = np.nan
        else:
          newunique = column + '_-10^' + str(int(unique))
        if newunique in labels_train and newunique == newunique:
          test_neg_dict.update({unique : newunique})
        else:
          test_neg_dict.update({unique : np.nan})

      mdf_test[negtempcolumn] = mdf_test[negtempcolumn].replace(test_neg_dict)

      #now log trasnform positive values in column column 
      mdf_test[tempcolumn] = \
      np.where(mdf_test[tempcolumn] != np.nan, np.floor(np.log10(mdf_test[tempcolumn])), mdf_test[tempcolumn].values)


      test_pos_dict = {}
      posunique = mdf_test[tempcolumn].unique()
      for unique in posunique:
        if unique != unique:
          newunique = np.nan
        else:
          newunique = column + '_10^' + str(int(unique))
        if newunique in labels_train and newunique == newunique:
          test_pos_dict.update({unique : newunique})
        else:
          test_pos_dict.update({unique : np.nan})


      mdf_test[tempcolumn] = mdf_test[tempcolumn].replace(test_pos_dict)    

      #combine the two columns
      mdf_test[tempcolumn] = mdf_test[negtempcolumn].where(mdf_test[negtempcolumn] == mdf_test[negtempcolumn], mdf_test[tempcolumn])

      #apply onehotencoding
      df_test_cat = pd.get_dummies(mdf_test[tempcolumn])

      #Get missing columns in test set that are present in training set
      missing_cols = set( textcolumns ) - set( df_test_cat.columns )

      #Add a missing column in test set with default value equal to 0
      for c in missing_cols:
          df_test_cat[c] = 0

      #Ensure the order of column in the test set is in the same order than in train set
      #Note this also removes categories in test set that aren't present in training set
      df_test_cat = df_test_cat[textcolumns]


      #concatinate the sparse set with the rest of our training data
      mdf_test = pd.concat([df_test_cat, mdf_test], axis=1)

      #replace original column
      del mdf_test[negtempcolumn]

      del mdf_test[tempcolumn]


      #change data types to 8-bit (1 byte) integers for memory savings
      for textcolumn in textcolumns:

        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

    
    return mdf_test
  
  def postprocess_por2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating bins coresponding to powers
    #of ten in ordinal encoded columns
    
    #pwrs will be intended for a raw set that is not yet normalized
    
    #infill has 0, other designations are based on the data
    
    #negative values allows, comparable to pwr2
    '''
    
    #get normkey
    normkey = column + '_por2'
    
    #retrieve stuff from normalization dictionary
    train_replace_dict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['train_replace_dict']
    
    pworcolumn = column + '_por2'

    #store original column for later reversion
    mdf_test[pworcolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[pworcolumn] = pd.to_numeric(mdf_test[pworcolumn], errors='coerce')
    
    
    #copy set for negative values
    negtempcolumn = column + '_negtempcolumn'
    
    mdf_test[negtempcolumn] = mdf_test[pworcolumn].copy()
    
    #convert all values >= 0 to Nan
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] >= 0, np.nan, mdf_test[negtempcolumn].values)
    
    #take abs value of negtempcolumn
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].abs()
    
    
    #convert all values <= 0 in column to Nan
    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] <= 0, np.nan, mdf_test[pworcolumn].values)


    mdf_test[pworcolumn] = \
    np.where(mdf_test[pworcolumn] != np.nan, np.floor(np.log10(mdf_test[pworcolumn])), mdf_test[pworcolumn].values)
    
    #do same for negtempcolumn
    mdf_test[negtempcolumn] = \
    np.where(mdf_test[negtempcolumn] != np.nan, np.floor(np.log10(mdf_test[negtempcolumn])), mdf_test[negtempcolumn].values)


    newunique_list = list(train_replace_dict)
      
    test_neg_dict = {}
    negunique = mdf_test[negtempcolumn].unique()
    for unique in negunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_-10^' + str(int(unique))
      if newunique in newunique_list and newunique == newunique:
        test_neg_dict.update({unique : newunique})
      else:
        test_neg_dict.update({unique : np.nan})
        
    mdf_test[negtempcolumn] = mdf_test[negtempcolumn].replace(test_neg_dict)
    
    #now do same for column
 
    test_pos_dict = {}
    posunique = mdf_test[pworcolumn].unique()
    for unique in posunique:
      if unique != unique:
        newunique = np.nan
      else:
        newunique = column + '_10^' + str(int(unique))
      if newunique in newunique_list and newunique == newunique:
        test_pos_dict.update({unique : newunique})
      else:
        test_pos_dict.update({unique : np.nan})
    
    
    mdf_test[pworcolumn] = mdf_test[pworcolumn].replace(test_pos_dict)
    
    
    #combine the two columns
    mdf_test[pworcolumn] = mdf_test[negtempcolumn].where(mdf_test[negtempcolumn] == mdf_test[negtempcolumn], mdf_test[pworcolumn])
    
    test_unique = mdf_test[pworcolumn].unique()
  
    #Get missing entries in test set that are present in training set
    missing_cols = set( list(newunique_list) ) - set( list(test_unique) )
    
    extra_cols = set( list(test_unique) ) - set( list(newunique_list) )

      
    test_replace_dict = {}
    for testunique in test_unique:
      if testunique in newunique_list:
        test_replace_dict.update({testunique : train_replace_dict[testunique]})
      else:
        test_replace_dict.update({testunique : 0})
    
#     pworcolumn = column + '_por2'
#     mdf_test[pworcolumn] = mdf_test[column].copy()
    
    
    mdf_test[pworcolumn] = mdf_test[pworcolumn].replace(test_replace_dict)

    
    #replace original column from training data
    del mdf_test[negtempcolumn]    
        
    
    return mdf_test

  def postprocess_sqrt_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
        
    '''
    #function to apply square root transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #applies a square root transform
    #replaces zeros, negative, and missing or improperly formatted data with post-log mean as default infill
    #returns same dataframes with new column of name column + '_log0'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_sqrt'
    
    meansqrt = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['meansqrt']

    #copy original column for implementation
    mdf_test[column + '_sqrt'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_sqrt'] = pd.to_numeric(mdf_test[column + '_sqrt'], errors='coerce')
    
    #replace all non-positive with nan for the log operation
    mdf_test.loc[mdf_test[column + '_sqrt'] < 0, (column + '_sqrt')] = np.nan
    
    #log transform column
    #note that this replaces negative values with nan which we will infill with meanlog
    mdf_test[column + '_sqrt'] = np.sqrt(mdf_test[column + '_sqrt'])
    

    #get mean of training data
    meansqrt = meansqrt  

    #replace missing data with training set mean
    mdf_test[column + '_sqrt'] = mdf_test[column + '_sqrt'].fillna(meansqrt)

#     #replace missing data with 0
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].fillna(0)

#     #change data type for memory savings
#     mdf_test[column + '_log0'] = mdf_test[column + '_log0'].astype(np.float32)

    return mdf_test

  def postprocess_addd_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_addd_class(.)
    #function to apply addition transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'add' for amount of addition, otherwise defaults to adding 1
    #applies an addition transform
    #replaces non-numeric entries with set mean after addition
    #returns same dataframes with new column of name column + '_addd'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_addd'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    add = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['add']

    #copy original column for implementation
    mdf_test[column + '_addd'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_addd'] = pd.to_numeric(mdf_test[column + '_addd'], errors='coerce')
    
    #lperform addition
    mdf_test[column + '_addd'] = mdf_test[column + '_addd'] + add
    

    #replace missing data with training set mean
    mdf_test[column + '_addd'] = mdf_test[column + '_addd'].fillna(mean)


    return mdf_test
  
  
  def postprocess_sbtr_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_sbtr_class(.)
    #function to apply subtraction transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'subtract' for amount of subtraction, otherwise defaults to subtracting 1
    #applies a subtraction transform
    #replaces non-numeric entries with set mean after subtraction
    #returns same dataframes with new column of name column + '_sbtr'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_sbtr'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    subtract = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['subtract']

    #copy original column for implementation
    mdf_test[column + '_sbtr'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_sbtr'] = pd.to_numeric(mdf_test[column + '_sbtr'], errors='coerce')
    
    #lperform subtraction
    mdf_test[column + '_sbtr'] = mdf_test[column + '_sbtr'] - subtract
    

    #replace missing data with training set mean
    mdf_test[column + '_sbtr'] = mdf_test[column + '_sbtr'].fillna(mean)


    return mdf_test
  
  
  def postprocess_mltp_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_mltp_class(.)
    #function to apply multiplication transform
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') and parent category (category)
    #accepts parameter 'multiply' for amount of addition, otherwise defaults to multiplying 2
    #applies an multiplication transform
    #replaces non-numeric entries with set mean after addition
    #returns same dataframes with new column of name column + '_mltp'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_mltp'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    multiply = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiply']

    #copy original column for implementation
    mdf_test[column + '_mltp'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_mltp'] = pd.to_numeric(mdf_test[column + '_mltp'], errors='coerce')
    
    #lperform addition
    mdf_test[column + '_mltp'] = mdf_test[column + '_mltp'] * multiply
    

    #replace missing data with training set mean
    mdf_test[column + '_mltp'] = mdf_test[column + '_mltp'].fillna(mean)


    return mdf_test
  
  
  def postprocess_divd_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_divd_class(.)
    #function to apply division transform
    #accepts parameter 'divide' for amount of division, otherwise defaults to dividing by 2
    #applies an division transform
    #replaces non-numeric entries with set mean after division
    #returns same dataframes with new column of name column + '_divd'
    '''
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_divd'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    divide = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['divide']

    #copy original column for implementation
    mdf_test[column + '_divd'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_divd'] = pd.to_numeric(mdf_test[column + '_divd'], errors='coerce')
    
    #lperform addition
    mdf_test[column + '_divd'] = mdf_test[column + '_divd'] / divide
    

    #replace missing data with training set mean
    mdf_test[column + '_divd'] = mdf_test[column + '_divd'].fillna(mean)


    return mdf_test
  
  
  def postprocess_rais_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_rais_class(.)
    #function to apply raise to a power transform
    #accepts parameter 'raiser' for amount of power, otherwise defaults to square (raise by 2)
    #applies an raise transform
    #replaces non-numeric entries with set mean after raise
    #returns same dataframes with new column of name column + '_rais'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_rais'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    
    raiser = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['raiser']

    #copy original column for implementation
    mdf_test[column + '_rais'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_rais'] = pd.to_numeric(mdf_test[column + '_rais'], errors='coerce')
    
    #lperform addition
    mdf_test[column + '_rais'] = mdf_test[column + '_rais'] ** raiser
    

    #replace missing data with training set mean
    mdf_test[column + '_rais'] = mdf_test[column + '_rais'].fillna(mean)


    return mdf_test
  
  
  def postprocess_absl_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #postprocess_absl_class(.)
    #function to apply absolute transform
    #does not accept paraemters
    #applies an absolute transform
    #replaces non-numeric entries with set mean after transform
    #returns same dataframes with new column of name column + '_absl'
    '''
    
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_absl'
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']

    #copy original column for implementation
    mdf_test[column + '_absl'] = mdf_test[column].copy()


    #convert all values to either numeric or NaN
    mdf_test[column + '_absl'] = pd.to_numeric(mdf_test[column + '_absl'], errors='coerce')
    
    #lperform addition
    mdf_test[column + '_absl'] = mdf_test[column + '_absl'].abs()
    

    #replace missing data with training set mean
    mdf_test[column + '_absl'] = mdf_test[column + '_absl'].fillna(mean)


    return mdf_test
  
  def postprocess_bins_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #note that bins is intended for raw data that has not yet been nromalized
    #bint is intended for values that have already recieved z-score normalization
    
    
    #process_numerical_class(mdf_train, mdf_test, column)
    #function to normalize data to mean of 0 and standard deviation of 1 \
    #z score normalization) and also create set of onehot encoded bins based \
    #on standaqrds deviation increments from training distribution \
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') 
    #replaces missing or improperly formatted data with mean of remaining values
    #replaces original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bins_s<-2'
    #normkey = columnkey
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    std = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsstd']

    binscolumn = column + '_bins'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #subtract mean from column for test
    mdf_test[binscolumn] = mdf_test[binscolumn] - mean

    #divide column values by std for both training and test data
    mdf_test[binscolumn] = mdf_test[binscolumn] / std


    #create bins based on standard deviation increments
#     binscolumn = column + '_bins'
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = ['s<-2','s-21','s-10','s+01','s+12','s>+2'], precision=4)



    textcolumns = \
    [binscolumn + '_s<-2', binscolumn + '_s-21', binscolumn + '_s-10', \
     binscolumn + '_s+01', binscolumn + '_s+12', binscolumn + '_s>+2']


#     #process bins as a categorical set
#     mdf_test = \
#     self.postprocess_text_class(mdf_test, binscolumn, textcolumns)

    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    temppostprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns, \
                                                       'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, temppostprocess_dict, tempkey)
    
    
    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
    
    #delete the support column
    del mdf_test[binscolumn]


    #create list of columns
    #nmbrcolumns = [column + '_nmbr', column + '_NArw'] + textcolumns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []
    
    return mdf_test
  
  
  def postprocess_bint_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #note that bins is intended for raw data that has not yet been nromalized
    #bint is intended for values that have already recieved z-score normalization
    
    
    #process_numerical_class(mdf_train, mdf_test, column)
    #function to normalize data to mean of 0 and standard deviation of 1 \
    #z score normalization) and also create set of onehot encoded bins based \
    #on standaqrds deviation increments from training distribution \
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') 
    #replaces missing or improperly formatted data with mean of remaining values
    #replaces original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column + '_bint_t<-2'
    #normkey = columnkey
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bintmean']
    std = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bintstd']

    binscolumn = column + '_bint'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

#     #subtract mean from column for test
#     mdf_test[column] = mdf_test[column] - mean

#     #divide column values by std for both training and test data
#     mdf_test[column] = mdf_test[column] / std


    #create bins based on standard deviation increments
#     binscolumn = column + '_bint'
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = ['t<-2','t-21','t-10','t+01','t+12','t>+2'], precision=4)



    textcolumns = \
    [binscolumn + '_t<-2', binscolumn + '_t-21', binscolumn + '_t-10', \
     binscolumn + '_t+01', binscolumn + '_t+12', binscolumn + '_t>+2']

    
#     #process bins as a categorical set
#     mdf_test = \
#     self.postprocess_text_class(mdf_test, binscolumn, textcolumns)

    #we're going to use the postprocess_text_class function here since it 
    #allows us to force the columns even if no values present in the set
    #however to do so we're going to have to construct a fake postprocess_dict
    
    #a future extension should probnably build this capacity into a new distinct function
    
    #here are some data structures for reference to create the below
#     def postprocess_text_class(self, mdf_test, column, postprocess_dict, columnkey):
#     textcolumns = postprocess_dict['column_dict'][columnkey]['columnslist']
    
    tempkey = 'tempkey'
    temppostprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns, \
                                                        'categorylist' : textcolumns}}}
    
    #process bins as a categorical set
    mdf_test = \
    self.postprocess_textsupport_class(mdf_test, binscolumn, temppostprocess_dict, tempkey)
    

    #change data type for memory savings
    for textcolumn in textcolumns:
      mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)
    
    #delete the support column
    del mdf_test[binscolumn]
    

    #create list of columns
    #nmbrcolumns = [column + '_nmbr', column + '_NArw'] + textcolumns
    nmbrcolumns = textcolumns



    #nmbrnormalization_dict = {'mean' : mean, 'std' : std}

    #store some values in the nmbr_dict{} for use later in ML infill methods
    column_dict_list = []
    
    return mdf_test
  
  def postprocess_bsor_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #note that bins is intended for raw data that has not yet been nromalized
    #bint is intended for values that have already recieved z-score normalization
    
    
    #process_numerical_class(mdf_train, mdf_test, column)
    #function to normalize data to mean of 0 and standard deviation of 1 \
    #z score normalization) and also create set of onehot encoded bins based \
    #on standaqrds deviation increments from training distribution \
    #takes as arguement pandas dataframe of training and test data (mdf_train), (mdf_test)\
    #and the name of the column string ('column') 
    #replaces missing or improperly formatted data with mean of remaining values
    #replaces original specified column in dataframe
    #returns transformed dataframe
    #expect this approach works better when the numerical distribution is thin tailed
    #if only have training but not test data handy, use same training data for both dataframe inputs
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bsor'
    #normkey = columnkey
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    std = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsstd']
    ordinal_dict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    binscolumn = column + '_bsor'

    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    #subtract mean from column for test
    mdf_test[binscolumn] = mdf_test[binscolumn] - mean

    #divide column values by std for both training and test data
    mdf_test[binscolumn] = mdf_test[binscolumn] / std


    #create bins based on standard deviation increments
#     binscolumn = column + '_bsor'
    mdf_test[binscolumn] = \
    pd.cut( mdf_test[binscolumn], bins = [-float('inf'), -2, -1, 0, 1, 2, float('inf')],  \
           labels = [0,1,2,3,4,5], precision=4)


    
    return mdf_test
  
  
  def postprocess_bnwd_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bn_width_bnwd' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bn_width_bnwd'] == bn_width:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwd']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bnwd'
      
      #store original column for later reversion
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

      #replace missing data with training set mean
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)))

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

      #delete the support column
      del mdf_test[binscolumn]

    
    return mdf_test
  
  
  def postprocess_bnwK_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bn_width_bnwK' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bn_width_bnwK'] == bn_width:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwK']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bnwK'
      
      #store original column for later reversion
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

      #replace missing data with training set mean
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)))

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

      #delete the support column
      del mdf_test[binscolumn]

    
    return mdf_test
  
  
  
  def postprocess_bnwM_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000000
    #and returning in one-hot encoded set
    
    #deletes columns without activations
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'width' in params:
        
      bn_width = params['width']
    
    else:
      
      bn_width = 1000000
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bn_width_bnwM' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bn_width_bnwM'] == bn_width:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwM']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bnwM'
      
      #store original column for later reversion
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

      #replace missing data with training set mean
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)))

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

      #delete the support column
      del mdf_test[binscolumn]

    
    return mdf_test
  
  
  
  def postprocess_bnwo_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bnwo'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width']
      

    binscolumn = column + '_bnwo'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)



    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    
    return mdf_test
  
  
  
  def postprocess_bnKo_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bnKo'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width']
      

    binscolumn = column + '_bnKo'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)



    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    
    return mdf_test
  
  
  def postprocess_bnMo_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal width bins coresponding to 
    #parameter 'width' which defaults to 1000000
    #and returning in ordinal encoded set
    
    #segments without activations are included
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bnMo'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_width = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width']
      

    binscolumn = column + '_bnMo'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)



    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(bn_count)))
    
    #change column dtype
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)



    
    return mdf_test
  
  def postprocess_bnep_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 5
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
      
    else:
      
      bincount = 5
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bincount_bnep' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bincount_bnep'] == bincount:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount_bnep']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bnep'
      
      #copy original column
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#       #replace missing data with training set mean
#       mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      if bn_delta > 0 and bn_min == bn_min:

        mdf_test[binscolumn] = \
        pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
               labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

        tempkey = 'tempkey'
        tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                                 'categorylist' : textcolumns}}}

        mdf_test = \
        self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


        #change data type for memory savings
        for textcolumn in textcolumns:
          mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

        #delete the support column
        del mdf_test[binscolumn]
        
      else:
        
        mdf_test[binscolumn] = 0

    
    return mdf_test
  
  def postprocess_bne7_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 7
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 7
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bincount_bne7' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bincount_bne7'] == bincount:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount_bne7']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bne7'
      
      #copy original column
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#       #replace missing data with training set mean
#       mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      if bn_delta > 0 and bn_min == bn_min:

        mdf_test[binscolumn] = \
        pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
               labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

        tempkey = 'tempkey'
        tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                                 'categorylist' : textcolumns}}}

        mdf_test = \
        self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


        #change data type for memory savings
        for textcolumn in textcolumns:
          mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

        #delete the support column
        del mdf_test[binscolumn]
        
      else:
        
        mdf_test[binscolumn] = 0

    
    return mdf_test
  
  
  def postprocess_bne9_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 9
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bincount_bne9' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bincount_bne9'] == bincount:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount_bne9']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bne9'
      
      #copy original column
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#       #replace missing data with training set mean
#       mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      if bn_delta > 0 and bn_min == bn_min:

        mdf_test[binscolumn] = \
        pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
               labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

        tempkey = 'tempkey'
        tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                                 'categorylist' : textcolumns}}}

        mdf_test = \
        self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


        #change data type for memory savings
        for textcolumn in textcolumns:
          mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

        #delete the support column
        del mdf_test[binscolumn]
        
      else:
        
        mdf_test[binscolumn] = 0

    
    return mdf_test
  
  def postprocess_bneo_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 5
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bneo'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount']
      

    binscolumn = column + '_bneo'
    
    #copy original column
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#     #replace missing data with training set mean
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)

    if bn_delta > 0 and bn_min == bn_min:

      #create bins based on prepared increments
  #     binscolumn = column + '_bnwo'
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')


      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)


      #change column dtype
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_test[binscolumn] = 0

    
    return mdf_test
  
  def postprocess_bn7o_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 7
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bn7o'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount']
      

    binscolumn = column + '_bn7o'
    
    #copy original column
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#     #replace missing data with training set mean
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    if bn_delta > 0 and bn_min == bn_min:

      #create bins based on prepared increments
  #     binscolumn = column + '_bnwo'
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')


      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)


      #change column dtype
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_test[binscolumn] = 0

    
    return mdf_test
  
  
  
  def postprocess_bn9o_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in ordinal encoded set
    
    #default infill is adjacent cell infill
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bn9o'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
    bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount']
      

    binscolumn = column + '_bn9o'
    
    #copy original column
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#     #replace missing data with training set mean
#     mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)


    if bn_delta > 0 and bn_min == bn_min:

      #create bins based on prepared increments
  #     binscolumn = column + '_bnwo'
      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(bn_count)), duplicates='drop')


      #apply ffill to replace NArows with value from adjacent cell in pre4ceding row
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='ffill')

      #we'll follow with a bfill just in case first row had a nan
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(method='bfill')

      #and if the entire set was nan we'll infill with a 0 plug
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)


      #change column dtype
      mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)
      
    else:
      
      mdf_test[binscolumn] = 0

    
    return mdf_test
  
  
  def postprocess_tlbn_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating equal population bins coresponding to 
    #parameter 'bincount' which defaults to 9
    #and returning in one-hot encoded set
    
    #default infill is to have no activations in a row
    
    #can be applied top either a raw set not yet normalized or after normalization
    #such as after z-score normalization)
    '''
    
    if 'bincount' in params:
        
      bincount = params['bincount']
    
    else:
      
      bincount = 9
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'bincount_tlbn' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['bincount_tlbn'] == bincount:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      bn_min = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
      bn_max = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_max']
      bn_delta = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
      bn_count = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bincount = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bincount_tlbn']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_tlbn'
      
      #copy original column
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

#       #replace missing data with training set mean
#       mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      if bn_delta > 0 and bn_min == bn_min:

        mdf_test[binscolumn] = \
        pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
               labels = bins_id, precision=len(str(bn_count)), duplicates='drop')

        tempkey = 'tempkey'
        tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                                 'categorylist' : textcolumns}}}

        mdf_test = \
        self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)
                
        #initialize binscolumn once more
        mdf_test[binscolumn] = mdf_test[column].copy()        
        mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')
        
        
        if len(textcolumns) > 1:

          #for i in range(bincount):
          for i in range(len(textcolumns)):

            tlbn_column = binscolumn + '_' + str(i)

            if i == 0:

              mdf_test[tlbn_column] = \
              np.where(mdf_test[tlbn_column] == 1, \
                      (bins_cuts[i+1] - mdf_test[binscolumn]) / (bins_cuts[i+1] - bn_min), -1)

            elif i == bincount - 1:

              mdf_test[tlbn_column] = \
              np.where(mdf_test[tlbn_column] == 1, \
                      (mdf_test[binscolumn] - bins_cuts[i]) / (bn_max - bins_cuts[i]), -1)

            else:

              mdf_test[tlbn_column] = \
              np.where(mdf_test[tlbn_column] == 1, \
                      (mdf_test[binscolumn] - bins_cuts[i]) / (bins_cuts[i+1] - bins_cuts[i]), -1)


#         #change data type for memory savings
#         for textcolumn in textcolumns:
#           mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

        #delete the support column
        del mdf_test[binscolumn]
        
      else:
        
        mdf_test[binscolumn] = 0

    
    return mdf_test
  
  
  def postprocess_bkt1_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets unconstrained
    #and returning in one-hot encoded set
    
    #removes buckets without activations in train set
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
      
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'buckets_bkt1' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['buckets_bkt1'] == buckets:

            normkey = columnkey
          
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      buckets = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets_bkt1']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bkt1'
      
      #store original column for later reversion
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

      #replace missing data with training set mean
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(len(bins_id))))

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

      #delete the support column
      del mdf_test[binscolumn]

    
    return mdf_test
  
  
  def postprocess_bkt2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets unconstrained
    #and returning in one-hot encoded set
    
    #removes buckets without activations in train set
    '''
    
    if 'buckets' in params:
        
      buckets = params['buckets']
    
    else:
      
      buckets = [0,1,2]
    
    #to retrieve the normalization dictionary we're going to use new method since we don't yet 
    #know what the returned columns titles are yet
    
    normkey = False
    
    if column in postprocess_dict['origcolumn']:
      
      columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
      
    else:
      
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      
      columnkeylist = postprocess_dict['origcolumn'][origcolumn]['columnkeylist']
    
    for columnkey in columnkeylist:
      
      if column == postprocess_dict['column_dict'][columnkey]['inputcolumn']:

        if 'buckets_bkt2' in postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]:

          if postprocess_dict['column_dict'][columnkey]['normalization_dict'][columnkey]['buckets_bkt2'] == buckets:

            normkey = columnkey
        
    if normkey is not False:
      
      mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
      buckets = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets_bkt2']
      bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
      bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
      textcolumns = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
      
      binscolumn = column + '_bkt2'
      
      #store original column for later reversion
      mdf_test[binscolumn] = mdf_test[column].copy()

      #convert all values to either numeric or NaN
      mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

      #replace missing data with training set mean
      mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)
      
      #create bins based on standard deviation increments
#       binscolumn = column + '_bnwd'

      mdf_test[binscolumn] = \
      pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
             labels = bins_id, precision=len(str(len(bins_id))))

      tempkey = 'tempkey'
      tempbins_postprocess_dict = {'column_dict' : {tempkey : {'columnslist' : textcolumns,\
                                                               'categorylist' : textcolumns}}}

      mdf_test = \
      self.postprocess_textsupport_class(mdf_test, binscolumn, tempbins_postprocess_dict, tempkey)


      #change data type for memory savings
      for textcolumn in textcolumns:
        mdf_test[textcolumn] = mdf_test[textcolumn].astype(np.int8)

      #delete the support column
      del mdf_test[binscolumn]

    
    return mdf_test
  
  
  def postprocess_bkt3_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets unconstrained
    #and returning in ordinal encoded set
    
    #segments without activations are included
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bkt3'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    buckets = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    ordl_activations_dict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_activations_dict']
      

    binscolumn = column + '_bkt3'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)



    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
    
    #change column dtype
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)

    
    return mdf_test
  
  
  def postprocess_bkt4_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #processes a numerical set by creating custom bins coresponding to 
    #parameter 'buckets' which defaults to [0,1]
    #first and last buckets bounded
    #and returning in ordinal encoded set
    
    #segments without activations are included
    '''
    
    #retrieve normalization parameters from postprocess_dict
    normkey = column +'_bkt4'
    
    
    mean = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    buckets = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets']
    bins_cuts = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    ordl_activations_dict = postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_activations_dict']
      

    binscolumn = column + '_bkt4'
    
    #store original column for later reversion
    mdf_test[binscolumn] = mdf_test[column].copy()

    #convert all values to either numeric or NaN
    mdf_test[binscolumn] = pd.to_numeric(mdf_test[binscolumn], errors='coerce')
    
    #set all values that fall outside of bounded buckets to nan for replacement with mean
    mdf_test.loc[mdf_test[binscolumn] <= buckets[0], (binscolumn)] = np.nan
    
    mdf_test.loc[mdf_test[binscolumn] > buckets[-1], (binscolumn)] = np.nan

    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(mean)



    #create bins based on standard deviation increments
#     binscolumn = column + '_bnwo'
    mdf_test[binscolumn] = \
    pd.cut(mdf_test[binscolumn], bins = bins_cuts,  \
           labels = bins_id, precision=len(str(len(bins_id))))
  
    #edge case
    #replace missing data with training set mean
    mdf_test[binscolumn] = mdf_test[binscolumn].fillna(0)
    
    #change column dtype
    mdf_test[binscolumn] = mdf_test[binscolumn].astype(int)

    
    return mdf_test
  

  def postprocess_exc2_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #here we'll address any columns that returned a 'excl' category
    #note this is a. singleprocess transform
    #we'll simply maintain the same column but with a suffix to the header
    '''
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_exc2'
    
    fillvalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['fillvalue']
    
    
    exclcolumn = column + '_exc2'
    
    
    mdf_test[exclcolumn] = mdf_test[column].copy()
    
    #del df[column]
    
    mdf_test[exclcolumn] = pd.to_numeric(mdf_test[exclcolumn], errors='coerce')

    
    #fillvalue = mdf_train[exclcolumn].mode()[0]
    
    #replace missing data with fill value
    mdf_test[exclcolumn] = mdf_test[exclcolumn].fillna(fillvalue)
    


    return mdf_test
  
  def postprocess_exc5_class(self, mdf_test, column, postprocess_dict, columnkey, params = {}):
    '''
    #here we'll address any columns that returned a 'excl' category
    #note this is a. singleprocess transform
    #we'll simply maintain the same column but with a suffix to the header
    '''
    
    #retrieve normalizastion parameters from postprocess_dict
    normkey = column + '_exc5'
    
    fillvalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['fillvalue']
    
    
    exclcolumn = column + '_exc5'
    
    
    mdf_test[exclcolumn] = mdf_test[column].copy()
    
    #del df[column]
    
    mdf_test[exclcolumn] = pd.to_numeric(mdf_test[exclcolumn], errors='coerce')
    
    
    #non integers are subject to infill
    mdf_test[exclcolumn] = np.where(mdf_test[exclcolumn] == mdf_test[exclcolumn].round(), mdf_test[exclcolumn], np.nan)
    
    
    #fillvalue = mdf_train[exclcolumn].mode()[0]
    
    #replace missing data with fill value
    mdf_test[exclcolumn] = mdf_test[exclcolumn].fillna(fillvalue)
    


    return mdf_test


  def createpostMLinfillsets(self, df_test, column, testNArows, category, \
                             postprocess_dict, columnslist = [], categorylist = []):
    '''
    #createpostMLinfillsets(df_test, column, testNArows, category, \
    #columnslist = []) function that when fed dataframe of
    #test set, column id, df of True/False corresponding to rows from original \
    #sets with missing values, a string category of 'text', 'date', 'nmbr', or \
    #'bnry', and a list of column id's for the text category if applicable. The \
    #function returns a series of dataframes which can be applied to apply a \
    #machine learning model previously trained on our train set as part of the 
    #original automunge application to predict apppropriate infill values for those\
    #points that had missing values from the original sets, returning the dataframe\
    #df_test_fillfeatures
    '''
    
    MLinfilltype = postprocess_dict['process_dict'][category]['MLinfilltype']

    #if category in ['nmbr', 'nbr2', 'bxcx', 'bnry', 'text', 'bins', 'bint']:
    if MLinfilltype in ['numeric', 'singlct', 'binary', \
                        'multirt', 'multisp', '1010', \
                        'concurrent_act', 'concurrent_nmbr']:

      #if this is a single column set or concurrent_act
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][category]['MLinfilltype'] in ['concurrent_act', 'concurrent_nmbr']:

        #first concatinate the NArows True/False designations to df_train & df_test
  #       df_train = pd.concat([df_train, trainNArows], axis=1)
        df_test = pd.concat([df_test, testNArows], axis=1)

  #       #create copy of df_train to serve as training set for fill
  #       df_train_filltrain = df_train.copy()
  #       #now delete rows coresponding to True
  #       df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns[0]] == False]

  #       #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df
  #       df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)
  #       df_train_filltrain = df_train_filltrain.drop([trainNArows.columns[0]], axis=1)


  #       #create a copy of df_train[column] for fill train labels
  #       df_train_filllabel = pd.DataFrame(df_train[column].copy())
  #       #concatinate with the NArows
  #       df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)
  #       #drop rows corresponding to True
  #       df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns[0]] == False]

  #       #delete the NArows column
  #       df_train_filllabel = df_train_filllabel.drop([trainNArows.columns[0]], axis=1)

  #       #create features df_train for rows needing infill
  #       #create copy of df_train (note it already has NArows included)
  #       df_train_fillfeatures = df_train.copy()
  #       #delete rows coresponding to False
  #       df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns[0]])]
  #       #delete columnslist and column+'_NArows'
  #       df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)
  #       df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns[0]], axis=1)


        #create features df_test for rows needing infill
        #create copy of df_test (note it already has NArows included)
        df_test_fillfeatures = df_test.copy()
        #delete rows coresponding to False
        df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns[0]])]
        #delete column and column+'_NArows'
        df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)
        df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns[0]], axis=1)

        #delete NArows from df_train, df_test
  #       df_train = df_train.drop([trainNArows.columns[0]], axis=1)
        df_test = df_test.drop([testNArows.columns[0]], axis=1)

      #else if categorylist wasn't empty
      else:

        #create a list of columns representing columnslist exlucding elements from
        #categorylist
        noncategorylist = columnslist[:]
        #this removes categorylist elements from noncategorylist
        noncategorylist = list(set(noncategorylist).difference(set(categorylist)))


        #first concatinate the NArows True/False designations to df_train & df_test
  #       df_train = pd.concat([df_train, trainNArows], axis=1)
        df_test = pd.concat([df_test, testNArows], axis=1)

  #       #create copy of df_train to serve as training set for fill
  #       df_train_filltrain = df_train.copy()
  #       #now delete rows coresponding to True
  #       df_train_filltrain = df_train_filltrain[df_train_filltrain[trainNArows.columns[0]] == False]

  #       #now delete columns = columnslist and the NA labels (orig column+'_NArows') from this df
  #       df_train_filltrain = df_train_filltrain.drop(columnslist, axis=1)
  #       df_train_filltrain = df_train_filltrain.drop([trainNArows.columns[0]], axis=1)


  #       #create a copy of df_train[columnslist] for fill train labels
  #       df_train_filllabel = df_train[columnslist].copy()
  #       #concatinate with the NArows
  #       df_train_filllabel = pd.concat([df_train_filllabel, trainNArows], axis=1)
  #       #drop rows corresponding to True
  #       df_train_filllabel = df_train_filllabel[df_train_filllabel[trainNArows.columns[0]] == False]

  #       #now delete columns = noncategorylist from this df
  #       df_train_filltrain = df_train_filltrain.drop(noncategorylist, axis=1)

  #       #delete the NArows column
  #       df_train_filllabel = df_train_filllabel.drop([trainNArows.columns[0]], axis=1)


  #       #create features df_train for rows needing infill
  #       #create copy of df_train (note it already has NArows included)
  #       df_train_fillfeatures = df_train.copy()
  #       #delete rows coresponding to False
  #       df_train_fillfeatures = df_train_fillfeatures[(df_train_fillfeatures[trainNArows.columns[0]])]
  #       #delete columnslist and column+'_NArows'
  #       df_train_fillfeatures = df_train_fillfeatures.drop(columnslist, axis=1)
  #       df_train_fillfeatures = df_train_fillfeatures.drop([trainNArows.columns[0]], axis=1)


        #create features df_test for rows needing infill
        #create copy of df_test (note it already has NArows included)
        df_test_fillfeatures = df_test.copy()
        #delete rows coresponding to False
        df_test_fillfeatures = df_test_fillfeatures[(df_test_fillfeatures[testNArows.columns[0]])]
        #delete column and column+'_NArows'
        df_test_fillfeatures = df_test_fillfeatures.drop(columnslist, axis=1)
        df_test_fillfeatures = df_test_fillfeatures.drop([testNArows.columns[0]], axis=1)
        

        #delete NArows from df_train, df_test
  #       df_train = df_train.drop([trainNArows.columns[0]], axis=1)
        
        df_test = df_test.drop([testNArows.columns[0]], axis=1)
        
        
        

    #if category == 'date':
    #if MLinfilltype in ['exclude']:
    else:

      #create empty sets for now
      #an extension of this method would be to implement a comparable method \
      #for the time category, based on the columns output from the preprocessing
  #     df_train_filltrain = pd.DataFrame({'foo' : []}) 
  #     df_train_filllabel = pd.DataFrame({'foo' : []})
  #     df_train_fillfeatures = pd.DataFrame({'foo' : []})
      df_test_fillfeatures = pd.DataFrame({'foo' : []})
    
    return df_test_fillfeatures


  def predictpostinfill(self, category, model, df_test_fillfeatures, \
                        postprocess_dict, columnslist = []):
    '''
    #predictpostinfill(category, model, df_test_fillfeatures, \
    #columnslist = []), function that takes as input \
    #a category string, a model trained as part of automunge on the coresponding \
    #column from the train set, the output of createpostMLinfillsets(.), a seed \
    #for randomness, and a list of columns \
    #produced by a text class preprocessor when applicable and returns \
    #predicted infills for the test feature sets as df_testinfill based on \
    #derivations using scikit-learn, with the lenth of \
    #infill consistent with the number of True values from NArows
    #a reasonable extension of this funciton would be to allow ML inference with \
    #other ML architectures such a SVM or something SGD based for instance
    '''
    
    MLinfilltype = postprocess_dict['process_dict'][category]['MLinfilltype']
    
    #convert dataframes to numpy arrays
  #   df_train_filltrain = df_train_filltrain.values
  #   df_train_filllabel = df_train_filllabel.values
  #   df_train_fillfeatures = df_train_fillfeatures.values
    df_test_fillfeatures = df_test_fillfeatures.values

    #ony run the following if we have any rows needing infill
  #   if df_train_fillfeatures.shape[0] > 0:
    #since we don't have df_train_fillfeatures to work with we'll look at the 
    #model which will be set to False if there was no infill model trained
    #if model[0] is not False:
    if model is not False:

      #if category in ['nmbr', 'bxcx', 'nbr2']:
      if MLinfilltype in ['numeric', 'concurrent_nmbr']:

  #       #train linear regression model using scikit-learn for numerical prediction
  #       #model = LinearRegression()
  #       #model = PassiveAggressiveRegressor(random_state = randomseed)
  #       #model = Ridge(random_state = randomseed)
  #       #model = RidgeCV()
  #       #note that SVR doesn't have an argument for random_state
  #       model = SVR()
  #       model.fit(df_train_filltrain, df_train_filllabel)    


  #       #predict infill values
  #       df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          df_testinfill = np.array([0])

        #convert infill values to dataframe
  #       df_traininfill = pd.DataFrame(df_traininfill, columns = ['infill'])
        df_testinfill = pd.DataFrame(df_testinfill, columns = ['infill'])

  #       print('category is nmbr, df_traininfill is')
  #       print(df_traininfill)



#       if category == 'bxcx':


#   #       model = SVR()
#   #       model.fit(df_train_filltrain, df_train_filllabel)   

#   #       #predict infill values
#   #       df_traininfill = model.predict(df_train_fillfeatures)



#         #only run following if we have any test rows needing infill
#         if df_test_fillfeatures.shape[0] > 0:
#           df_testinfill = model.predict(df_test_fillfeatures)
#         else:
#           df_testinfill = np.array([0])

#         #convert infill values to dataframe
#   #       df_traininfill = pd.DataFrame(df_traininfill, columns = ['infill'])
#         df_testinfill = pd.DataFrame(df_testinfill, columns = ['infill'])     


      #if category == 'bnry':
      if MLinfilltype in ['singlct', 'binary', 'concurrent_act']:

  #       #train logistic regression model using scikit-learn for binary classifier
  #       #model = LogisticRegression()
  #       #model = LogisticRegression(random_state = randomseed)
  #       #model = SGDClassifier(random_state = randomseed)
  #       model = SVC(random_state = randomseed)

  #       model.fit(df_train_filltrain, df_train_filllabel)

  #       #predict infill values
  #       df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          df_testinfill = np.array([0])

        #convert infill values to dataframe
  #       df_traininfill = pd.DataFrame(df_traininfill, columns = ['infill'])
        df_testinfill = pd.DataFrame(df_testinfill, columns = ['infill'])


      #if category in ['text', 'bins', 'bint']:
      if MLinfilltype in ['multirt', 'multisp']:

  #       #train logistic regression model using scikit-learn for binary classifier
  #       #with multi_class argument activated
  #       #model = LogisticRegression()
  #       #model = SGDClassifier(random_state = randomseed)
  #       model = SVC(random_state = randomseed)

  #       model.fit(df_train_filltrain, df_train_filllabel_argmax)

  #       #predict infill values
  #       df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
        else:
          #this needs to have same number of columns as text category
          df_testinfill = np.zeros(shape=(1,len(columnslist)))
          
        #convert infill values to dataframe
  #       df_traininfill = pd.DataFrame(df_traininfill, columns = columnslist)
        df_testinfill = pd.DataFrame(df_testinfill, columns = columnslist) 
          
      if MLinfilltype in ['1010']:

  #       #train logistic regression model using scikit-learn for binary classifier
  #       #with multi_class argument activated
  #       #model = LogisticRegression()
  #       #model = SGDClassifier(random_state = randomseed)
  #       model = SVC(random_state = randomseed)

  #       model.fit(df_train_filltrain, df_train_filllabel_argmax)

  #       #predict infill values
  #       df_traininfill = model.predict(df_train_fillfeatures)

        #only run following if we have any test rows needing infill
        if df_test_fillfeatures.shape[0] > 0:
          df_testinfill = model.predict(df_test_fillfeatures)
          
          df_testinfill = \
          self.convert_onehot_to_1010(df_testinfill)
          
        else:
          #this needs to have same number of columns as text category
          df_testinfill = np.zeros(shape=(1,len(columnslist)))



        #convert infill values to dataframe
  #       df_traininfill = pd.DataFrame(df_traininfill, columns = columnslist)
        df_testinfill = pd.DataFrame(df_testinfill, columns = columnslist) 


  #       print('category is text, df_traininfill is')
  #       print(df_traininfill)

      #if category == 'date':
      if MLinfilltype in ['exclude', 'boolexclude']:

        #create empty sets for now
        #an extension of this method would be to implement a comparable infill \
        #method for the time category, based on the columns output from the \
        #preprocessing
  #       df_traininfill = pd.DataFrame({'infill' : [0]}) 
        df_testinfill = pd.DataFrame({'infill' : [0]}) 

  #       model = False

  #       print('category is text, df_traininfill is')
  #       print(df_traininfill)




    #else if we didn't have any infill rows let's create some plug values
    else:

      df_testinfill = np.zeros(shape=(1,len(columnslist)))
      df_testinfill = pd.DataFrame(df_testinfill, columns = columnslist) 


  
    return df_testinfill



  def postMLinfillfunction(self, df_test, column, postprocess_dict, \
                            masterNArows_test):

    '''
    #new function ML infill, generalizes the MLinfill application
    #def MLinfill (df_train, df_test, column, postprocess_dict, \
    #masterNArows_train, masterNArows_test, randomseed)
    #function that applies series of functions of createMLinfillsets, 
    #predictinfill, and insertinfill to a categorical encoded set.
    #for the record I'm sure that the conversion of the single column
    #series to a dataframe is counter to the intent of pandas
    #it's probably less memory efficient but it's the current basis of
    #the functions so we're going to maintain that approach for now
    #the revision of these functions to accept pandas series is a
    #possible future extension
    '''
    
    if postprocess_dict['column_dict'][column]['infillcomplete'] is False:

      columnslist = postprocess_dict['column_dict'][column]['columnslist']
      categorylist = postprocess_dict['column_dict'][column]['categorylist']
      origcolumn = postprocess_dict['column_dict'][column]['origcolumn']
      category = postprocess_dict['column_dict'][column]['category']
      model = postprocess_dict['column_dict'][column]['infillmodel']
      
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        #copy the datatype to ensure returned set is consistent
        df_temp_dtype = pd.DataFrame(df_test[column][:0]).copy()

      elif len(categorylist) > 1:
        #copy the datatype to ensure returned set is consistent
        df_temp_dtype = pd.DataFrame(df_test[categorylist][:0]).copy()
      
      #createMLinfillsets
      df_test_fillfeatures = \
      self.createpostMLinfillsets(df_test, column, \
                         pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \
                         category, postprocess_dict, \
                         columnslist = columnslist, \
                         categorylist = categorylist)

      #predict infill values using defined function predictinfill(.)
      df_testinfill = \
      self.predictpostinfill(category, model, df_test_fillfeatures, \
                             postprocess_dict, columnslist = categorylist)

      
      #if model is not False:
      if postprocess_dict['column_dict'][column]['infillmodel'] is not False:

        df_test = self.insertinfill(df_test, column, df_testinfill, category, \
                               pd.DataFrame(masterNArows_test[origcolumn+'_NArows']), \
                               postprocess_dict, columnslist = columnslist, \
                               categorylist = categorylist)

          
      #now change the infillcomplete marker in the text_dict for each \
      #associated text column unless in concurrent_activations MLinfilltype
      if postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        postprocess_dict['column_dict'][column]['infillcomplete'] = True
        
      else:
        for columnname in categorylist:
          postprocess_dict['column_dict'][columnname]['infillcomplete'] = True
        
      #reset data type to ensure returned data is consistent with what was passed
      if len(categorylist) == 1 or \
      postprocess_dict['process_dict'][postprocess_dict['column_dict'][column]['category']]['MLinfilltype'] \
      in ['concurrent_act', 'concurrent_nmbr']:
        df_test[column] = \
        df_test[column].astype({column:df_temp_dtype[column].dtypes})

      elif len(categorylist) > 1:
        for dtype_column in categorylist:
          df_test[dtype_column] = \
          df_test[dtype_column].astype({dtype_column:df_temp_dtype[dtype_column].dtypes})

    return df_test, postprocess_dict



  def postcreatePCAsets(self, df_test, postprocess_dict):
    '''
    Function that takes as input the dataframes df_train and df_test 
    Removes those columns associated with the PCAexcl (which are the original 
    columns passed to automunge which are to be exlcuded from PCA), and returns 
    those sets as PCAset_trian, PCAset_test, and the list of columns extracted as
    PCAexcl_posttransform.
    '''

    PCAexcl = postprocess_dict['PCAexcl']

    #initiate list PCAexcl_postransform
    PCAexcl_posttransform = []

    #derive the excluded columns post-transform using postprocess_dict
    for exclcolumn in PCAexcl:
      
      #if this is one of the original columns (pre-transform)
      if exclcolumn in postprocess_dict['origcolumn']:
      
        #get a column key for this column (used to access stuff in postprofcess_dict)
        exclcolumnkey = postprocess_dict['origcolumn'][exclcolumn]['columnkey']

        #get the columnslist from this columnkey
        exclcolumnslist = postprocess_dict['column_dict'][exclcolumnkey]['columnslist']

        #add these items to PCAexcl_posttransform
        PCAexcl_posttransform.extend(exclcolumnslist)
        
      #if this is a post-transformation column
      elif exclcolumn in postprocess_dict['column_dict']:
        
        #if we hadn't already done another column from the same source
        if exclcolumn not in PCAexcl_posttransform:
          
          #add these items to PCAexcl_posttransform
          PCAexcl_posttransform.extend([exclcolumn])

    #assemble the sets by dropping the columns excluded
    PCAset_test = df_test.drop(PCAexcl_posttransform, axis=1)

    return PCAset_test, PCAexcl_posttransform


  def postPCAfunction(self, PCAset_test, postprocess_dict):
    '''
    Function that takes as input the train and test sets intended for PCA
    dimensionality reduction. Returns a trained PCA model saved in postprocess_dict
    and trasnformed sets.
    '''

    PCAmodel = postprocess_dict['PCAmodel']

    #convert PCAsets to numpy arrays
    PCAset_test = PCAset_test.values

    #apply the transform
    PCAset_test = PCAmodel.transform(PCAset_test)

    #get new number of columns
    newcolumncount = np.size(PCAset_test,1)

    #generate a list of column names for the conversion to pandas
    columnnames = ['PCAcol'+str(y) for y in range(newcolumncount)]

    #convert output to pandas
    PCAset_test = pd.DataFrame(PCAset_test, columns = columnnames)

    return PCAset_test, postprocess_dict



  def postfeatureselect(self, df_test, labelscolumn, testID_column, \
                        postprocess_dict, printstatus):
    '''
    featureselect is a function called within automunge() that applies methods
    to evaluate predictive power of derived features towards a downstream model
    such as to trim the branches of the transform tree.
    
    The function returns a list of column names that "made the cut" so that
    automunge() can then remove extraneous branches.
    '''
    
    #now we'll use automunge() to prepare the subset for feature evaluation
    #note the passed arguments, these are all intentional (no MLinfill applied,
    #primary goal here is to produce a processed dataframe for df_subset
    #with corresponding labels)
    
        
    #printout display progress
    if printstatus is True:
      print("_______________")
      print("Begin Feature Importance evaluation")
      print("")
      
    if labelscolumn is False:
      
      FSmodel = False
      
      #printout display progress
      if printstatus is True:
        print("_______________")
        print("No labels_column passed, Feature Importance halted")
        print("")
        
    elif labelscolumn is not False:
    
      #copy postprocess_dict to customize for feature importance evaluation
      FSpostprocess_dict = deepcopy(postprocess_dict)
      testID_column = testID_column
      labelscolumn = labelscolumn
      pandasoutput = True
      printstatus = printstatus
      TrainLabelFreqLevel = False
      featureeval = False
      FSpostprocess_dict['shuffletrain'] = True
      FSpostprocess_dict['TrainLabelFreqLevel'] = False
      FSpostprocess_dict['MLinfill'] = False
      FSpostprocess_dict['featureselection'] = False
      FSpostprocess_dict['PCAn_components'] = None
      FSpostprocess_dict['Binary'] = False
      FSpostprocess_dict['excl_suffix'] = True
      FSpostprocess_dict['ML_cmnd']['PCA_type'] = 'off'
      FSpostprocess_dict['assigninfill'] = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \
                                             'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}
      randomseed = FSpostprocess_dict['randomseed']
      process_dict = FSpostprocess_dict['process_dict']
      ML_cmnd = FSpostprocess_dict['ML_cmnd']

      FS_LabelSmoothing = False

  #     #FUTURE EXTENSION
  #     FS_LabelSmoothing = FSpostprocess_dict['ML_cmnd']['MLinfill_cmnd']['RandomForestClassifier']['LabelSmoothing']

  #     #but first real quick we'll just deal with PCA default functionality for FS
  #     FSML_cmnd = deepcopy(ML_cmnd)
  #     FSML_cmnd['PCA_type'] = 'off'

      #totalvalidation = valpercent1 + valpercent2

      #if totalvalidation == 0:
      totalvalidation = 0.2

      #prepare sets for FS with postmunge
      am_train, _1, am_labels, labelsencoding_dict, finalcolumns_train = \
      self.postmunge(FSpostprocess_dict, df_test, testID_column = testID_column, \
                     labelscolumn = labelscolumn, pandasoutput = pandasoutput, printstatus = printstatus, \
                     TrainLabelFreqLevel = TrainLabelFreqLevel, featureeval = featureeval, \
                     LabelSmoothing = FS_LabelSmoothing, shuffletrain = True)

      #prepare validaiton sets for FS
      am_train, am_validation1 = \
      self.df_split(am_train, totalvalidation, False, randomseed)

      am_labels, am_validationlabels1 = \
      self.df_split(am_labels, totalvalidation, False, randomseed)


      #this is the returned process_dict
      #(remember "processdict" is what we pass to automunge() call, "process_dict" is what is 
      #assembled inside automunge, there is a difference)
      FSprocess_dict = FSpostprocess_dict['process_dict']

      if am_labels.empty is True:
        FSmodel = False
        
        #printout display progress
        if printstatus is True:
          print("_______________")
          print("No labels returned from postmunge(.), Feature Importance halted")
          print("")
    
      #if am_labels is not an empty set
      if am_labels.empty is False:

        #find origcateogry of am_labels from FSpostprocess_dict
        labelcolumnkey = list(am_labels)[0]
        origcolumn = FSpostprocess_dict['column_dict'][labelcolumnkey]['origcolumn']
        origcategory = FSpostprocess_dict['column_dict'][labelcolumnkey]['origcategory']

        #find labelctgy from process_dict based on this origcategory
        labelctgy = process_dict[origcategory]['labelctgy']

        am_categorylist = []

        for am_label_column in list(am_labels):

          if FSpostprocess_dict['column_dict'][am_label_column]['category'] == labelctgy:

            am_categorylist = FSpostprocess_dict['column_dict'][am_label_column]['categorylist']
            
            #we'll follow convention that if target label category MLinfilltype is concurrent
            #we'll arbitrarily take the first column and use that as target
            if FSpostprocess_dict['process_dict'][labelctgy]['MLinfilltype'] \
            in ['concurrent_act', 'concurrent_nmbr']:
              
              am_categorylist = [am_categorylist[0]]
              
            break

        if len(am_categorylist) == 1:
          am_labels = pd.DataFrame(am_labels[am_categorylist[0]])
          am_validationlabels1 = pd.DataFrame(am_validationlabels1[am_categorylist[0]])

        else:
          am_labels = am_labels[am_categorylist]
          am_validationlabels1 = am_validationlabels1[am_categorylist]

        #if there's a bug occuring after this point it might mean the labelctgy wasn't
        #properly populated in the process_dict for the root category assigned to the labels
        #again the labelctgy entry to process_dict represents for labels returned in 
        #multiple configurations the trasnofrmation category whose returned set will be
        #used to train the feature selection model


        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Training feature importance evaluation model")
          print("")

        #apply function trainFSmodel
        #FSmodel, baseaccuracy = \
        FSmodel = \
        self.trainFSmodel(am_train, am_labels, randomseed, labelsencoding_dict, \
                          FSprocess_dict, FSpostprocess_dict, labelctgy, ML_cmnd)
        
        if FSmodel is False:
          
          #initialize dictionary FScolumn_dict = {}
          FScolumn_dict = {}
          
          FS_origcolumns = list(FSpostprocess_dict['origcolumn'])
          
          #printout display progress
          if printstatus is True:
            print("_______________")
            print("No model returned from training, Feature Importance halted")
            print("")
          
        
        elif FSmodel is not False:


          #update v2.11 baseaccuracy should be based on validation set
          baseaccuracy = self.shuffleaccuracy(am_validation1, am_validationlabels1, \
                                              FSmodel, randomseed, labelsencoding_dict, \
                                              FSprocess_dict, labelctgy, FSpostprocess_dict)

          #get list of columns
          am_train_columns = list(am_train)

          #initialize dictionary FScolumn_dict = {}
          FScolumn_dict = {}
          
          FS_origcolumns = list(FSpostprocess_dict['origcolumn'])

          #assemble FScolumn_dict to support the feature evaluation
          for column in am_train_columns:

            #pull categorylist, category, columnslist
            categorylist = FSpostprocess_dict['column_dict'][column]['categorylist']
            category = FSpostprocess_dict['column_dict'][column]['category']
            columnslist = FSpostprocess_dict['column_dict'][column]['columnslist']
            origcolumn = FSpostprocess_dict['column_dict'][column]['origcolumn']

            #create entry to FScolumn_dict
            FScolumn_dict.update({column : {'categorylist' : categorylist, \
                                            'category' : category, \
                                            'columnslist' : columnslist, \
                                            'origcolumn' : origcolumn, \
                                            'FScomplete' : False, \
                                            'shuffleaccuracy' : None, \
                                            'shuffleaccuracy2' : None, \
                                            'baseaccuracy' : baseaccuracy, \
                                            'metric' : None, \
                                            'metric2' : None}})

          #printout display progress
          if printstatus is True:
            print("_______________")
            print("Evaluating feature importances")
            print("")


          #perform feature evaluation on each column
          for column in am_train_columns:

#             if FSpostprocess_dict['column_dict'][column]['category'] != 'NArw' \
#             and FScolumn_dict[column]['FScomplete'] is False:
            if FScolumn_dict[column]['FScomplete'] is False:

              #categorylist = FScolumn_dict[column]['categorylist']
              #update version 1.80, let's perform FS on columnslist instead of categorylist
              columnslist = FScolumn_dict[column]['columnslist']

              #create set with columns shuffle from columnslist
              #shuffleset = self.createFSsets(am_train, column, categorylist, randomseed)
              #shuffleset = self.createFSsets(am_train, column, columnslist, randomseed)
              shuffleset = self.createFSsets(am_validation1, column, columnslist, randomseed)

              #determine resulting accuracy after shuffle
              columnaccuracy = self.shuffleaccuracy(shuffleset, am_validationlabels1, \
                                                    FSmodel, randomseed, labelsencoding_dict, \
                                                    FSprocess_dict, labelctgy, FSpostprocess_dict)


              #I think this will clear some memory
              del shuffleset

              #category accuracy penalty metric
              metric = baseaccuracy - columnaccuracy
              #metric2 = baseaccuracy - columnaccuracy2



              #save accuracy to FScolumn_dict and set FScomplete to True
              #(for each column in the categorylist)
              #for categorycolumn in FSpostprocess_dict['column_dict'][column]['categorylist']:
              for categorycolumn in FSpostprocess_dict['column_dict'][column]['columnslist']:

                FScolumn_dict[categorycolumn]['FScomplete'] = True
                FScolumn_dict[categorycolumn]['shuffleaccuracy'] = columnaccuracy
                FScolumn_dict[categorycolumn]['metric'] = metric
                #FScolumn_dict[categorycolumn]['shuffleaccuracy2'] = columnaccuracy2
                #FScolumn_dict[categorycolumn]['metric2'] = metric2



            columnslist = FScolumn_dict[column]['columnslist']

            #create second set with all but one columns shuffled from columnslist
            #this will allow us to compare the relative importance between columns
            #derived from the same parent
            #shuffleset2 = self.createFSsets2(am_train, column, columnslist, randomseed)
            shuffleset2 = self.createFSsets2(am_validation1, column, columnslist, randomseed)

            #determine resulting accuracy after shuffle
    #           columnaccuracy2 = self.shuffleaccuracy(shuffleset2, am_labels, FSmodel, \
    #                                                 randomseed, labelsencoding_dict, \
    #                                                 process_dict)
            columnaccuracy2 = self.shuffleaccuracy(shuffleset2, am_validationlabels1, \
                                                  FSmodel, randomseed, labelsencoding_dict, \
                                                  FSprocess_dict, labelctgy, FSpostprocess_dict)

            metric2 = baseaccuracy - columnaccuracy2

            FScolumn_dict[column]['shuffleaccuracy2'] = columnaccuracy2
            FScolumn_dict[column]['metric2'] = metric2




    #     madethecut = self.assemblemadethecut(FScolumn_dict, featurepct, featuremetric, \
    #                                          featuremethod, am_train_columns)


        #if the only column left in madethecut from origin column is a NArw, delete from the set
        #(this is going to lean on the column ID string naming conventions)
        #couldn't get this to work, this functionality a future extension
    #     trimfrommtc = []
    #     for traincolumn in list(df_train):
    #       if (traincolumn + '_') not in [checkmtc[:(len(traincolumn)+1)] for checkmtc in madethecut]:
    #         for mtc in madethecut:
    #           #if mtc originated from traincolumn
    #           if mtc[:(len(traincolumn)+1)] == traincolumn + '_':
    #             #count the number of same instance in madethecut set
    #             madethecut_trim = [mdc_trim[:(len(traincolumn)+1)] for mdc_trim in madethecut]
    #             if madethecut_trim.count(mtc[:(len(traincolumn)+1)]) == 1 \
    #             and mtc[-5:] == '_NArw':
    #               trimfrommtc = trimfrommtc + [mtc]
    #     madethecut = list(set(madethecut).difference(set(trimfrommtc)))


        #apply function madethecut(FScolumn_dict, featurepct)
        #return madethecut
        #where featurepct is the percent of features that we intend to keep
        #(might want to make this a passed argument from automunge)

        #I think this will clear some memory
    #     del am_train, _1, am_labels, am_validation1, _3, \
    #     am_validationlabels1, _5, _6, _7, \
    #     _8, _9, labelsencoding_dict, finalcolumns_train, _10,  \
    #     FSpostprocess_dict

          del am_train, _1, am_labels, labelsencoding_dict, finalcolumns_train, am_validation1, am_validationlabels1

          if printstatus is True:
            print("_______________")
            print("Feature Importance results:")
            print("")

          #to inspect values returned in featureimportance object one could run
          if printstatus is True:
            for keys,values in FScolumn_dict.items():
              print(keys)
              print('metric = ', values['metric'])
              print('metric2 = ', values['metric2'])
              print()
              
    FS_sorted = {'metric_key':{}, 'column_key':{}, 'metric2_key':{}, 'metric2_column_key':{}}
    
    #first we'll handle first metric based on source column
    for FS_origcolumn in FS_origcolumns:
      for key in FScolumn_dict:
        if FScolumn_dict[key]['origcolumn'] == FS_origcolumn:
          if FScolumn_dict[key]['metric'] in FS_sorted['metric_key']:
            if isinstance(FS_sorted['metric_key'][FScolumn_dict[key]['metric']], list):
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']].append(FS_origcolumn)
            else:
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']] = \
              [FS_sorted['metric_key'][FScolumn_dict[key]['metric']]]
              FS_sorted['metric_key'][FScolumn_dict[key]['metric']].append(FS_origcolumn)
          else:
            FS_sorted['metric_key'].update({FScolumn_dict[key]['metric'] : [FS_origcolumn]})
          break

          
    FS_sorted['metric_key'] = dict(sorted(FS_sorted['metric_key'].items(), reverse=True))
    
    for key in FS_sorted['metric_key']:
      for entry in FS_sorted['metric_key'][key]:
        entry_index = FS_sorted['metric_key'][key].index(entry)
        FS_sorted['column_key'].update({FS_sorted['metric_key'][key][entry_index] : key})
      
    
    #now for metric2 based on derived columns relative importance, note sorted in other order
    for FS_origcolumn in FS_origcolumns:
      FS_sorted['metric2_key'].update({FS_origcolumn : {}})
      for key in FScolumn_dict:
        if FScolumn_dict[key]['origcolumn'] == FS_origcolumn:
          if FScolumn_dict[key]['metric2'] in FS_sorted['metric2_key'][FS_origcolumn]:
            if isinstance(FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']], list):
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']].append(key)
            else:
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']] = \
              [FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']]]
              FS_sorted['metric2_key'][FS_origcolumn][FScolumn_dict[key]['metric2']].append(key)
          else:
            FS_sorted['metric2_key'][FS_origcolumn].update({FScolumn_dict[key]['metric2'] : [key]})
    
    for key in FS_sorted['metric2_key']:
      FS_sorted['metric2_key'][key] = dict(sorted(FS_sorted['metric2_key'][key].items(), reverse=False))
    
    for key1 in FS_sorted['metric2_key']:
      FS_sorted['metric2_column_key'].update({key1 : {}})
      for key2 in FS_sorted['metric2_key'][key1]:
        for entry in FS_sorted['metric2_key'][key1][key2]:
          entry_index = FS_sorted['metric2_key'][key1][key2].index(entry)
          FS_sorted['metric2_column_key'][key1].update({FS_sorted['metric2_key'][key1][key2][entry_index] : key2})
        
    
    if printstatus is True:
      print()
      print("______________________")
      print("sorted metric results:")
      print()
      for keys,values in FS_sorted['metric_key'].items():
        for entry in values:
          print(entry)
          print(keys)
          print()
      print("______________________")
      print("sorted metric2 results:")
      print()
      for key in FS_sorted['metric2_key']:
        print("for source column: ", key)
        for keys,values in FS_sorted['metric2_key'][key].items():
          for entry in values:
            print(entry)
            print(keys)
            print()
        print()
              
              
    if FSmodel is False:
      
      FScolumn_dict = {}
    
    #printout display progress
    if printstatus is True:
      
      print("")
      print("_______________")
      print("Feature Importance evaluation complete")
      print("")

    
    
    return FSmodel, FScolumn_dict, FS_sorted
  

  def prepare_driftreport(self, df_test, postprocess_dict, printstatus):
    """
    #driftreport uses the processfamily functions as originally implemented
    #in automunge to recalculate normalization parameters based on the test
    #set passed to postmunge and print a comparison with those original 
    #normalization parameters saved in the postprocess_dict, such as may 
    #prove useful to track drift from original training data.
    #returns a store of the temporary postprocess_dict containing the newly 
    #calculated normalziation parameters and a report of the results
    """
    
    if printstatus is True:
      print("_______________")
      print("Preparing Drift Report:")
      print("")
      
    #initialize empty dictionary to store results
    drift_report = {}
    
    #temporary store for updated normalization parameters
    #we'll copy all the support stuff from original pp_d but delete the 'column_dict'
    #entries for our new derivations below
    drift_ppd = deepcopy(postprocess_dict)
    drift_ppd['column_dict'] = {}
    
    #for each column in df_test
    for drift_column in df_test:
      
      returnedcolumns = postprocess_dict['origcolumn'][drift_column]['columnkeylist']
      returnedcolumns.sort()
      
      if printstatus is True:
        print("______")
        print("Preparing drift report for columns derived from: ", drift_column)
        print("")
        print("original returned columns:")
        print(returnedcolumns)
        print("")
        
      
      if len(returnedcolumns) > 0:

        drift_category = \
        postprocess_dict['column_dict'][postprocess_dict['origcolumn'][drift_column]['columnkey']]['origcategory']
        
      else:
        
        drift_category = 'null'
      
      #update driftreport with this column
      drift_report.update({drift_column : {'origreturnedcolumns_list':returnedcolumns, \
                                           'newreturnedcolumns_list':[], \
                                           'drift_category' : drift_category, \
                                           'orignotinnew' : {}, \
                                           'newnotinorig' : {}, \
                                           'newreturnedcolumn':{}}})
      
      drift_process_dict = \
      postprocess_dict['process_dict']
      
      drift_transform_dict = \
      postprocess_dict['transform_dict']
      
      drift_assign_param = \
      postprocess_dict['assign_param']
      
      #we're only going to copy one source column at a time, as should be 
      #more memory efficient than copying the entire set
      df_test2_temp = pd.DataFrame(df_test[drift_column].copy())
      
      #then a second copy set, here of just a few rows, to follow convention of 
      #automunge processfamily calls
#       df_test3_temp = df_test2_temp[0:10].copy()
      df_test3_temp = df_test2_temp[0:1].copy()
      
      #here's a templist to support the columnkey entry below
      templist1 = list(df_test2_temp)
    
      #now process family
      df_test2_temp, df_test3_temp, drift_ppd = \
      self.processfamily(df_test2_temp, df_test3_temp, drift_column, drift_category, \
                         drift_category, drift_process_dict, drift_transform_dict, \
                         drift_ppd, drift_assign_param)

      #here's a second templist to support the columnkey entry below
      templist2 = list(df_test2_temp)
      
      #ok now we're going to pick one of the new entries of. returned columns to serve 
      #as a "columnkey" for pulling datas from the postprocess_dict 
      #columnkeylist = list(set(templist2) - set(templist1))[0]
      columnkeylist = list(set(templist2) - set(templist1))


      #so last line I believe returns string if only one entry, so let's run a test
      if isinstance(columnkeylist, str):
        columnkey = columnkeylist
      else:
        #if list is empty
        if len(columnkeylist) == 0:
          columnkey = drift_column
        else:
          columnkey = columnkeylist[0]

      
      #if drift_ppd['origcolumn'][drift_column]['columnkey'] not in drift_ppd['column_dict']:
      if len(columnkeylist) == 0:
        
        if printstatus is True:
          print("no new returned columns:")
          print("")
        
        newreturnedcolumns = []
        
      else:
        
#         newreturnedcolumns = \
#         drift_ppd['column_dict'][drift_ppd['origcolumn'][drift_column]['columnkey']]['columnslist']
        newreturnedcolumns = \
        drift_ppd['column_dict'][columnkey]['columnslist']


        newreturnedcolumns.sort()

        if printstatus is True:
          print("new returned columns:")
          print(newreturnedcolumns)
          print("")
          
        #add to driftreport
        drift_report[drift_column]['newreturnedcolumns_list'] = newreturnedcolumns
      
      for origreturnedcolumn in returnedcolumns:
        if origreturnedcolumn not in newreturnedcolumns:
          if printstatus is True:
            print("___")
            print("original derived column not in new returned column: ", origreturnedcolumn)
            print("")
            print("original automunge normalization parameters:")
            print(postprocess_dict['column_dict'][origreturnedcolumn]['normalization_dict'][origreturnedcolumn])
            print("")
          
          drift_report[drift_column]['orignotinnew'].update({origreturnedcolumn:{'orignormparam':\
          postprocess_dict['column_dict'][origreturnedcolumn]['normalization_dict'][origreturnedcolumn]}})
      
      for returnedcolumn in newreturnedcolumns:
        
        drift_report[drift_column]['newreturnedcolumn'].update(\
        {returnedcolumn:{'orignormparam':{}, 'newnormparam':{}}})
        
        
        if printstatus is True:
          print("___")
          print("derived column: ", returnedcolumn)
          print("")
          
        if returnedcolumn in returnedcolumns:
          if printstatus is True:
            print("original automunge normalization parameters:")
            
            print(postprocess_dict['column_dict'][returnedcolumn]['normalization_dict'][returnedcolumn])
            print("")
            
          #add to driftreport
          drift_report[drift_column]['newreturnedcolumn'][returnedcolumn]['orignormparam'] \
          = postprocess_dict['column_dict'][returnedcolumn]['normalization_dict'][returnedcolumn]
          
        else:
          if printstatus is True:
            print("new derived column not in original returned columns: ", returnedcolumn)
            print("")
            
          drift_report[drift_column]['newnotinorig'].update({returnedcolumn:{'newnormparam':\
          drift_ppd['column_dict'][returnedcolumn]['normalization_dict'][returnedcolumn]}})
          
        if printstatus is True:
          print("new postmunge normalization parameters:")
          print(drift_ppd['column_dict'][returnedcolumn]['normalization_dict'][returnedcolumn])
          print("")
          
        #add to driftreport
        drift_report[drift_column]['newreturnedcolumn'][returnedcolumn]['newnormparam'] \
        = drift_ppd['column_dict'][returnedcolumn]['normalization_dict'][returnedcolumn]
      
      #free up some memory
      del df_test2_temp, df_test3_temp, returnedcolumns
      
    if printstatus is True:
      print("")
      print("_______________")
      print("Drift Report Complete")
      print("")
      
    return drift_ppd, drift_report
  

  def postmunge(self, postprocess_dict, df_test, \
                testID_column = False, labelscolumn = False, \
                pandasoutput = False, printstatus = True, \
                TrainLabelFreqLevel = False, featureeval = False, driftreport = False, \
                LabelSmoothing = False, LSfit = False, inversion = False, \
                returnedsets = True, shuffletrain = False):
    """
    #This function documented in READ ME, available online at:
    # https://github.com/Automunge/AutoMunge/blob/master/README.md
    """

      
    indexcolumn = postprocess_dict['indexcolumn']
    testID_column_orig = testID_column

    #quick conversion of any passed column idenitfiers to str
    labelscolumn = self.parameter_str_convert(labelscolumn)
    testID_column = self.parameter_str_convert(testID_column)
    
    #check the range of parameters 
    #(generally speaking other than passed dictionaries, dataframes, or column identifiers)
    pm_miscparameters_results = \
    self.check_pm_miscparameters(pandasoutput, printstatus, TrainLabelFreqLevel, \
                                featureeval, driftreport, LabelSmoothing, LSfit, \
                                returnedsets, shuffletrain, inversion)
    

    #printout display progress
    if printstatus is True:
      print("_______________")
      print("Begin Postmunge processing")
      print("")
    
    
    #feature selection analysis performed here if elected
    if featureeval is True:

      if inversion is not False:
        print("featureselection not available when performing inversion")
        print()
        
        madethecut = []
        FSmodel = False
        FScolumn_dict = {}
        FS_sorted = {}
      
      elif labelscolumn is False:
        print("featureselection not available without labels_column in training set")
        print()
        
        madethecut = []
        FSmodel = False
        FScolumn_dict = {}
        FS_sorted = {}

      else:
        FSmodel, FScolumn_dict, FS_sorted = \
        self.postfeatureselect(df_test, labelscolumn, testID_column, \
                               postprocess_dict, printstatus)

    else:

      madethecut = []
      FSmodel = None
      FScolumn_dict = {}
      FS_sorted = {}

    #initialize postreports_dict
    postreports_dict = {'featureimportance':FScolumn_dict, \
                        'FS_sorted' : FS_sorted, \
                        'finalcolumns_test':[], \
                        'driftreport':{}, \
                        'pm_miscparameters_results':pm_miscparameters_results}


    #functionality to support passed numpy arrays
    #if passed object was a numpy array, convert to pandas dataframe
    checknp = np.array([])
    if isinstance(checknp, type(df_test)):
      df_test = pd.DataFrame(df_test)

    #this converts any numeric columns labels, such as from a passed numpy array, to strings
    testlabels=[]
    for column in list(df_test):
      testlabels.append(str(column))
    df_test.columns = testlabels
    

    #initialize processing dicitonaries

    powertransform = postprocess_dict['powertransform']
    binstransform = postprocess_dict['binstransform']
    NArw_marker = postprocess_dict['NArw_marker']
    floatprecision = postprocess_dict['floatprecision']

#     transform_dict = self.assembletransformdict(binstransform, NArw_marker)

#     if bool(postprocess_dict['transformdict']) is not False:
#       transform_dict.update(postprocess_dict['transformdict'])

#     process_dict = self.assembleprocessdict()

    transform_dict = postprocess_dict['transform_dict']
    process_dict = postprocess_dict['process_dict']

#     assign_param = self.assembleassignparam()

#     if bool(postprocess_dict['assignparam']) is not False:
#       assign_param.update(postprocess_dict['assignparam'])
      
    assign_param = postprocess_dict['assign_param']


    #copy input dataframes to internal state so as not to edit exterior objects
#     if inplace is False:
    df_test = df_test.copy()
#     elif inplace is True:
#       pass


    #____________
    #here is where inversion is performed if selected
    if inversion is not False:
      
      
      if inversion == 'test':
        
        #this is to handle edge case of excl transforms
        #which after processing have their suffix removed from header
        columns_train = postprocess_dict['finalcolumns_train']
        source_columns = postprocess_dict['origtraincolumns']
        
        columns_train = [str(c)+'_excl' if c in source_columns else c for c in columns_train]
        
        #confirm consistency of train an test sets

        #check number of columns is consistent
        if len(columns_train)!= df_test.shape[1]:
          print("error, different number of returned columns in train and test sets")
          return
        
        #check order of column headers are consistent
        columns_test = list(df_test)
        if set(columns_train) == set(columns_test):
          if columns_train != columns_test:
            print("error, different order of column labels in the train and test set")
            return
        #this is for excl edge case again in case we had any updates to finalcolumns_labels above
        elif set(postprocess_dict['finalcolumns_train']) == set(columns_test):
          if postprocess_dict['finalcolumns_train'] != columns_test:
            print("error, different order of column labels in the train and test set")
            return

        #assign labels to column headers if they weren't passed
        if columns_train != columns_test:
          df_test.columns = columns_train
        
        
        if printstatus is True:
          print("Performing inversion recovery of original columns for test set.")
          print()

        df_test, recovered_list, inversion_info_dict = \
        self.df_inversion_meta(df_test, postprocess_dict['origtraincolumns'], postprocess_dict, printstatus)
        
        if printstatus is True:
          print("Inversion succeeded in recovering original form for columns:")
          print(recovered_list)
          print()
        
        if pandasoutput is False:
          
          df_test = df_test.values
          
        return df_test, recovered_list, inversion_info_dict
        
        
      if inversion == 'labels':
        
        #this is to handle edge case of excl transforms
        #which after processing have their suffix removed from header
        finalcolumns_labels = postprocess_dict['finalcolumns_labels']
        source_columns = postprocess_dict['origtraincolumns']
        
        columns_train = [str(c)+'_excl' if c in source_columns else c for c in finalcolumns_labels]
        
        #confirm consistency of label sets

        #check number of columns is consistent
        if len(finalcolumns_labels)!= df_test.shape[1]:
          print("error, different number of returned label columns in train and test sets")
          return
        
        #check order of column headers are consistent
        columns_test = list(df_test)
        if set(finalcolumns_labels) == set(columns_test):
          if finalcolumns_labels != columns_test:
            print("error, different order of column labels in the train and test set")
            return
        #this is for excl edge case again in case we had any updates to finalcolumns_labels above
        elif set(postprocess_dict['finalcolumns_labels']) == set(columns_test):
          if postprocess_dict['finalcolumns_labels'] != columns_test:
            print("error, different order of column labels in the train and test set")
            return
        
        #assign labels to column headers if they weren't passed
        if finalcolumns_labels != columns_test:
          df_test.columns = finalcolumns_labels
          
        if printstatus is True:
          print("Performing inversion recovery of original columns for label set.")
          print()
          
        #first revert any label smoothing to one-hot encoding, LabelSmoothing can be passed as True
        #for basis of LabelSmoothing_train passed to automunge, or float 0-1 matching activation setting 
        #assumes if labels encoded in multiple smoothed configurations they have consistent activations
        df_test = self.meta_LS_invert(LabelSmoothing, df_test, postprocess_dict)
          
        df_test, recovered_list, inversion_info_dict = \
        self.df_inversion_meta(df_test, [postprocess_dict['labels_column']], postprocess_dict, printstatus)
        
        if printstatus is True:
          print("Inversion succeeded in recovering original form for columns:")
          print(recovered_list)
          print()
          
        if pandasoutput is False:
          
          df_test = df_test.values
          
        return df_test, recovered_list, inversion_info_dict
    
    
    #end inversion option sequence
    #____________

    if type(df_test.index) != pd.RangeIndex:
      #if df_train.index.names == [None]:
      if None in df_test.index.names:
        if len(list(df_test.index.names)) == 1 and df_test.index.dtype == int:
          pass
        elif len(list(df_test.index.names)) == 1 and df_test.index.dtype != int:
          print("error, non integer index passed without columns named")
        else:
          print("error, non integer index passed without columns named")
      else:
        if testID_column is False:
          testID_column = []
        elif isinstance(testID_column, str):
          testID_column = [testID_column]
        elif not isinstance(testID_column, list):
          print("error, testID_column allowable values are False, string, or list")
        testID_column = testID_column + list(df_test.index.names)
        df_test = df_test.reset_index(drop=False)


    if labelscolumn is not False:
      labels_column = postprocess_dict['labels_column']
#       if labels_column in list(df_test):
#         df_test = df_test.dropna(subset=[labels_column])

      if labelscolumn is not True:
        if labelscolumn != labels_column:
          print("error, labelscolumn in test set passed to postmunge must have same column")
          print("labeling convention, labels column from automunge was: ", labels_column)


      df_testlabels = pd.DataFrame(df_test[labels_column])
      del df_test[labels_column]
      
      #if we only had one (label) column to begin with we'll create a dummy test set
      if df_test.shape[1] == 0:
#         df_test = df_testlabels[0:10].copy()
        df_test = df_testlabels[0:1].copy()
  
    else:
      df_testlabels = pd.DataFrame()


    #here we derive a range integer index for inclusion in the test ID sets
    tempIDlist = []
    df_test_tempID = pd.DataFrame({indexcolumn:range(0,df_test.shape[0])})
    
    #extract the ID columns from train and test set
    if testID_column is not False:
      
      testIDcolumn = postprocess_dict['trainID_column_orig']
      if testID_column is True:
        testID_column = testIDcolumn
      if testID_column is not True:
        if testID_column != testIDcolumn:
          print("please note the ID column(s) passed to postmunge is different than the ID column(s)")
          print("that was originally passed to automunge. That's ok as long as the test set columns")
          print("remaining are the same, just wanted to give you a heads up in case wasn't intentional.")

      if testID_column is not False:

        df_testID = pd.DataFrame(df_test[testID_column])

        if isinstance(testID_column, str):
          testID_column = [testID_column]
        elif isinstance(testID_column, list):
          testID_column = testID_column
        else:
          print("error, testID_column value must be False, str, or list")
        
        df_test_tempID.index = df_testID.index
        
        df_testID = pd.concat([df_testID, df_test_tempID], axis=1)

        for IDcolumn in testID_column:
          del df_test[IDcolumn]

        #then append the indexcolumn to testID_column list for use in later methods
        testID_column.append(indexcolumn)
        
      else:
        df_testID = df_test_tempID.copy()
        testID_column = [indexcolumn]
      
    else:
      df_test_tempID.index = df_test.index
      df_testID = df_test_tempID.copy()
      testID_column = [indexcolumn]

    del df_test_tempID


    #confirm consistency of train an test sets

    #check number of columns is consistent
    if len(postprocess_dict['origtraincolumns'])!= df_test.shape[1]:
      print("error, different number of original columns in train and test sets")
      return

    #check column headers are consistent (this works independent of order)
    columns_train_set = set(postprocess_dict['origtraincolumns'])
    columns_test_set = set(list(df_test))
    if columns_train_set != columns_test_set:
      print("error, different column labels in the train and test set")
      return

    #check order of column headers are consistent
    columns_train = postprocess_dict['origtraincolumns']
    columns_test = list(df_test)
    if columns_train != columns_test:
      print("error, different order of column labels in the train and test set")
      return

    #Automunge currently is based on convention that all np.inf are treated as np.nan
    #for purposes of infill
    df_test = self.convert_inf_to_nan(df_test)
    df_testlabels = self.convert_inf_to_nan(df_testlabels)

    #here we'll perform drift report if elected
    #if driftreport is True:
    if driftreport in [True, 'report_full']:

      #returns a new partially populated postpr4ocess_dict containing
      #column_dict entries populated with newly calculated normalizaiton parameters
      #for now we'll just print the results in the function, a future expansion may
      #return these to the user somehow, need to put some thought into that
      drift_ppd, drift_report = self.prepare_driftreport(df_test, postprocess_dict, printstatus)

      postreports_dict['driftreport'] = drift_report
      
    if driftreport in ['report_full', 'report_effic']:
      
      postdrift_dict = {}

      if printstatus is True:
        print("_______________")
        print("Preparing Source Column Drift Report:")
        print("")
      
      for column in df_test:

        if column in postprocess_dict['drift_dict']:

          if printstatus is True:
            print("______")
            print("Preparing source column drift report for column: ", column)
            print("")
            print("original drift stats:")
            print(postprocess_dict['drift_dict'][column])
            print("")

          category = postprocess_dict['origcolumn'][column]['category']

          _1, postdrift_dict = \
          self.getNArows(df_test, column, category, postprocess_dict, postdrift_dict, True)

          if printstatus is True:
            print("new drift stats:")
            print(postdrift_dict[column])
            print("")
          
      postreports_dict.update({'sourcecolumn_drift' : {'orig_driftstats' : postprocess_dict['drift_dict'], \
                                                       'new_driftstats' : postdrift_dict}})

      if printstatus is True:
        print("_______________")
        print("Source Column Drift Report Complete")
        print("")
      
      return [], [], [], [], postreports_dict

    #create an empty dataframe to serve as a store for each column's NArows
    #the column id's for this df will follow convention from NArows of 
    #column+'_NArows' for each column in columns_train
    #these are used in the ML infill methods
    #masterNArows_train = pd.DataFrame()
    masterNArows_test = pd.DataFrame()
    
    
    #initialize postdrift_dict
    postdrift_dict = {}


    #For each column, determine appropriate processing function
    #processing function will be based on evaluation of train set
    for column in columns_train:

      columnkey = postprocess_dict['origcolumn'][column]['columnkey']        
      #traincategory = postprocess_dict['column_dict'][columnkey]['origcategory']
      traincategory = postprocess_dict['origcolumn'][column]['category']

      #originally I seperately used evalcategory to check the actual category of
      #the test set, but now that we are allowing assigned categories that could
      #get too complex, this type of functionality could be a future extension
      #for now let's just make explicit assumption that test set has same 
      #properties as train set

      category = traincategory

      #printout display progress
      if printstatus is True:
        print("______")
        print("")
        print("processing column: ", column)
        print("    root category: ", category)
        print("")



      #here we'll delete any columns that returned a 'null' category
      if category == 'null':
        df_test = df_test.drop([column], axis=1)

      #so if we didn't delete the column let's proceed
      else:

        #create NArows (column of True/False where True coresponds to missing data)
        if driftreport in ['efficient', True]:
          testNArows, postdrift_dict = \
          self.getNArows(df_test, column, category, postprocess_dict, postdrift_dict, True)

          if printstatus is True:
            print("original source column drift stats:")
            print(postprocess_dict['drift_dict'][column])
            print("")
            print("new source column drift stats:")
            print(postdrift_dict[column])
            print("")

        else:
          testNArows = self.getNArows(df_test, column, category, postprocess_dict)

        #now append that NArows onto a master NA rows df
        masterNArows_test = pd.concat([masterNArows_test, testNArows], axis=1)


        #process family
        df_test = \
        self.postprocessfamily(df_test, column, category, category, process_dict, \
                              transform_dict, postprocess_dict, columnkey, assign_param)


        #delete columns subject to replacement
        df_test = \
        self.postcircleoflife(df_test, column, category, category, process_dict, \
                              transform_dict, postprocess_dict, columnkey)


#         #now we'll apply the floatprecision transformation
#         columnkeylist = postprocess_dict['origcolumn'][column]['columnkeylist']
#         df_test = self.floatprecision_transform(df_test, columnkeylist, floatprecision)

        #printout display progress
        if printstatus is True:
          print(" returned columns:")
          print(postprocess_dict['origcolumn'][column]['columnkeylist'])
          print("")



    #process labels consistent to the train set if any are included in the postmunge test set

    #first let's get the name of the labels column from postprocess_dict
    labels_column = postprocess_dict['labels_column']

    #ok now let's check if that labels column is present in the test set
    
    if labelscolumn is not False:
      if labelscolumn is not True:
        if labelscolumn != labels_column:
          print("error, labelscolumn in test set passed to postmunge must have same column")
          print("labeling convention, labels column from automunge was: ", labels_column)

      #ok 
      #initialize processing dicitonaries (we'll use same as for train set)
      #a future extension may allow custom address for labels
      labelstransform_dict = transform_dict
      labelsprocess_dict = process_dict


      #ok this replaces some methods from 1.76 and earlier for finding a column key
      #troubleshoot "find labels category"
      columnkey = postprocess_dict['origcolumn'][labels_column]['columnkey']        
      #traincategory = postprocess_dict['column_dict'][columnkey]['origcategory']
      labelscategory = postprocess_dict['origcolumn'][labels_column]['category']

      if printstatus is True:
        #printout display progress
        print("______")
        print("")
        print("processing label column: ", labels_column)
        print("    root label category: ", labelscategory)
        print("")


      #process family
      df_testlabels = \
      self.postprocessfamily(df_testlabels, labels_column, labelscategory, labelscategory, process_dict, \
                             transform_dict, postprocess_dict, columnkey, assign_param)


      #delete columns subject to replacement
      df_testlabels = \
      self.postcircleoflife(df_testlabels, labels_column, labelscategory, labelscategory, process_dict, \
                            transform_dict, postprocess_dict, columnkey)
      
      #now we'll apply the floatprecision transformation
#       columnkeylist = postprocess_dict['origcolumn'][labels_column]['columnkeylist']
#       df_testlabels = self.floatprecision_transform(df_testlabels, columnkeylist, floatprecision)


      #marker for printouts
      pmsmoothing = False
    
      #marker
      match_testLS_to_train = False
      
      #special case for postmunge label smoothing, 
      #if user passes True grab LabelSmoothing_train from postprocess_dict to apply consistently as automunge
      if str(LabelSmoothing) == 'True':
        LabelSmoothing = postprocess_dict['LabelSmoothing_train']
#         LSfit = postprocess_dict['LSfit']
        LSfitparams_dict = postprocess_dict['LSfitparams_dict']
        match_testLS_to_train = True
      
      #apply label smoothing to test set if elected
      if LabelSmoothing > 0.0 and LabelSmoothing < 1.0 and str(LabelSmoothing) != 'False':
        
        pmsmoothing = True
        
        testsmoothing = True
        
        #this will be our marker to indicate if labelsmoothing is already conducted
        categorycomplete_dict = dict(zip(postprocess_dict['origcolumn'][labels_column]['columnkeylist'], [False]*len(postprocess_dict['origcolumn'][labels_column]['columnkeylist'])))

        
        #for column in label columns
        for labelsmoothingcolumn in postprocess_dict['origcolumn'][labels_column]['columnkeylist']:
          
          if categorycomplete_dict[labelsmoothingcolumn] is False:
            
            if match_testLS_to_train is False:
            
              label_categorylist = postprocess_dict['column_dict'][labelsmoothingcolumn]['categorylist']
              label_category = postprocess_dict['column_dict'][labelsmoothingcolumn]['category']

              df_testlabels, categorycomplete_dict, _1 = \
              self.apply_LabelSmoothing(df_testlabels, labelsmoothingcolumn, LabelSmoothing, label_categorylist, label_category, categorycomplete_dict, LSfit, {})
              
              del _1
              
            else:
              
              df_testlabels, categorycomplete_dict = \
              self.postapply_LabelSmoothing(df_testlabels, labelsmoothingcolumn, categorycomplete_dict, LSfitparams_dict)
        

      #printout display progress
      if printstatus is True:
        print(" returned columns:")
        print(postprocess_dict['origcolumn'][labels_column]['columnkeylist'])
        print("")
        
        if pmsmoothing:
          print("Label Smoothing applied to labels")
          print("")


    labelsencoding_dict = postprocess_dict['labelsencoding_dict']



    #now that we've pre-processed all of the columns, let's run through them again\
    #using infill to derive plug values for the previously missing cells
    
    #printout display progress
    if printstatus is True:
      print("______")
      print("")
    
    infillcolumns_list = list(df_test)

    #access infill assignments derived in automunge(.) call
    postprocess_assigninfill_dict = \
    postprocess_dict['postprocess_assigninfill_dict']
    
    df_test = \
    self.apply_pm_infill(df_test, postprocess_assigninfill_dict, \
                        postprocess_dict, printstatus, infillcolumns_list, \
                        masterNArows_test, process_dict)


    #trim branches associated with feature selection
    if postprocess_dict['featureselection'] is True:


      #get list of columns currently included
      currentcolumns = list(df_test)

      #get list of columns to trim
      madethecutset = set(postprocess_dict['madethecut'])
      trimcolumns = [b for b in currentcolumns if b not in madethecutset]

      if len(trimcolumns) > 0:
        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Begin feature importance dimensionality reduction")
          print("")
          print("   method: ", postprocess_dict['featuremethod'])
          if postprocess_dict['featuremethod'] == 'pct':
            print("threshold: ", postprocess_dict['featurepct'])
          if postprocess_dict['featuremethod'] == 'metric':
            print("threshold: ", postprocess_dict['featuremetric'])
          print("")
          print("trimmed columns: ")
          print(trimcolumns)
          print("")
          print("returned columns: ")
          print(postprocess_dict['madethecut'])
          print("")

      #trim columns manually
      for trimmee in trimcolumns:
        del df_test[trimmee]



    #first this check allows for backward compatibility with published demonstrations
    if 'PCAn_components' in postprocess_dict:
      #grab parameters from postprocess_dict
      PCAn_components = postprocess_dict['PCAn_components']
      #prePCAcolumns = postprocess_dict['prePCAcolumns']


      if PCAn_components != None:


        PCAset_test, PCAexcl_posttransform = \
        self.postcreatePCAsets(df_test, postprocess_dict)

        #printout display progress
        if printstatus is True:
          print("_______________")
          print("Applying PCA dimensionality reduction")
          print("")
          if len(postprocess_dict['PCAexcl']) > 0:
            print("columns excluded from PCA: ")
            print(postprocess_dict['PCAexcl'])
            print("")

        PCAset_test, postprocess_dict = \
        self.postPCAfunction(PCAset_test, postprocess_dict)
        
        #we want the indexes to match
#         PCAset_test.set_index(df_test.index)

        #reattach the excluded columns to PCA set
        #df_test = pd.concat([PCAset_test, df_test[PCAexcl_posttransform]], axis=1)
        df_test = pd.concat([PCAset_test.set_index(df_test.index), df_test[PCAexcl_posttransform]], axis=1)

        #printout display progress
        if printstatus is True:
          print("returned PCA columns: ")
          print(list(PCAset_test))
          print("")

    #Binary dimensionality reduction goes here
    #we'll only apply to test data not labels
    #making an executive decvision for now that ordinal encoded columns will be excluded
    if postprocess_dict['Binary'] in [True, 'retain']:
      
      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin Binary dimensionality reduction")
        print("")
        print("Before transform test set column count = ")
        print(df_test.shape[1])
        print("")
      
      Binary_dict = postprocess_dict['Binary_dict']
      Binary = postprocess_dict['Binary']
          
      df_test = self.postBinary_convert(df_test, Binary_dict, Binary)
      
      #printout display progress
      if printstatus is True:
        print("Boolean column count = ")
        print(len(Binary_dict['bool_column_list']))
        print("")
        print("After transform test set column count = ")
        print(df_test.shape[1])
        print("")
    
    

    #here is the process to levelize the frequency of label rows in train data
    #currently only label categories of 'bnry' or 'text' are considered
    #a future extension will include numerical labels by adding supplemental 
    #label columns to designate inclusion in some fractional bucket of the distribution
    #e.g. such as quintiles for instance
    if TrainLabelFreqLevel is True \
    and labelscolumn is not False:

      #printout display progress
      if printstatus is True:
        print("_______________")
        print("Begin label rebalancing")
        print("")
        print("Before rebalancing row count = ")
        print(df_testlabels.shape[0])
        print("")


  #       train_df = pd.DataFrame(np_train, columns = finalcolumns_train)
  #       labels_df = pd.DataFrame(np_labels, columns = finalcolumns_labels)
      if testID_column is not False:
  #         trainID_df = pd.DataFrame(np_trainID, columns = [trainID_column])
        #add trainID set to train set for consistent processing
  #         train_df = pd.concat([train_df, trainID_df], axis=1)                        
        df_test = pd.concat([df_test, df_testID], axis=1)                        


      #apply LabelFrequencyLevelizer defined function
      df_test, df_testlabels = \
      self.LabelFrequencyLevelizer(df_test, df_testlabels, labelsencoding_dict, \
                                   postprocess_dict, process_dict, LabelSmoothing)



      #extract trainID
      if testID_column is not False:

        df_testID = pd.DataFrame(df_test[testID_column])

        if isinstance(testID_column, str):
          tempIDlist = [testID_column]
        elif isinstance(testID_column, list):
          tempIDlist = testID_column
        for IDcolumn in tempIDlist:
          del df_test[IDcolumn]
        #del df_train[trainID_column]

      #printout display progress
      if printstatus is True:
        print("After rebalancing row count = ")
        print(df_testlabels.shape[0])
        print("")


    #if shuffletrain passed to postmunge it takes place here
    #(postmunge does not default to consistent shuffle as train set, relies on parameter)
    if shuffletrain is True:
      #shuffle training set and labels
      df_test = self.df_shuffle(df_test, postprocess_dict['randomseed'])
      df_testlabels = self.df_shuffle(df_testlabels, postprocess_dict['randomseed'])
      

      if testID_column is not False:
        df_testID = self.df_shuffle(df_testID, postprocess_dict['randomseed'])
        
        

    #a special case, those columns that we completely excluded from processing via excl
    #we'll scrub the suffix appender
    #(we won't perform this step to test data if PCA was applied)
    if postprocess_dict['PCA_applied'] is False and postprocess_dict['excl_suffix'] is False:
      df_test.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                         else column for column in df_test.columns]
      
    if labelscolumn is not False and postprocess_dict['excl_suffix'] is False:
      df_testlabels.columns = [column[:-5] if postprocess_dict['column_dict'][column]['category'] == 'excl' \
                               else column for column in df_testlabels.columns]

    #here's a list of final column names saving here since the translation to \
    #numpy arrays scrubs the column names
    finalcolumns_test = list(df_test)

    postreports_dict['finalcolumns_test'] = finalcolumns_test
    
    postreports_dict.update({'sourcecolumn_drift' : {'orig_driftstats' : postprocess_dict['drift_dict'], \
                                                     'new_driftstats' : postdrift_dict}})



    #printout display progress
    if printstatus is True:

      print("Postmunge returned column set: ")
      print(list(df_test))
      print("")

      if labelscolumn is not False:
        print("Postmunge returned label column set: ")
        print(list(df_testlabels))
        print("")

    #now we'll apply the floatprecision transformation
    df_test = self.floatprecision_transform(df_test, finalcolumns_test, floatprecision)
    if labelscolumn is not False:
      finalcolumns_labels = list(df_testlabels)
      df_testlabels = self.floatprecision_transform(df_testlabels, finalcolumns_labels, floatprecision)


    if testID_column is not False:
      #testID = df_testID
      #pass
      if returnedsets in ['test_ID', 'test_ID_labels']:
        df_test = pd.concat([df_test, df_testID], axis=1)
      
    else:
      df_testID = pd.DataFrame()

    if labelscolumn is not False:
      #testlabels = df_testlabels
      #pass
      if returnedsets in ['test_labels', 'test_ID_labels']:
        df_test = pd.concat([df_test, df_testlabels], axis=1)
      
    else:
      df_testlabels = pd.DataFrame()

    #else output numpy arrays
    #else:
    if pandasoutput is False:
      #global processing to test set including conversion to numpy array
      df_test = df_test.values

      if testID_column is not False \
      and returnedsets not in [False, 'test_ID', 'test_labels', 'test_ID_labels']:
        df_testID = df_testID.values
      else:
        df_testID = []


      if labelscolumn is not False \
      and returnedsets not in [False, 'test_ID', 'test_labels', 'test_ID_labels']:
        df_testlabels = df_testlabels.values

        #apply ravel to labels if appropriate - converts from eg [[1,2,3]] to [1,2,3]
        if df_testlabels.ndim == 2 and df_testlabels.shape[1] == 1:
          df_testlabels = np.ravel(df_testlabels)

      else:
        df_testlabels = []


    #printout display progress
    if printstatus is True:

      print("_______________")
      print("Postmunge Complete")
      print("")
    
    if returnedsets is True:
    
      return df_test, df_testID, df_testlabels, labelsencoding_dict, postreports_dict
    
    else:
      
      return df_test
    
  def populate_categorytree(self, postprocess_dict):
    """
    #Populates mirror tree of transformations
    #to facilitate translation between transformation category space 
    #and suffix appender space
    
    #As an example of a populated tree for bxcx root cateogry transform 
    #which returns columns ['column_bxcx_nmbr', 'column_NArw']
    
    categorytree = \
    {'__root__' : \
    ['__root__', '', ['__root__'], {'NArw': ['sup', inputcol, categorylist, {}], \
                                    'bxcx': ['rep', inputcol, categorylist, \
                                            {'nmbr': ['rep', inputcol, categorylist, {}]}]}]}
    
    #here 'sup'/'rep' refers to distinction between supplement and replace primitives
    #and 'root' is for the source column entry
    
    #inputcol and categorylist access from column_dict from key of one of entries in categorylist
    #so we'll search column_dict for key with entries that match category 
    #and inputcolumn that matches categorylist entry of preceding entry
    #(convention is we only have downstream transforms applied to single entry categorylists)
    
    #Once we have this populated, we'll translate it to an inverse
    #with returned columns in the root and root in each final
    #such as to facilitate any translation from returned sets to source sets
    #(such as for converting predictions back to original label formatting)
    
    #I think this will work let's give it a try
    """
    
    #these are the columns passed to automunge(.) including labels
    source_columns = list(postprocess_dict['origcolumn'])
    
    #these are derived columns including replaced columns and labels
    produced_columns = list(postprocess_dict['column_dict'])
    
    #initialize categorytree
    categorytree = {'__root__' : {}}
    
    for origcolumn in source_columns:
      
      categorytree['__root__'].update({origcolumn : ['__root__', '', ['__root__'], {}]})
      
      root_category = postprocess_dict['origcolumn'][origcolumn]['category']
          
      parents     = postprocess_dict['transform_dict'][root_category]['parents']
      siblings    = postprocess_dict['transform_dict'][root_category]['siblings']
      auntsuncles = postprocess_dict['transform_dict'][root_category]['auntsuncles']
      cousins     = postprocess_dict['transform_dict'][root_category]['cousins']
      
      
      categorytree_entry = self.populate_family(postprocess_dict, categorytree['__root__'][origcolumn][3], origcolumn, '__root__', \
                                           parents, siblings, auntsuncles, cousins)
#       categorytree_entry = populate_family(postprocess_dict, categorytree['__root__'][origcolumn][3], origcolumn, '__root__', \
#                                            parents, siblings, auntsuncles, cousins)
      
      categorytree['__root__'][origcolumn][3].update(categorytree_entry)
      
    return categorytree
    
    
  

  def populate_family(self, postprocess_dict, categorytree, inputcolumn, inputcategory, \
                      parents, siblings, auntsuncles, cousins):
    """
    #populates categorytree entries from seeding of a source-column and root category
    
    #we will run in order of
    #parents, auntsuncles, siblings, cousins
    
    #see also notes for populate_categorytree function
    """
    
    for entry in parents:
      
      if entry != None:
      
        categorylist = self.get_categorylist(postprocess_dict, inputcolumn, entry)
#         categorylist = get_categorylist(postprocess_dict, inputcolumn, entry)
        
        if entry not in categorytree:
          categorytree.update({entry : {}})

        #parents have downstream offspring
        children      = postprocess_dict['transform_dict'][entry]['children']
        niecesnephews = postprocess_dict['transform_dict'][entry]['niecesnephews']
        coworkers     = postprocess_dict['transform_dict'][entry]['coworkers']
        friends       = postprocess_dict['transform_dict'][entry]['friends']

        #convention is downstream offspring only allowed on sets returned with single column categorylist
        if len(categorylist) == 1:

          parentcolumn = categorylist[0]
          
          #parents is replace primitive
          categorytree[entry].update({parentcolumn : ['rep', inputcolumn, categorylist, {}]})

          categorytree_entry = self.populate_family(postprocess_dict, categorytree[entry][parentcolumn][3], parentcolumn, entry, \
                                               children, niecesnephews, coworkers, friends)
#           categorytree_entry = populate_family(postprocess_dict, categorytree[entry][parentcolumn][3], parentcolumn, entry, \
#                                                children, niecesnephews, coworkers, friends)
          
          categorytree[entry][parentcolumn][3].update(categorytree_entry)
          
        else:
          
          for category_column in categorylist:
            
            categorytree[entry].update({category_column : ['rep', inputcolumn, categorylist, {}]})
          
        
    for entry in auntsuncles:
      
      if entry != None:
        
        if entry not in categorytree:
          categorytree.update({entry : {}})
      
        categorylist = self.get_categorylist(postprocess_dict, inputcolumn, entry)
#         categorylist = get_categorylist(postprocess_dict, inputcolumn, entry)
        
        for category_column in categorylist:

          #auntsuncles is replace primitive
          categorytree[entry].update({category_column : ['rep', inputcolumn, categorylist, {}]})

          #auntsuncles have no downstream offspring
      
    for entry in siblings:
      
      if entry != None:
      
        categorylist = self.get_categorylist(postprocess_dict, inputcolumn, entry)
#         categorylist = get_categorylist(postprocess_dict, inputcolumn, entry)

        if entry not in categorytree:
          categorytree.update({entry : {}})

        #siblings have downstream offspring
        children      = postprocess_dict['transform_dict'][entry]['children']
        niecesnephews = postprocess_dict['transform_dict'][entry]['niecesnephews']
        coworkers     = postprocess_dict['transform_dict'][entry]['coworkers']
        friends       = postprocess_dict['transform_dict'][entry]['friends']

        #convention is downstream offspring only allowed on sets returned with single column categorylist
        if len(categorylist) == 1:

          parentcolumn = categorylist[0]
          
          #siblings is supplement primitive
          categorytree[entry].update({parentcolumn : ['sup', inputcolumn, categorylist, {}]})


          categorytree_entry = self.populate_family(postprocess_dict, categorytree[entry][parentcolumn][3], parentcolumn, entry, \
                                               children, niecesnephews, coworkers, friends)
#           categorytree_entry = populate_family(postprocess_dict, categorytree[entry][parentcolumn][3], parentcolumn, entry, \
#                                                children, niecesnephews, coworkers, friends)
          
          categorytree[entry][parentcolumn][3].update(categorytree_entry)
          
        else:
          for category_column in categorylist:
            
            categorytree[entry].update({category_column : ['sup', inputcolumn, categorylist, {}]})
                    
        
    for entry in cousins:
      
      if entry != None:
      
        categorylist = self.get_categorylist(postprocess_dict, inputcolumn, entry)
        #categorylist = get_categorylist(postprocess_dict, inputcolumn, entry)

        if entry not in categorytree:
          categorytree.update({entry : {}})

        #cousins have no downstream offspring
        
        for category_column in categorylist:

          #cousins is supplement primitive
          categorytree[entry].update({category_column : ['sup', inputcolumn, categorylist, {}]})

          #auntsuncles have no downstream offspring
      
      
    return categorytree
    
    
    
    
  def get_categorylist(self, postprocess_dict, inputcolumn, category):
    """
    #access a returned categorylist
    #corresponding to the category of transformation applied to an inputcolumn
    #by searching entries to postprocess_dict['column_dict']
    """
    
    categorylist = []
    
    for entry in list(postprocess_dict['column_dict']):
      
      if postprocess_dict['column_dict'][entry]['inputcolumn'] == inputcolumn \
      and postprocess_dict['column_dict'][entry]['category'] == category:
        
        categorylist = postprocess_dict['column_dict'][entry]['categorylist']
          
        break
        
    #this is for edge case when a transform does not return columns
    if categorylist == None:
      categorylist = []
        
    return categorylist
      
  def populate_inverse_categorytree(self, postprocess_dict):
    """
    #So this is similar to the categorytree in that we are mirror the transformations
    #in a populated data structure
    #but in this inverse version the bottom tier are the conclusion branches
    #which progress back to the root
    #note that we'll allow redundant entries in first tiers
    #such as to aggregate each distinct path by common starting point in list
    
    #here we'll want additional data points for:
    #- depth of branch
    #- information retention of transform
    #- availability of inverse transform
    
    #As an example of excerpt from a populated tree for bxcx root category transform 
    #which returns columns ['column_bxcx_nmbr', 'column_NArw']
    
    inverse_categorytree['nmbr'] = \
    {'Age_bxcx_nmbr': ['sup', 'Age_bxcx', ['Age_bxcx_nmbr'], ['Age_bxcx_nmbr', 'Age_NArw'],
                      2, False, False, \
                      {'bxcx': {'Age_bxcx': ['rep', 'Age', ['Age_bxcx'], ['Age_bxcx_nmbr', 'Age_NArw'],
                                             1, False, False, \
                                             {'__root__': {'Age': ['__root__','__root__','__root__',\
                                                                   '__root__', 1 , False , False, \
                                                                    {}]}}]}}]}
    """
    
    #all returned columns including labels
    returned_columns = \
    postprocess_dict['pre_dimred_finalcolumns_train'] + postprocess_dict['finalcolumns_labels']
    
    #these are all derived columns including replaced columns and labels
    produced_columns = list(postprocess_dict['column_dict'])
    
    #these are the columns passed to automunge(.) including labels
    source_columns = list(postprocess_dict['origcolumn'])
    
    
    #initialize inverse_categorytree
    inverse_categorytree = {}
    
    for returned_column in returned_columns:
      
      category     = postprocess_dict['column_dict'][returned_column]['category']
      inputcolumn  = postprocess_dict['column_dict'][returned_column]['inputcolumn']
      origcolumn   = postprocess_dict['column_dict'][returned_column]['origcolumn']
      columnslist  = postprocess_dict['column_dict'][returned_column]['columnslist']
      categorylist = postprocess_dict['column_dict'][returned_column]['categorylist']
      
#       preceding_category = postprocess_dict['column_dict'][inputcolumn]['category']
#       preceding_inputcolumn
      
      if category not in inverse_categorytree:
      
        inverse_categorytree.update(
        {category : {}}
        )
      
      depth = 1
      
      info_retention = False
      if 'info_retention' in postprocess_dict['process_dict'][category]:
        if postprocess_dict['process_dict'][category]['info_retention'] is True:
          info_retention = True
      
      transforms_avail = False
      if 'inverseprocess' in postprocess_dict['process_dict'][category]:
        if callable(postprocess_dict['process_dict'][category]['inverseprocess']):
          transforms_avail = True
      
      for entry in categorylist:
        
        if entry in returned_columns:
          sup_or_rep = 'sup'
        else:
          sup_or_rep = 'rep'

        #note the index number of entries in this list are used to access
        #so any added points here should be tacked on end (after {})
        #and mirrored in other function
        inverse_categorytree[category].update(
        {entry : [sup_or_rep, 
                  inputcolumn, 
                  categorylist, 
                  columnslist, 
                  depth, 
                  info_retention,
                  transforms_avail,
                  {}]}
        )
        
        if inputcolumn not in source_columns:

          inverse_categorytree_entry, depth_, info_retention_, transforms_avail_ = \
          self.populate_inverse_family(
            postprocess_dict, inverse_categorytree[category][entry][7], inputcolumn,
            returned_columns, source_columns
          )
          
          inverse_categorytree[category][entry][4] += depth_
          
          inverse_categorytree[category][entry][5] = \
          inverse_categorytree[category][entry][5] and info_retention_
          
          inverse_categorytree[category][entry][6] = \
          inverse_categorytree[category][entry][6] and transforms_avail_
          
          
          inverse_categorytree[category][entry][7].update(inverse_categorytree_entry)
        
        
        else:
          
          inverse_categorytree[category][entry][7].update(
          {'__root__' : {inputcolumn : ['__root__', 
                                        '__root__', 
                                        '__root__', 
                                        '__root__', 
                                        depth, 
                                        info_retention,
                                        transforms_avail,
                                        {}]}}
          )
          
        
    return inverse_categorytree
    
  
  def populate_inverse_family(self, postprocess_dict, inverse_categorytree, column, \
                              returned_columns, source_columns):
    """
    #populates inverse_categorytree entries from seeding of an inputcolumn
    
    #see also notes for populate_inverse_categorytree function
    """
      
    category     = postprocess_dict['column_dict'][column]['category']
    inputcolumn  = postprocess_dict['column_dict'][column]['inputcolumn']
    origcolumn   = postprocess_dict['column_dict'][column]['origcolumn']
    columnslist  = postprocess_dict['column_dict'][column]['columnslist']
    categorylist = postprocess_dict['column_dict'][column]['categorylist']
    
    
    
    if category not in inverse_categorytree:
    
      inverse_categorytree.update(
      {category : {}}
      )
    
    depth = 1
    
    info_retention = False
    if 'info_retention' in postprocess_dict['process_dict'][category]:
      if postprocess_dict['process_dict'][category]['info_retention'] is True:
        info_retention = True

    transforms_avail = False
    if 'inverseprocess' in postprocess_dict['process_dict'][category]:
      if callable(postprocess_dict['process_dict'][category]['inverseprocess']):
        transforms_avail = True
        
    for entry in categorylist:

      if entry in returned_columns:
        sup_or_rep = 'sup'
      else:
        sup_or_rep = 'rep'
      
      #note the index number of entries in this list are used to access
      #so any added points here should be tacked on end (after {})
      #and mirrored in other function
      inverse_categorytree[category].update(
      {entry : [sup_or_rep, 
                inputcolumn, 
                categorylist, 
                columnslist, 
                depth, 
                info_retention,
                transforms_avail,
                {}]}
      )
      
      
      if inputcolumn not in source_columns:
        
        inverse_categorytree_entry, depth_, info_retention_, transforms_avail_ = \
        self.populate_inverse_family(
          postprocess_dict, inverse_categorytree[category][entry][7], inputcolumn,
          returned_columns, source_columns
        )
  
        inverse_categorytree[category][entry][4] += depth_

        inverse_categorytree[category][entry][5] = \
        inverse_categorytree[category][entry][5] and info_retention_

        inverse_categorytree[category][entry][6] = \
        inverse_categorytree[category][entry][6] and transforms_avail_


        inverse_categorytree[category][entry][7].update(inverse_categorytree_entry)


      else:

        inverse_categorytree[category][entry][7].update(
        {'__root__' : {inputcolumn : ['__root__', 
                                      '__root__', 
                                      '__root__', 
                                      '__root__', 
                                      depth, 
                                      info_retention,
                                      transforms_avail,
                                      {}]}}
        )
        
        
    depth            = inverse_categorytree[category][entry][4]
    info_retention   = inverse_categorytree[category][entry][5]
    transforms_avail = inverse_categorytree[category][entry][6]
    
    return inverse_categorytree, depth, info_retention, transforms_avail
  

  def populate_inputcolumn_dict(self, postprocess_dict):
    """
    #we'll create another structure, this one flatted, similar to origcolumn or column_dict
    #this one as inputcolumn_dict
    
    #this will populate a strucutre with example entry
    
    #inputcolumn_dict = {inputcolumn : {category : {column : column_dict[column]}}}
    
    #where inputcolumn is a column serving as input to a specific generation (set) of trasnformtions
    #such as either entries to parents/siblings/auntsuncles/cousins
    #or for downstream generations entries to children/niecesnephews/coworkers/friends
    """
    
    inputcolumn_dict = {}
    
    for column in postprocess_dict['column_dict']:
      
      inputcolumn = postprocess_dict['column_dict'][column]['inputcolumn']
      
      if inputcolumn not in inputcolumn_dict:
        
        inputcolumn_dict.update({inputcolumn : {}})
        
      category = postprocess_dict['column_dict'][column]['category']
      
      if category not in inputcolumn_dict[inputcolumn]:
        
        inputcolumn_dict[inputcolumn].update({category:{}})
        
      inputcolumn_dict[inputcolumn][category].update({column : postprocess_dict['column_dict'][column]})
      
    return inputcolumn_dict
  
  
  def LS_invert(self, LabelSmoothing, df, categorylist, postprocess_dict):
    """
    #Converts smoothed labels back to one-hot encoding
    #for a particular categorylist
    """
    
    if LabelSmoothing is True:
      
      LabelSmoothing = postprocess_dict['LabelSmoothing_train']
    
    if LabelSmoothing > 0 and LabelSmoothing < 1:
      
      for categorylist_entry in categorylist:
        
        df[categorylist_entry] = np.where(df[categorylist_entry] == LabelSmoothing, 1, 0)
        
        df[categorylist_entry] = df[categorylist_entry].astype(np.int8)
        
    return df
  
  
  def meta_LS_invert(self, LabelSmoothing, df, postprocess_dict):
    """
    #Performs labels smoothing inversion
    #Note that label smoothing may have been applied to multiple distinct sets
    #originating from same source column
    """
    
    if LabelSmoothing is True or (LabelSmoothing > 0 and LabelSmoothing < 1):

      labels_column = postprocess_dict['labels_column']

      labels_root_category = postprocess_dict['origcolumn'][labels_column]['category']

      label_returnedcolumns = postprocess_dict['origcolumn'][labels_column]['columnkeylist']

      inverse_categorytree = postprocess_dict['inverse_categorytree']

      #we'll strike columns from this list if they are in another column's categorylist
      returnedcolumns_copy = label_returnedcolumns.copy()

      for returnedcolumn in label_returnedcolumns:

        #only continue if not previiously stricken from antoher column's cateogrylist
        if returnedcolumn in returnedcolumns_copy:

          returned_categorylist = postprocess_dict['column_dict'][returnedcolumn]['categorylist']

          for returned_categorylist_entry in returned_categorylist:

            returnedcolumns_copy.remove(returned_categorylist_entry)

          returned_category = postprocess_dict['column_dict'][returnedcolumn]['category']

          #if column is from a multicolumn boolean MLinfilltype then we'll apply LSinvert
          if postprocess_dict['process_dict'][returned_category]['MLinfilltype'] in \
          ['multirt', 'multisp']:

            df = self.LS_invert(LabelSmoothing, df, returned_categorylist, postprocess_dict)
            
    return df


  #def inverseprocess_nmbr(df, categorylist, postprocess_dict):
  def inverseprocess_nmbr(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_numerical_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    std = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['std']
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = ((df[normkey] - offset) * std / multiplier ) + mean
    
    return df, inputcolumn
  
  #def inverseprocess_mean(df, categorylist, postprocess_dict):
  def inverseprocess_mean(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_mean_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    maxminusmin = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxminusmin']
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = ((df[normkey] - offset) * maxminusmin / multiplier ) + mean
    
    return df, inputcolumn
  
  #def inverseprocess_MADn(df, categorylist, postprocess_dict):
  def inverseprocess_MADn(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_MADn_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    MAD = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['MAD']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] * MAD + mean
    
    return df, inputcolumn
  
  #def inverseprocess_MAD3(df, categorylist, postprocess_dict):
  def inverseprocess_MAD3(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_MAD3_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    mean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['mean']
    MAD = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['MAD']
    datamax = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['datamax']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] * MAD + datamax
    
    return df, inputcolumn
  
  #def inverseprocess_mnmx(df, categorylist, postprocess_dict):
  def inverseprocess_mnmx(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_mnmx_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']
    maxminusmin = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maxminusmin']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] * maxminusmin + minimum
    
    return df, inputcolumn
  
  #def inverseprocess_retn(df, categorylist, postprocess_dict):
  def inverseprocess_retn(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_retn_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    minimum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['minimum']
    maximum = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['maximum']
    divisor = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['divisor']
    multiplier = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiplier']
    offset = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['offset']

    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    if maximum >= 0 and minimum <= 0:
      
      df[inputcolumn] = (df[normkey] - offset) * divisor / multiplier
      
    elif maximum >= 0 and minimum >= 0:
      
      df[inputcolumn] = (df[normkey] - offset) * divisor / multiplier + minimum
      
    elif maximum <= 0 and minimum <= 0:
      
      df[inputcolumn] = (df[normkey] - offset) * divisor / multiplier + maximum
    
    return df, inputcolumn
  
  #def inverseprocess_log0(df, categorylist, postprocess_dict):
  def inverseprocess_log0(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_log0_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 10 ** df[normkey]
    
    return df, inputcolumn
  
  #def inverseprocess_logn(df, categorylist, postprocess_dict):
  def inverseprocess_logn(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_logn_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = np.e ** df[normkey]
    
    return df, inputcolumn
  
  #def inverseprocess_addd(df, categorylist, postprocess_dict):
  def inverseprocess_addd(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_addd_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    add = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['add']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] - add
    
    return df, inputcolumn
  
  #def inverseprocess_sbtr(df, categorylist, postprocess_dict):
  def inverseprocess_sbtr(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_sbtr_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    subtract = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['subtract']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] + subtract
    
    return df, inputcolumn
  
  #def inverseprocess_mltp(df, categorylist, postprocess_dict):
  def inverseprocess_mltp(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_mltp_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    multiply = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['multiply']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] / multiply
    
    return df, inputcolumn
  
  #def inverseprocess_divd(df, categorylist, postprocess_dict):
  def inverseprocess_divd(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_divd_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    divide = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['divide']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] * divide
    
    return df, inputcolumn
  
  #def inverseprocess_rais(df, categorylist, postprocess_dict):
  def inverseprocess_rais(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_rais_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    raiser = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['raiser']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] ** (1 / raiser)
    
    return df, inputcolumn
  
  #def inverseprocess_absl(df, categorylist, postprocess_dict):
  def inverseprocess_absl(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_absl_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #incomplete info retention, no transformation performed
    #this funciton only populated to support partial info recovery
    #in case a full info_retention path is not available
    """
    
    normkey = categorylist[0]
    
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey]
    
    return df, inputcolumn
  
  #def inverseprocess_sqrt(df, categorylist, postprocess_dict):
  def inverseprocess_sqrt(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_sqrt_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey] ** 2
    
    return df, inputcolumn
  
  def inverseprocess_UPCS(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_UPCS_class
    #is simply a pass-through function, original character cases not retained
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey]
    
    return df, inputcolumn
  
  
  #def inverseprocess_pwrs(df, categorylist, postprocess_dict):
  def inverseprocess_pwrs(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_pwrs_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as power level returned to single column
    
    #in interest of expediency, building this method to extract power
    #level from column suffix appender
    #a potential improvement would be to add an additional entry to pwrs normalization_dict
    #matching column header to power, eg {'column_10^-1' : -1}
    #saving that for a future update
    #(same functionality, but would better match convention of library for use of column headers)
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for column in categorylist:
      
      power = 10 ** int(column.replace(inputcolumn + '_10^', ''))
      
      df[inputcolumn] = np.where(df[column] == 1, power, df[inputcolumn])
    
    return df, inputcolumn
  
  
  #def inverseprocess_pwr2(df, categorylist, postprocess_dict):
  def inverseprocess_pwr2(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_pwr2_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #note that pwr2 differs from pwrs in that negative numbers are allowed
    
    #this only achieves partial information recovery as power level returned to single column
    
    #in interest of expediency, building this method to extract power
    #level from column suffix appender
    #a potential improvement would be to add an additional entry to pwrs normalization_dict
    #matching column header to power, eg {'column_10^-1' : -1}
    #saving that for a future update
    #(same functionality, but would better match convention of library for use of column headers)
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    #for pwr2 trasnform suffix is either '_10^#' for positive values or '_-10^#' for negative
    for column in categorylist:
      
      if column[len(inputcolumn)+1] == '1':
      
        power = int(column.replace(inputcolumn + '_10^', ''))
      
        df[inputcolumn] = np.where(df[column] == 1, 10 ** power, df[inputcolumn])
        
      if column[len(inputcolumn)+1] == '-':
        
        power = int(column.replace(inputcolumn + '_-10^', ''))
        
        df[inputcolumn] = np.where(df[column] == 1, -(10 ** power), df[inputcolumn])
        
    return df, inputcolumn
  
  #def inverseprocess_pwor(df, categorylist, postprocess_dict):
  def inverseprocess_pwor(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_pwor_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    ordl_activations_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_activations_dict']
    
    powers = list(ordl_activations_dict)
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 10 ** df[normkey]
    
    return df, inputcolumn
  
  #def inverseprocess_por2(df, categorylist, postprocess_dict):
  def inverseprocess_por2(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_por2_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    #this has keys of pwr2 equivalent column headers and values of ordinal values found in column
    train_replace_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['train_replace_dict']
    
    inverse_train_replace_dict = {value:key for key,value in train_replace_dict.items()}
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
        
    #for pwr2 trasnform suffix is either '_10^#' for positive values or '_-10^#' for negative
    for column in list(train_replace_dict):
      
      #this dictionary is including a nan key entry for infill points, we'll leave these points as 0
      if column == column:

        if column[len(inputcolumn)+1] == '1':

          power = int(column.replace(inputcolumn + '_10^', ''))

          df[inputcolumn] = np.where(df[normkey] == train_replace_dict[column], 10 ** power, df[inputcolumn])

        if column[len(inputcolumn)+1] == '-':

          power = int(column.replace(inputcolumn + '_-10^', ''))

          df[inputcolumn] = np.where(df[normkey] == train_replace_dict[column], -(10 ** power), df[inputcolumn])
        
    return df, inputcolumn
  
  
  #def inverseprocess_bins(df, categorylist, postprocess_dict):
  def inverseprocess_bins(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bins_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #incomplete information recovery
    """
    
    normkey = categorylist[0]
    
    #this has keys of pwr2 equivalent column headers and values of ordinal values found in column
    binsmean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    binsstd = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsstd']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll follow convention that values placed in mid point of buckets
    #eg for bucket with boundaries mean through mean+stsndard devation
    #we'll return value mean + 1/2 standard deviation
    #and for tails we'll arbitraricly follow consistent deltas between buckets
    #(so this loses a lot of tail distribution information in the inversion)
    returned_values_dict = {}
    
    i = -2.5
    
    for bucket in ['s<-2', 's-21', 's-10', 's+01', 's+12', 's>+2']:
      
      #relies on suffix appender conventions
      column = inputcolumn + '_bins_' + bucket
      value = (i * binsstd + binsmean)
      
      returned_values_dict.update({column : value})
      
      i += 1
      
    
    df[inputcolumn] = 0
    
    for column in categorylist:
        
      df[inputcolumn] = np.where(df[column] == 1, returned_values_dict[column], df[inputcolumn])
      
        
    return df, inputcolumn
  
  
  #def inverseprocess_bint(df, categorylist, postprocess_dict):
  def inverseprocess_bint(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bint_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #incomplete information recovery
    """
    
    normkey = categorylist[0]
    
    #this has keys of pwr2 equivalent column headers and values of ordinal values found in column
#     binsmean = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
#     binsstd = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsstd']
    binsmean = 0
    binsstd = 1
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll follow convention that values placed in mid point of buckets
    #eg for bucket with boundaries mean through mean+stsndard devation
    #we'll return value mean + 1/2 standard deviation
    #and for tails we'll arbitraricly follow consistent deltas between buckets
    #(so this loses a lot of tail distribution information in the inversion)
    returned_values_dict = {}
    
    i = -2.5
    
    for bucket in ['t<-2', 't-21', 't-10', 't+01', 't+12', 't>+2']:
      
      #relies on suffix appender conventions
      column = inputcolumn + '_bint_' + bucket
      value = (i * binsstd + binsmean)
      
      returned_values_dict.update({column : value})
      
      i += 1
      
    
    df[inputcolumn] = 0
    
    for column in categorylist:
      
      df[inputcolumn] = np.where(df[column] == 1, returned_values_dict[column], df[inputcolumn])
      
    return df, inputcolumn
  
  
  #def inverseprocess_bsor(df, categorylist, postprocess_dict):
  def inverseprocess_bsor(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bsor_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #incomplete information recovery
    """
    
    normkey = categorylist[0]
    
    #this has keys of pwr2 equivalent column headers and values of ordinal values found in column
    binsmean = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsmean']
    binsstd = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['binsstd']
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll follow convention that values placed in mid point of buckets
    #eg for bucket with boundaries mean through mean+stsndard devation
    #we'll return value mean + 1/2 standard deviation
    #and for tails we'll arbitraricly follow consistent deltas between buckets
    #(so this loses a lot of tail distribution information in the inversion)
    returned_values_dict = {}
    
    i = -2.5
    
    for bucket in [0, 1, 2, 3, 4, 5]:
      
      returned_values_dict.update({bucket : (i * binsstd + binsmean)})
      
      i += 1
    
    df[inputcolumn] = df[normkey].copy()
    
    df[inputcolumn] = df[inputcolumn].replace(returned_values_dict)

        
    return df, inputcolumn
  
  
  #def inverseprocess_bnwd(df, categorylist, postprocess_dict):
  def inverseprocess_bnwd(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnwd_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_width_bnwd = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwd']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for i in range(len(bins_id)):
      
      _id =  bins_id[i]
      
      column = inputcolumn + '_bnwd_' + _id
      
      if column in list(df):
      
        df[inputcolumn] = np.where(df[column] == 1, i * bn_width_bnwd + bn_min, df[inputcolumn])
      
    
    return df, inputcolumn
  
  #def inverseprocess_bnwK(df, categorylist, postprocess_dict):
  def inverseprocess_bnwK(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnwK_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_width_bnwK = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwK']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for i in range(len(bins_id)):
      
      _id =  bins_id[i]
      
      column = inputcolumn + '_bnwK_' + _id
      
      if column in list(df):
      
        df[inputcolumn] = np.where(df[column] == 1, i * bn_width_bnwK + bn_min, df[inputcolumn])
      
    
    return df, inputcolumn
  
  #def inverseprocess_bnwM(df, categorylist, postprocess_dict):
  def inverseprocess_bnwM(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnwM_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_width_bnwM = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width_bnwM']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for i in range(len(bins_id)):
      
      _id =  bins_id[i]
      
      column = inputcolumn + '_bnwM_' + _id
      
      if column in list(df):
      
        df[inputcolumn] = np.where(df[column] == 1, i * bn_width_bnwM + bn_min, df[inputcolumn])
      
    
    return df, inputcolumn
  
  #def inverseprocess_bnwo(df, categorylist, postprocess_dict):
  def inverseprocess_bnwo(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnwo_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_width = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_width']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey].copy()
    
    df[inputcolumn] = df[inputcolumn] * bn_width + bn_min
    
    return df, inputcolumn
  
  
  #def inverseprocess_bnep(df, categorylist, postprocess_dict):
  def inverseprocess_bnep(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnep_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    textcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for i in range(len(textcolumns)):
      
      column = textcolumns[i]
      _id = int(bins_id[i])
      
      if i == 0:
        
        value = bins_cuts[i+1]
        
        df[inputcolumn] = np.where(df[column] == 1, value, df[inputcolumn])
        
      elif i == len(textcolumns)-1:
        
        value = bins_cuts[i]
        
        df[inputcolumn] = np.where(df[column] == 1, value, df[inputcolumn])
        
      else:
        
        value = (bins_cuts[i] + bins_cuts[i+1]) / 2
        
        df[inputcolumn] = np.where(df[column] == 1, value, df[inputcolumn])
    
    return df, inputcolumn
  
  #def inverseprocess_bneo(df, categorylist, postprocess_dict):
  def inverseprocess_bneo(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bneo_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    bn_min = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_min']
    bn_delta = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_delta']
    bn_count = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bn_count']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for i in range(len(bins_id)):
      
      _id = int(bins_id[i])
      
      if i == 0:
        
        value = bins_cuts[i+1]
        
        df[inputcolumn] = np.where(df[normkey] == _id, value, df[inputcolumn])
        
      elif i == len(bins_id)-1:
        
        value = bins_cuts[i]
        
        df[inputcolumn] = np.where(df[normkey] == _id, value, df[inputcolumn])
        
      else:
        
        value = (bins_cuts[i] + bins_cuts[i+1]) / 2
        
        df[inputcolumn] = np.where(df[normkey] == _id, value, df[inputcolumn])
    
    return df, inputcolumn
  
  
  #def inverseprocess_bkt1(df, categorylist, postprocess_dict):
  def inverseprocess_bkt1(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bkt1_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    textcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    buckets_bkt1 = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets_bkt1']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll use convention that bucket's replaced with the midpoint value
    #except for bottom bucket replaced with ceiling and top bucket replaced with floor
    #so don't have to deal with inf
    
    df[inputcolumn] = 0
    
    bucket_ids = []
    for textcolumn in textcolumns:
      bucket_id = textcolumn.replace(inputcolumn + '_bkt1_', '')
      bucket_ids.append(int(bucket_id))

    for i in bucket_ids:
      
      textcolumn = inputcolumn + '_bkt1_' + str(i)
      
      if i == 0:
        
        value = buckets_bkt1[i]
        
      elif i == bins_id[-1]:
        
        value = buckets_bkt1[-1]
        
      else:
        
        value = (buckets_bkt1[i-1] + buckets_bkt1[i]) / 2
        
      df[inputcolumn] = np.where(df[textcolumn] == 1, value, df[inputcolumn])
    
    return df, inputcolumn
  
  #def inverseprocess_bkt2(df, categorylist, postprocess_dict):
  def inverseprocess_bkt2(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bkt2_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    buckets_bkt2 = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets_bkt2']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    textcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textcolumns']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll use convention that bucket's replaced with the midpoint value
    #except for bottom bucket replaced with ceiling and top bucket replaced with floor
    #so don't have to deal with inf
    
    df[inputcolumn] = 0
    
    bucket_ids = []
    for textcolumn in textcolumns:
      bucket_id = textcolumn.replace(inputcolumn + '_bkt2_', '')
      bucket_ids.append(int(bucket_id))
      
    for i in bucket_ids:
      
      value = (bins_cuts[i] + bins_cuts[i+1]) / 2
      
      df[inputcolumn] = np.where(df[inputcolumn + '_bkt2_' + str(i)]==1, value, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_bkt3(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bkt3_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    buckets = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    ordl_activations_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_activations_dict']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll use convention that bucket's replaced with the midpoint value
    #except for bottom bucket replaced with ceiling and top bucket replaced with floor
    #so don't have to deal with inf
    
    df[inputcolumn] = 0
    
    for i in bins_id:
      
      if i == 0:
        
        value = bins_cuts[i+1]
        
      elif i == len(bins_id)-1:
        
        value = bins_cuts[i]
        
      else:
        
        value = (bins_cuts[i] + bins_cuts[i+1]) / 2
        
      df[inputcolumn] = np.where(df[normkey] == i, value, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_bkt4(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bkt4_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery as bins sets aggregated to single value per bin
    """
    
    normkey = categorylist[0]
    
    buckets = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['buckets']
    bins_cuts = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_cuts']
    bins_id = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['bins_id']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    #we'll use convention that bucket's replaced with the midpoint value
    #except for bottom bucket replaced with ceiling and top bucket replaced with floor
    #so don't have to deal with inf
    
    
    df[inputcolumn] = 0
    
    for i in bins_id:
      
      value = (bins_cuts[i] + bins_cuts[i+1]) / 2
        
      df[inputcolumn] = np.where(df[normkey] == i, value, df[inputcolumn])
    
    return df, inputcolumn
    
  
  #def inverseprocess_text(df, categorylist, postprocess_dict):
  def inverseprocess_text(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_text_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    #note that this will return numeric entries as str
    """
    
    normkey = categorylist[0]
    
    textlabelsdict_text = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['textlabelsdict_text']
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = 0
    
    for categorylist_entry in categorylist:
      
      df[inputcolumn] = \
      np.where(df[categorylist_entry], textlabelsdict_text[categorylist_entry], df[inputcolumn])
      
      
    return df, inputcolumn
  
  #def inverseprocess_ordl(df, categorylist, postprocess_dict):
  def inverseprocess_ordl(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_ordl_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    #note that this will return numeric entries as str
    """
    
    normkey = categorylist[0]
    
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    inverse_ordinal_dict = {value:key for key,value in ordinal_dict.items()}
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = \
    df[normkey].replace(inverse_ordinal_dict)
    
    
    return df, inputcolumn
  
  #def inverseprocess_ord3(df, categorylist, postprocess_dict):
  def inverseprocess_ord3(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_ord3_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    #note that this will return numeric entries as str
    """
    
    normkey = categorylist[0]
    
    ordinal_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordinal_dict']
    
    inverse_ordinal_dict = {value:key for key,value in ordinal_dict.items()}
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = \
    df[normkey].replace(inverse_ordinal_dict)
    
    
    return df, inputcolumn
  
  #def inverseprocess_bnry(df, categorylist, postprocess_dict):
  def inverseprocess_bnry(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_bnry_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    """
    
    normkey = categorylist[0]
    
    onevalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][1]
    zerovalue = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey][0]
    
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    
    df[inputcolumn] = \
    np.where(df[normkey] == 1, onevalue, 0)
    
    df[inputcolumn] = \
    np.where(df[normkey] == 0, zerovalue, df[inputcolumn])
      
    
    return df, inputcolumn
  
  #def inverseprocess_1010(df, categorylist, postprocess_dict):
  def inverseprocess_1010(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_1010_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    #note that this will return numeric entries as str
    """
    
    normkey = categorylist[0]
    
    _1010_binary_encoding_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['_1010_binary_encoding_dict']
    
    inverse_binary_encoding_dict = {value:key for key,value in _1010_binary_encoding_dict.items()}
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    
    for categorylist_entry in categorylist:
      
      if categorylist_entry == categorylist[0]:
        
        df[inputcolumn] = df[categorylist_entry].astype(str)
        
      else:
        
        df[inputcolumn] = df[inputcolumn] + df[categorylist_entry].astype(str)
        
        
    df[inputcolumn] = df[inputcolumn].replace(inverse_binary_encoding_dict)
      
    return df, inputcolumn
  
  def inverseprocess_splt(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_splt_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    
    #returns the overlaps, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_splt']
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 0
    
    for column in newcolumns:
      
      overlap = column.replace(inputcolumn + '_splt_', '')
      
      df[inputcolumn] = np.where(df[column] == 1, overlap, df[inputcolumn])
    
    return df, inputcolumn
  
  def inverseprocess_spl8(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_splt_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    
    #returns the overlaps, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_spl8']
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 0
    
    for column in newcolumns:
      
      overlap = column.replace(inputcolumn + '_spl8_', '')
      
      df[inputcolumn] = np.where(df[column] == 1, overlap, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_spl2(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_spl2_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    
    #returns the overlaps as a passthrough, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey]

    
    return df, inputcolumn
  
  
  def inverseprocess_spl5(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_spl5_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    #also missing entries that didn't have any overlaps identified (returned as 0)
    
    #returns the overlaps, not the full entries
    #since it doesn't know which of the full entries to return
    
    #returning zeros from inversion is counter to the convention used in other transforms
    #So we'll replace zeros with 'zzzinfill'
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    df[inputcolumn] = df[normkey]
    
    df[inputcolumn] = np.where(df[inputcolumn] == '0', 'zzzinfill', df[inputcolumn])

    
    return df, inputcolumn
  
  
  def inverseprocess_sp15(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_sp15_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    
    #returns the overlaps, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_sp15']
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    for column in newcolumns:

      overlap = column.replace(inputcolumn + '_sp15_', '')

      df[inputcolumn] = np.where((df[column] == 1) & (df[inputcolumn] == 'zzzinfill'), overlap, df[inputcolumn])

    return df, inputcolumn
  
  
  def inverseprocess_sp16(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_sp15_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without overlap
    
    #returns the overlaps, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['splt_newcolumns_sp16']
#     overlap_dict = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    for column in newcolumns:
      
      overlap = column.replace(inputcolumn + '_sp16_', '')
      
      df[inputcolumn] = np.where((df[column] == 1) & (df[inputcolumn] == 'zzzinfill'), overlap, df[inputcolumn])
    
    return df, inputcolumn
  
  
  
  def inverseprocess_srch(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_srch_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without srch term
    
    #returns the search term, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    search_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['search_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    #to match convention of prioritizing search parameter entries at end of list
    inverse_search = list(search_dict)
    inverse_search.reverse()
    
    for column in inverse_search:
      
      search = search_dict[column]
      
      if column in list(df):

        df[inputcolumn] = np.where((df[column] == 1) & (df[inputcolumn] == 'zzzinfill'), search, df[inputcolumn])
    
    return df, inputcolumn
  
  
  
  def inverseprocess_src2(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_src2_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without search term
    
    #returns the search term, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['src2_newcolumns_src2']
#     overlap_dict = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    #to match convention of prioritizing search parameter entries at end of list
    newcolumns.reverse()
    
    for column in newcolumns:
      
      searchterm = column.replace(inputcolumn + '_src2_', '')
      
      if column in list(df):
      
        df[inputcolumn] = np.where((df[column] == 1) & (df[inputcolumn] == 'zzzinfill'), searchterm, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_src3(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_src3_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without search term
    
    #returns the search term, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    newcolumns = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['srch_newcolumns_src3']
#     overlap_dict = \
#     postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    #to match convention of prioritizing search parameter entries at end of list
    newcolumns.reverse()
    
    for column in newcolumns:
      
      searchterm = column.replace(inputcolumn + '_src3_', '')
      
      if column in list(df):
      
        df[inputcolumn] = np.where((df[column] == 1) & (df[inputcolumn] == 'zzzinfill'), searchterm, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_src4(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_src4_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without search term
    
    #returns the search term, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    ordl_dict1 = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['ordl_dict1']
    
    df[inputcolumn] = 'zzzinfill'
    
    #to match convention of prioritizing search parameter entries at end of list
    keys = list(ordl_dict1)
    keys.reverse()
    
    for key in keys:
      
      searchterm = ordl_dict1[key].replace(inputcolumn + '_src4_', '')
      
      df[inputcolumn] = np.where((df[normkey] == key) & (df[inputcolumn] == 'zzzinfill'), searchterm, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def inverseprocess_nmrc(self, df, categorylist, postprocess_dict):
    """
    #inverse transform corresponding to process_nmrc_class
    #assumes any relevant parameters were saved in normalization_dict
    #does not perform infill, assumes clean data
    
    #this only achieves partial information recovery, missing entries without search term
    
    #returns the search term, not the full entries
    #since it doesn't know which of the full entries to return
    """
    
    normkey = categorylist[0]
    
    inputcolumn = postprocess_dict['column_dict'][normkey]['inputcolumn']
    
    overlap_dict = \
    postprocess_dict['column_dict'][normkey]['normalization_dict'][normkey]['overlap_dict']
    
    df[inputcolumn] = 'zzzinfill'
    
    
    for key in overlap_dict:
      
      extract = overlap_dict[key]
      
      df[inputcolumn] = np.where((df[normkey] == extract) & (df[inputcolumn] == 'zzzinfill'), key, df[inputcolumn])
    
    return df, inputcolumn
  
  
  def df_inversion(self, categorylist_entry, df_test, postprocess_dict, inverse_categorytree, printstatus):
    """
    #support function for df_inversion_meta
    #this is where the inverseprocess functions are applied
    """
    
    origcolumn = postprocess_dict['column_dict'][categorylist_entry]['origcolumn']
    category = postprocess_dict['column_dict'][categorylist_entry]['category']
    categorylist = postprocess_dict['column_dict'][categorylist_entry]['categorylist']
    
    columns_before_inversion = set(df_test)
    
    if 'inverseprocess' in postprocess_dict['process_dict'][category]:
      
      if callable(postprocess_dict['process_dict'][category]['inverseprocess']):
        
        df_test, inputcolumn = \
        postprocess_dict['process_dict'][category]['inverseprocess'](df_test, categorylist, postprocess_dict)
        #df, categorylist, postprocess_dict
    
    columns_after_inversion = set(df_test)
    
    #convention is that inversion always returns a single column
    #(multi-column transformations only performed on final leaf of a forward pass branch)
    inputcolumn = list(columns_after_inversion - columns_before_inversion)[0]
    
    if inputcolumn != origcolumn:
      
      df_test, inputcolumn = \
      self.df_inversion(inputcolumn, df_test, postprocess_dict, inverse_categorytree, printstatus)
      
    
    
    return df_test, inputcolumn
    
    
  def df_inversion_meta(self, df_test, source_columns, postprocess_dict, printstatus):
    """
    #Performs inversion of transformation sets
    #Relies on optional processdict entries of info_retention and inverseprocess
    #Uses the inverse_categorytree populated during automunge
    #For all entries associated with a single source column, 
    #Selects the returned categorylist with the lowest depth with info retention through the branch
    #If info retention not available, instead takes the shortest depth with inverse transformations available
    #If full set of inverse transfomations not available, does not perfgorm inversion for that source column
    #For each inversion path selected performs the sets of transfomrations in inverse order to the original path's population steps
    #All returned sets are added to the dataframe, and at conclusion of the function
    #any columns not matching a source column are removed form the set
    #also returns a list of the recovered columns
    #relies of column headers of received df_test matching the column headers of original columns returned from automunge
    #note that this function may be applied consistently to test or label sets
    """
    
    inverse_categorytree = postprocess_dict['inverse_categorytree']
    
    #we'll store the inversion paths info in this dictionary
    inversion_info_dict = {}
    
    #this will be a list of columns successfully recovered
    recovered_list = []
    
    
    for source_column in source_columns:
      
      if printstatus is True:
        print("Evaluating inversion paths for columns derived from: ", source_column)
      
      returned_columns = postprocess_dict['origcolumn'][source_column]['columnkeylist']
      
      #we'll just take one of the columns here if it is part of a multicolumn set
      returned_columns_clean = returned_columns.copy()

      
      for returned_column in returned_columns:
        
        if returned_column in returned_columns_clean:
        
          categorylist = postprocess_dict['column_dict'][returned_column]['categorylist']

          for categorylist_entry in categorylist:

            if categorylist_entry != returned_column:
              
              #we'll just take one of the columns here if it is part of a multicolumn set
              returned_columns_clean.remove(categorylist_entry)
      
      #initialize for ranking paths of transformation inversions
      path_depth_eval         = {}
      inforetention_eval      = {}
      transformavailable_eval = {}
      
      for returned_column in returned_columns_clean:
        
        category = postprocess_dict['column_dict'][returned_column]['category']
        
        path_depth_eval.update({returned_column : inverse_categorytree[category][returned_column][4]})
        inforetention_eval.update({returned_column : inverse_categorytree[category][returned_column][5]})
        transformavailable_eval.update({returned_column : inverse_categorytree[category][returned_column][6]})
        
      #now invert the path_depth_eval dictionary for sorting
      #inverse_path_depth_eval = {value:key for key,value in path_depth_eval.items()}
      inverse_path_depth_eval = {}
      for key, value in path_depth_eval.items():
        if value not in inverse_path_depth_eval:
          inverse_path_depth_eval.update({value : [key]})
        else:
          inverse_path_depth_eval[value].append(key)
          
      #and sort it to find shortest depth
      inverse_path_depth_eval = dict(sorted(inverse_path_depth_eval.items()))
      
      #note that this sorted by depth method is based on a heuristic
      #that the fewest number of transforms will be the most efficient
      
      #now let's select our returned column for the path
      best_path = False
      info_retention_marker = False
      
      for depth in inverse_path_depth_eval:
        
        for path in inverse_path_depth_eval[depth]:
        
          inforetention = inforetention_eval[path]
          transformavailable = transformavailable_eval[path]

          if inforetention and transformavailable:

            best_path = path
            info_retention_marker = True

            break

          elif transformavailable and best_path is False:

            best_path = path
            
        if info_retention_marker is True:
          
          break
            
            
          
      if printstatus is True:
        
        if best_path is not False:
          
          if info_retention_marker is True:
          
            print("Inversion path selected based on returned column ", best_path)
            print("With full recovery.")
            
          else:
            
            print("Inversion path selected based on returned column ", best_path)
            print("With partial recovery.")
          
        else:
          
          print("No inversion path available for source column: ", source_column)
          print()
          
          
      #great we've selected our path for this source column's inversion
      inversion_info_dict.update({source_column : {'best_path' : best_path, \
                                                   'info_retention' : info_retention_marker}})
      
      #now let's apply our inversion transforms
      #if best_path is not False and callable(postprocess_dict['process_dict'][postprocess_dict['column_dict'][best_path]['category']]['inverseprocess']):
      if best_path is not False:
        
        columns_before_inversion = set(df_test)
        
        df_test, _1 = self.df_inversion(best_path, df_test, postprocess_dict, inverse_categorytree, printstatus)
        
        columns_after_inversion = set(df_test)
        
        #this gives, for the target source columns, returned columns, source columns, and intermediate columns
        full_returned_columns = columns_after_inversion - (columns_before_inversion - set(returned_columns))
        
      else:
        
        full_returned_columns = returned_columns
    
      for column in full_returned_columns:

        if column in source_columns:

          recovered_list += [column]

        else:

          #we're only retaining successfully recovered source columns in the returned df
          #this deletion is performed sequentially for columns returned from given source column for memory management
          del df_test[column]
          
      if printstatus is True:
        
        if best_path is not False:
          
          print("Recovered source column: ", source_column)
          print()
          
    
    return df_test, recovered_list, inversion_info_dict
  
  